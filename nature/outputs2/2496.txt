when carl bergstrom worked on plans to prepare the united states for a hypothetical pandemic, in the early 2000s, he and his colleagues were worried vaccines might not get to those who needed them most. â€œwe thought the problem would be to keep people from putting up barricades and stopping the truck and taking all the vaccines off it, giving them to each other,â€� he recalls.when covid-19 arrived, things played out quite differently. one-quarter of u.s. adults remain unvaccinated against a virus that has killed more than 1 million americans. â€œour ability to convince people that this was a vaccine that was going to save a lot of lives and that everyone needed to take was much, much worse than most of us imagined,â€� bergstrom says.he is convinced this catastrophic failure can be traced to social media networks and their power to spread false informationâ€in this case about vaccinesâ€far and fast. â€œbullshitâ€� is bergstromâ€™s umbrella term for the falsehoods that propagate onlineâ€both misinformation, which is spread inadvertently, and disinformation, designed to spread falsehoods deliberately.an evolutionary biologist at the university of washington (uw), seattle, bergstrom has studied the evolution of cooperation and communication in animals, influenza pandemics, and the best ways to rank scientific journals. but over the past 5 years, he has become more and more interested in how â€œbullshitâ€� spreads through our information ecosystem. he started fighting it before covid-19 emergedâ€through a popular book, a course he gives at uwâ€™s center for an informed public, and, ironically, a vigorous presence on social mediaâ€but the pandemic underscored how persuasive and powerful misinformation is, he says.â€œmisinformation has reached crisis proportions,â€� bergstrom and his uw colleague jevin west wrote in a 2021 paper in the proceedings of the national academy of sciences (pnas). â€œit poses a risk to international peace, interferes with democratic decision-making, endangers the well-being of the planet, and threatens public health.â€� in another pnas paper, bergstrom and others issued a call to arms for researchers to study misinformation and learn how to stop it.that research field is now taking off. â€œyou have scholars from so many different disciplines coming together around this common themeâ€�â€including biology, physics, sociology, and psychologyâ€says philipp lorenz-spreen, a physicist who studies social media networks at the max planck institute for human development.but the influx has yet to coalesce into a coherent field, says michael bang petersen, a political scientist at aarhus university. â€œitâ€™s still spread out in different disciplines and we are not really talking that much together.â€� thereâ€™s also disagreement about how best to study the phenomenon, and how significant its effects are. â€œthe field is really in its infancy,â€� lorenz-spreen says.bergstrom grew up in michigan, but as a child twice spent almost a year in australia, where his father, an economist, was on a sabbatical. â€œthatâ€™s where i fell in love with birds,â€� he says. in his bedroom he had a poster of all the parrots of australia: â€œitâ€™s like you handed an 8-year-old a bunch of crayons and a bunch of outlines of parrots and just said, â€˜make them pretty.â€™â€� corvids are his favorite birds because of their cognitive abilities. on twitter, his avatar is a crow.as a biology student at stanford university, bergstrom grew fascinated by communicationâ€an evolutionary puzzle because of the potential for deception. â€œif i listen to you, i give you a handle over my behavior,â€� bergstrom says. â€œi might do the things you want, even if theyâ€™re not in my interest.â€� in his ph.d. thesis, he tackled the question of how communication can stay useful when there is so much to be gained from misusing it.in nature, he concluded the answer is often that lies are costly. begging for food makes baby birds vulnerable, for example, so they have an incentive to do it only when necessary. â€œif youâ€™re just a defenseless ball of meat sitting in a nest and canâ€™t go anywhere, yelling at the top of your lungs is amazingly stupid,â€� bergstrom says. â€œif theyâ€™re not really hungry, theyâ€™ll just shut up.â€� on social media, such repercussions barely exist, he says: liars have little incentive to shut up.in the early 2000s, while a postdoc in the lab of population biologist bruce levin at emory university, bergstrom awoke to the threat of infectious disease. he collaborated with a fellow postdoc, marc lipsitchâ€now an epidemiologist at the harvard t.h. chan school of public healthâ€on papers about pandemic preparedness. then he delved into network theory, which aims to mathematically describe the properties of networks, including those that spread disease. â€œit appealed to multiple aspects of my intellectual interests,â€� he says.network theory in turn led bergstrom to the spread of information. together with martin rosvall, a physicist at umeã¥ university, he found a way to use citation data to create â€œmapsâ€� of the scientific enterprise that showed, for example, how fields cluster and which scientists are most influential. â€œthe algorithms we came up with turned out to work much, much better than i expected,â€� bergstrom says. â€œi still donâ€™t really understand why they are so good.â€�bergstromâ€™s path through different disciplines is a testament to his curiosity and creativity, west says: â€œyouâ€™d be hard-pressed to find someone that really has moved around in such disparate fields and had impacts in all these different areas.â€�around 2017, bergstromâ€™s interests started to coalesce around the topic of misinformation. the debate about russian misinformation and the role it played in the 2016 election of donald trump as u.s. president, and an inspiring 2017 meeting about misinformation organized by biologist joe bak-coleman, then at princeton university, made him realize â€œthis is actually a huge problem and one thatâ€™s going to take all these different approaches to deal with it, many of which i personally found very interesting,â€� he says.bergstrom sees social media, like many other things in life, through an evolutionary lens. the popular platforms exploit humanityâ€™s need for social validation and constant chatter, a product of our evolution, he says. he compares it to our craving for sugar, which was beneficial in an environment where sweetness was rare and signaled nutritious food, but can make us sick in a world where sugar is everywhere. facebook exploits humansâ€™ thirst for contact, in his view, like a coca-cola for the mind, allowing people to connect with others in larger numbers during a single day than they might have over a lifetime in humanityâ€™s past.and whereas coca-cola cannot tweak its formula on a weekly basis, social media platforms can constantly change their algorithms and test out new strategies to keep us engaged. â€œthe social media companies are able to run the largest scale psychological experiments in history by many orders of magnitude, and theyâ€™re running them in real time on all of us,â€� bergstrom says.often, engagement comes from crass conflict: â€œin a schoolyard people cluster around fights, and the same thing happens on twitter,â€� he says. zeynep tufekci, a sociologist at columbia university, agrees. â€œsocial connection is ingroup/outgroup,â€� tufekci says. that promotes polarization and tribalism as well as exaggeration and misinformation, she says.online networks also undermine traditional rules of thumb about communication. before the advent of the internet, for example, hearing the same information from multiple people made it more trustworthy. â€œin the physical world, it would be almost impossible to meet anyone else who thinks the world is flat,â€� stephan lewandowsky, a psychologist at the university of bristol, wrote in an email. â€œbut online, i can connect with the other .000001% of people who hold that belief, and may gather the (false) impression that it is widely shared.â€�social media companies have little incentive to change their practices because they make money selling ads. â€œthe network structures along which we share information have changed radically in the last 20 years, and theyâ€™ve changed without any kind of stewardship,â€� bergstrom says. â€œtheyâ€™ve changed basically just to help some tech startups sell ads.â€�seeing the problem does not mean he is immune to it. bergstrom admits sometimes waking up at 4 a.m. and checking his twitter mentions. â€œthatâ€™s the stupidest thing i could possibly do. because an hour and a half later, iâ€™m pissed off and canâ€™t sleep,â€� he says. â€œit works on all of us, even those of us who know what theyâ€™re doing.â€�in a perspective published in pnas last year, bergstrom and 16 other scientists from various fields argued that the study of how the information ecosystem influences human collective behavior needed to become a â€œcrisis discipline,â€� much like climate science, that could also suggest ways to tackle the issue. â€œthat paper was just pointing out that the building is on fire,â€� says bak-coleman, the lead author, whoâ€™s now also at uwâ€™s center for the informed public. the problem is we donâ€™t know how to quench the fire, he says.one key problem is that the way information spreads on social media is determined by the platformsâ€™ proprietary algorithms, which scientists have not been able to study. â€œeven if there was a crisis discipline like carl wants it, we simply do not have the data,â€� says dietram scheufele who studies science communication at the university of wisconsin, madison. (the algorithms have helped bring about a â€œtectonic shift in the balance of power in science information ecologies,â€� scheufele and his colleague dominique brossard argued in a recent perspective in science.) â€œthe only way in which we can create that discipline is by policymakers forcing tech companies to provide data access,â€� petersen says.researchers have tried to understand the flow of mis- and disinformation in other ways, but the results are often not clear-cut. last year, a report by the center for countering digital hate claimed that just 12 peopleâ€which it dubbed the â€œdisinformation dozenâ€�â€were the source of 73% of misinformation about covid-19 on facebook. banning these â€œsuperspreadersâ€� could reduce the amount of misinformation significantly, the authors suggested. but meta, facebookâ€™s parent company, pushed back against what it called â€œa faulty narrativeâ€� in a blog post. the report was based on just 483 pieces of content from only 30 groups and â€œin no way representative of the hundreds of millions of posts that people have shared about covid-19 vaccines in the past months on facebook,â€� meta said.in 2018, researchers from the massachusetts institute of technology published a study in science showing false news spreads â€œfarther, faster, deeper, and more broadly than the truth.â€� the reason is that people like novelty, and false stories are likely to be more novel, the authors suggested. if true, this might allow false news to be identified automatically, simply through the way it spreads.but the picture is complicated. the paper looked specifically at a subset of news that had been fact-checked by independent organizations such as snopes, which meant the stories had to spread far and wide to merit that kind of attention. the researchers replicated their results with a larger news sample that was not fact-checked, but whether the conclusions apply to all fake news is not clear. and a reanalysis by other researchers late last year found that whereas the fact-checked news stories did spread further, the pattern of their spread was not different from true news that reached a similar number of people.bak-coleman, who has studied how schools of fish suppress false alarms from fish in the periphery of the swarm, believes the density of connections on social media make it harder to filter out bad information. bergstrom agrees. â€œif we actually cared about resisting disinformation, twitter and facebook might be better off if they said, â€˜look, you can have 300 friends and thatâ€™s it,â€™â€� he says.but that, too, needs study, he says, as do strategic questions: â€œwhat would agents do if they wanted to try to inject misinformation into a network? where would they want to be? and then the flip side: how do you try to counter that?â€�to make progress, some researchers say, the budding field needs to focus less on the networkâ€™s properties and more on its human nodes. like viruses, misinformation needs people to spread, tufekci says. â€œso what you want to really do is study the people end of it,â€� including peopleâ€™s reasons for clicking like or retweet, and whether misinformation changes their behavior and beliefs.thatâ€™s also difficult to study, however. at uwâ€™s center for an informed public, billions of online conversations are captured every year. if a certain piece of misinformation is identified, â€œyou can go about measuring how itâ€™s amplified, how fast it grows, whoâ€™s amplifying it,â€� says west, who directs the center. â€œbut it is very difficult to see whether that translates into behavior, and not just behavior, but beliefs.â€�a review of 45 studies on misinformation about covid-19 vaccines, recently published as a preprint by researchers in norway, concluded thatâ€although misinformation was rampantâ€there were few high-quality studies of its effects. â€œthere is a need for more robust designs to become more certain regarding the actual effect of social media misinformation on vaccine hesitancy,â€� the authors concluded.scientists have tried to study the issue by isolating a very small part of the problem. a recent paper in nature human behaviour, for example, reported the results of an experiment conducted in september 2020, before covid-19 vaccines became available. researchers asked 4000 people in both the united kingdom and the united states whether they planned to get vaccinated, exposed them to either facts or false information about the vaccines in development, then measured their intent again. in both countries, exposure to misinformation led to a decline of six percentage points in the share of people saying they would â€œdefinitelyâ€� accept a vaccine.tufekci has no doubt social media has a major impact on society: â€œjust look around. itâ€™s a complete shift in how the information ecology works at a social level. how can you not expect it to have an impact?â€� but small-scale lab studies simply canâ€™t properly measure the problem, she says. â€œan ecological shift of this nature doesnâ€™t lend itself to that kind of study.â€� people in the real world are likely exposed not to one piece of misinformation, but to a lot of it over time, often coming to them through friends, family, or other people they trust. and online misinformation cascades through the information ecosystem, pushing more traditional media to spread it as well. â€œfox news is kind of competing with that stuff on facebook,â€� tufekci says. â€œfox news doesnâ€™t operate in a vacuum.â€�we in the infectious disease epidemiology world spent decades preparing for a crisis like this, but were never imagining that we'd be fighting on two fronts, the virus on one and this sort of hyper-partisan disinformation on the other.but scheufele isnâ€™t so sure misinformation has a big impact. â€œthere is a correlation of course between all this misinformation and the decision by so many people not to get vaccinated, but correlation does not mean causation,â€� he says. he believes people choose information to conform to their world view, not the other way around. in his view, misinformation is a symptom, but the real disease is polarization and a political system and societal climate that rewards it. given the deep political polarization in the united states and trumpâ€™s â€œunusualâ€� presidency, the pandemic â€œwas always going to be a shitshowâ€� there, scheufele says.a recent review in nature, however, argued that people do not fall for misinformation because of polarization. the authors cited studies suggesting true information is more likely to be believed than false information, even if it doesnâ€™t align with oneâ€™s political views. â€œpolitics does not trump truth,â€� they concluded. the real problem is people sharing information with little attention to whether it is true, the authors wrote. â€œrather than being bamboozled by partisanship, people often fail to discern truth from fiction because they fail to stop and reflect about the accuracy of what they see on social media.â€�reining in such thoughtless sharing is the goal of two approaches to tackling misinformation. one, known in the field as â€œnudging,â€� includes anything from flagging suspicious informationâ€for example because itâ€™s based on few or anonymous sourcesâ€to making it harder to share something. a platform might force users to copy and paste material before sharing it, or put a limit on how often a post can be reshared. â€œitâ€™s being shown again and again that it has some benefits when you give people a little time to think about their decision to share something,â€� lorenz-spreen says.the other approach, â€œboostering,â€� is designed to improve usersâ€™ critical skills. this includes â€œprebunkingâ€�â€teaching people how to spot misinformationâ€and â€œlateral reading,â€� verifying new information while you are reading it by looking for outside information. those skills are what bergstrom has tried to teach in a course he taught with west and in their book, calling bullshit: the art of skepticism in a data-driven world.when the pandemic started in early 2020, bergstrom initially thought it would be a great opportunity to study the spread of misinformation, drawing on his background in network theory. â€œi thought when this has all gone away in march, then i can go through these data sets and figure out what were the spread patterns that we were seeing.â€� but the pandemic did not go away and bergstrom was sucked into the practical work of explaining the science and calling out misinformation. he does so primarily in long threads on twitter, where he now has more than 150,000 followers.in early 2020, for example, he took on eric feigl-ding, a nutritional epidemiologist then at harvard chan who amassed a huge following with what many scientists felt were alarmist tweets. when feigl-ding tweeted about a preprint claiming that sars-cov-2 contained sequences from hiv and was likely engineered, bergstrom called him an â€œalarmist attention-seeker.â€� (the preprint was withdrawn within days.)but the spat showed that defining misinformation is difficult. feigl-ding rang the alarm many timesâ€he is â€œvery, very concernedâ€� about every new variant, bergstrom says, and â€œwill tweet about how itâ€™s gonna come kill us allâ€�â€but turned out to be right on some things. â€œitâ€™s misinformation if you present these things as certainties and donâ€™t adequately reflect the degree of uncertainty that we have,â€� bergstrom says.feigl-ding says using twitter early in the pandemic was â€œa big learning curveâ€� for most scientists and that bergstrom and he â€œhave long made amends and got along well since mid 2020.â€� indeed, bergstrom worries that his tone stoked unnecessary division and helped spread the sense that scientists are just fighting over their egos. â€œthe overall lesson, whether itâ€™s dealing with eric or others, is that i would have done better to be a bit drier, a bit more dispassionate.â€�bergstrom realizes his battle against misinformation is a sisyphean task. he likes to quote brandoliniâ€™s law, which says â€œthe amount of energy needed to refute bullshit is an order of magnitude larger than is needed to produce it.â€� tufekci concurs. â€œi like carlâ€™s stuff. i benefit from following him and iâ€™m sure the people who follow him benefit from following him,â€� she says. â€œbut the societal solution is not to need carl.â€�