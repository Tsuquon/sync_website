ai may uncover new scientific concepts that defy human intuition, but will we be able to understand and operate with them? this scenario might seem like science fiction, but physicists have faced it before.you have full access to this article via your institution.in ted chiang’s novella story of your life, humans find out that some of our basic concepts in physics make no sense to an alien species, making it challenging to communicate our scientific theories. the story is quite a departure from what most science fiction takes for granted: that science is the most likely language we can use to communicate with other intelligent species. chiang challenges this view, suggesting that the way we experience the physical world influences the way we communicate and develop languages, including the language of science. one might think that we are nowhere near putting this assumption to the test, but what if aliens are already here?they are, claims been kim, ai research scientist at google brain, in her talk at the tenth international conference on learning representations (iclr). she is not referring to a first contact with a biological alien species, but rather an alien we humans have created: alphago.in 2016, deepmind’s go playing program alphago defeated top professional player lee sedol in 4 out of 5 games. alphago’s victory overall was a landmark for artificial intelligence, but in particular its move number 37 in the second game took go experts by surprise, showing an unfathomable intuition of the game and a departure from the wisdom refined by human players over centuries. if only alphago could explain its move, then perhaps we could gain new insights into the game of go. unfortunately, alphago, like most cutting-edge ai today, is akin to the aliens in chiang’s story: we have little common ground to build communication on.back to chiang’s story — and a mild spoiler — the way aliens experienced the physical world shaped their idea of simple intuitive concepts, which turned out to be anything but simple for humans. this notion had also been contemplated by physicist phil anderson in 19721: for a hypothetical gaseous, but intelligent, citizen of jupiter or of a hydrogen cloud somewhere in the galactic centre, the properties of ordinary crystals might well be a more baffling and intriguing puzzle than those of superfluid helium. a machine, as a disembodied intelligence, would likely have an even stranger representation of the world it cannot experience directly, and translating that to human understanding would be even more challenging — perhaps even hopeless — than decoding the language of a biological alien, who might share at least some common experiences (like gravity or electromagnetic radiation).researchers are starting to discuss the implications of ai’s impenetrable insights. i think it is likely that ai will soon (if not already) understand things in ways that are beyond our abilities to understand. in this case, we may have to content ourselves with the knowledge that the machine understands the underlying physics, even if we never will, says matthew schwartz, professor of theoretical physics at harvard university. schwartz is not bothered by the prospect of ai developing a physics understanding that he may never comprehend. he points out that one does not need to understand economics to benefit from sound economic policy2. this pragmatic view is reminiscent of the shut-up-and-calculate approach to quantum mechanics, where one is less concerned with the interpretation of the theory as long as one can make useful computations and accurate predictions. still, schwartz hopes we will be able to develop a new language and new set of tools for interpreting the machine’s output. he is not alone.despite the obvious challenges, researchers are trying to create a common language for humans and machines (pictured). the endeavour starts with finding some common ground. kim is trying to understand how human representations align — if at all — with those of machines. for example, would stripy patterns appearing in various images make any sense to a machine? it seems the answer is yes, there is some alignment between the concept of stripes humans and machines have, at least in some situations3. what is perhaps more surprising is that machine learning algorithms seem to follow some of the laws of gestalt psychology, which posits that people tend to perceive objects as a whole rather than as parts or constituent features. for example, the law of closure states that a person will perceive a geometric shape with missing parts, say a tringle with missing bits of the edges, as whole. neural networks trained to classify natural images seem to have a similar tendency4.another gestalt principle is the law of symmetry, which states that the human mind spots symmetries, perceiving symmetrical objects as part of a whole. would ai also naturally find symmetry as a relevant feature, as it appears to do for the law of closure? that’s a question of particular interest to physicists.it is only slightly overstating the case to say that physics is the study of symmetry, wrote anderson1. given their central role in our understanding of physics, symmetries seem a good bet when looking for possible shared concepts with ai. in a recent study5, ziming liu and max tegmark, theoretical physicists at mit, developed a machine learning method to automatically discover hidden symmetry. the idea is to recast the degree of asymmetry as a quantity, parametrized by a neural network, that can be minimized: that is, the asymmetry is reduced. then liu and tegmark translate what the neural network has learned into a human-interpretable mathematical expression, using a physics-inspired tool called ai feynman6. ai feynman is based on symbolic regression, a method that searches through the space of mathematical expressions to find the one that best fits the data. in several test cases, ranging from the 1d harmonic oscillator to the schwarzschild metric of non-rotating black holes, they recovered a variety of symmetries.for the time being the method works for finding known symmetries, such as translational invariance, but liu and tegmark hope it can be expanded to search for unknown symmetries. but whether ai would find symmetries (known or unknown) meaningful in the way humans do is a different story.another concept dear to physicists, and intimately related to symmetry, are conservation laws. recall noether’s theorem, which says that each conservation law is associated with a symmetry — a connection that can be expressed in the equivalent languages of hamiltonian or lagrangian mechanics.in 2019, a paper contributed to the neurips conference introduced a neural network that would learn the hamiltonian of a system directly from the data7. a year later a contribution to the iclr conference reported a class of neural networks (lagrangian neural networks) that can learn arbitrary lagrangians from data8. both hamiltonian and lagrangian neural networks seem to capture the dynamics of physical systems and uncover conservation laws well but the latter work for arbitrary systems of coordinates.lagrangian neural networks, in particular, seem promising and were quickly adopted by others. for example, liu and collaborators used this approach to look for ‘new physics’ by decomposing force into conservative and non-conservative parts, the former learned by a lagrangian neural network and the latter by a more generic neural network9. the new physics is the non-conservative part, for example, friction in a damped double pendulum. both studies8,9 tested their ideas on simple toy models using numerically simulated data. the next step is to test these methods on real data.in a recent study10, a group of researchers trained a machine learning model on 30 years of observational data to simulate the dynamics of the sun and planets in our solar system. they then used symbolic regression to automatically find the governing equations and rediscovered — unsurprisingly — newton’s law of gravitation. we start by rediscovering what we already know, so we know that the machine approach works, then we try to discover what we do not know explains shirley ho, astrophysicist at the flatiron institute and one of the authors of the study. when they get there it’s going to be interesting, she adds.the use of symbolic regression allows the machine to produce results as mathematical expressions, which is a first attempt towards establishing a common language. this approach has gained attention in the past two years as a route to explainable ai-driven discovery. however, being able to read the latin alphabet yields little information if the words are written in a language one does not speak; in addition, the meaning of known words can be obscure without proper context. similarly, when a new equation, or additional terms to an established equation, are found, physicists need to find out what these mean.kim confesses her dream of overcoming our fundamental limitations, such as our intuition grounded in the physical world, with the help of machines. she hopes we can go beyond what one can already do through mathematics, which allows us to visualize and describe abstract objects beyond the intuition provided by our senses and operate at high levels of abstraction. yet, she acknowledges that if — or, for the optimist, when — ai helps us uncover a new scientific concept or paradigm, validating it will take a long time. the acceptance of new ideas in the scientific community is rarely straightforward. the history of science abounds in examples of forgotten or discredited theories, and of ideas that, being ahead of their time, had not been recognized and fully appreciated. an ai-generated theory will likely have a hard time gaining the trust and backing of the scientific community.ho thinks that when ai makes a prediction that contradicts our current understanding — as in the case of a human physicist proposing a new theory — the prediction will have to be tested experimentally and, in the case of ai, well outside the training set. funding experiments to test ai-generated theories will be a challenge in itself (not all human-generated theories get experimental tests either) so the case would have to be very strong. yet, if the prediction passes all the tests then, no matter how surprising or counter-intuitive, we will have to take it seriously. it would not be the first time.in the early 20th century, the development of quantum mechanics gave a mathematically elegant theory that subsequently passed all conceivable experimental tests. as reliable as quantum mechanics has proven to be, it has always been uncomfortable for those unsatisfied with the shut-up-and-calculate mindset. from the early days, quantum mechanics predicted baffling scenarios such as dead–alive cats and particles entangled via mysterious correlations. despite their beautiful mathematical formulation such concepts are far removed from our everyday experience of the world — much like crystals would be puzzling to anderson’s gaseous being. it’s therefore not surprising that they made no sense even to the brightest human minds and it took decades to become widely accepted. today, physicists are starting to move beyond the shut-up-and-calculate approach and use quantum mechanical concepts creatively even though the physical intuition of phenomena like entanglement still eludes us. researchers have learned how to use them to build new things and explore new possibilities: from quantum technologies to quantum-inspired classical algorithms and insights into theoretical high-energy and condensed-matter physics.when ai gives us alien insight into physics we may not recognize it immediately, and it will take time to fully appreciate its significance. but there is hope.anderson, p. w. more is different: broken symmetry and the nature of the hierarchical structure of science. science 177, 393–396 (1972).ads article google scholar schwartz, m. d. modern machine learning and particle physics. harvard data sci. rev. https://doi.org/10.1162/99608f92.beeb1183 (2021).article google scholar schrouff, j. et al. best of both worlds: local and global explanations with human-understandable concepts. preprint at https://arxiv.org/abs/2106.08641 (2021).kim, b. et al. neural networks trained on natural scenes exhibit gestalt closure. comput. brain behav. 4, 251–263 (2021).article google scholar liu, z. & tegmark, m. machine learning hidden symmetries. phys. rev. lett. 128, 180201 (2022).ads mathscinet article google scholar udrescu, s. m. & tegmark, m. ai feynman: a physics-inspired method for symbolic regression. sci. adv. 6, eaay2631 (2020).ads article google scholar greydanus, s. et al. hamiltonian neural networks. preprint at https://arxiv.org/abs/1906.01563 (2019)cranmer, m. et al lagrangian neural networks. preprint at https://arxiv.org/abs/2003.04630 (2020).liu, z. et al. machine-learning nonconservative dynamics for new-physics detection. phys. rev. e 104, 055302 (2021).ads article google scholar lemos, p. et al. rediscovering orbital mechanics with machine learning. preprint at https://arxiv.org/abs/2202.02306 (2022).download referencesnature reviews physics https://www.nature.com/natrevphys/iulia georgescuyou can also search for this author in pubmed google scholarcorrespondence to iulia georgescu.reprints and permissionsgeorgescu, i. how machines could teach physicists new scientific concepts. nat rev phys (2022). https://doi.org/10.1038/s42254-022-00497-5download citationpublished: 25 july 2022doi: https://doi.org/10.1038/s42254-022-00497-5anyone you share the following link with will be able to read this content:sorry, a shareable link is not currently available for this article. provided by the springer nature sharedit content-sharing initiative 