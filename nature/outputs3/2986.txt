deep learning models for sequential data can be trained to make accurate predictions from large biological datasets. new tools from computer vision and natural language processing can help us make these models interpretable to biologists.you have full access to this article via your institution.deep neural networks (dnns) excel at making predictions based on sequential data. these approaches can model intricate relationships between sequence elements, capture long-range dependencies between distant elements within a sequence, and learn how to map a sequence from one domain to another. given their versatility, it is no surprise that dnns have been successfully applied to various biological sequence prediction problems, such as predicting translation regulation1, transcription factor binding2 and protein structure3. despite their impressive accuracy, understanding how dnns make their predictions on sequential data in biological terms remains a challenging problem. writing in nature machine intelligence, linder et al.4 propose an approach to tackle this challenge, taking inspiration from ideas in computer vision and natural language processing. being able to unravel the underlying biological variables that influence a dnn when it issues a specific prediction instance, it becomes possible to distil insights into biological data and generate hypotheses about regulatory logic.linder et al.4 propose a model training procedure in which the model learns to ‘mask out’ irrelevant pieces of information in a biological sequence when making a prediction (fig. 1). this algorithm can operate on a diverse set of biological sequences and virtually any dnn model architecture. the authors illustrate how their algorithm can be used to interpret the effects of genetic variants, uncover nonlinear interactions between cis-regulatory elements, explain binding specificity for protein–protein interactions, and identify structural determinants of de novo designed proteins.the model4 comprises a scrambler network and a predictor network. the scrambler network learns which elements of a sequence to mask; the two networks are jointly trained to minimize a reconstruction loss function that reflects how well the masked sequences can be used to approximate the distribution of the original sequences. this loss function is regularized by the entropy of the masking patterns.there are many ways to ‘explain’ black-box machine learning models. traditionally, model interpretation has been a question of assessing the relative importance of all input variables. consider the coefficients of a simple linear regression model — the value of each coefficient serves as a measure of how influential the corresponding variable is on the model outputs. this approach provides ‘global’ explanations for the model predictions; that is, the relative importance of all input variables is assumed to be fixed for all prediction instances. the drawback of such approaches is that they provide very little explanatory power for complex and nonlinear dnns, in which the model may focus on a different subset of the input variables for every prediction instance. the algorithm proposed by linder et al.4 belongs to a different family of model interpretation approaches known as ‘feature attribution’5. these approaches aim to provide ‘local’ explanations for a machine learning model by marking the most influential input variables contributing to the predictions for each given instance. because predictions with biological sequences are highly contextual (that is, different segments within a sequence will be more relevant to the model prediction for different sequences), feature attribution is a sensible framework for explaining dnn models of biological sequences.the model interpretation algorithm developed by linder et al.4 uses two dnn models: a predictive dnn model (used to carry out the prediction task of interest) and an auxiliary dnn model known as ‘scrambler network’ (which is used to explain the predictions made by the first model using the feature attribution approach explained earlier). the two models are trained to work together so that the overall architecture issues both predictions and explanations (attributions) for every instance of a biological sequence. importantly, whereas existing feature attributions are optimized for continuous rather than discrete input data, the scrambler network learns attributions for discrete data by probabilistically sampling different sequence positions and masking them out before feeding the data into the predictive dnn model. the scrambler network learns the frequency by which it should mask different sequence positions for each input sequence, and these frequencies are used to measure the relative importance of the sequence elements.this mask-based feature attribution procedure is well suited for application to genomics and proteomics as it enables inference of relationships between discrete variables, which can be applied to problems such as interpreting genetic variants and uncovering the determinants of protein–protein interactions. in their paper, linder et al.4 demonstrate how the proposed scrambler network can be used to explain predictors of dimer binding and interpret predictions of protein structure detected by the trrosetta model6 — a dnn model that uses the primary sequence and corresponding multiple sequence alignment as input to predict three-dimensional atom–atom distances and backbone torsion angles.model interpretability is essential for fully capitalizing on the power of deep learning in applications pertaining to natural sciences. interpretations of model predictions are not only useful for ensuring model transparency and diagnosing model failures, but can also be used to distil scientific insights and induce new hypothesis. there are many approaches for interpreting model predictions — the feature attribution is particularly suited for rich scientific data in general, including biological sequences. the method proposed by linder et al.4 is one approach for feature attribution that can effectively handle high-dimensional discrete-valued biological sequences. beyond the applications discussed in their paper, there are exciting opportunities for using the algorithm to inform rational protein design and validate known design rules.sample, p. j. et al. nat. biotechnol. 37, 803–809 (2019).article google scholar alipanahi, b., delong, a., weirauch, m. t. & frey, b. j. nat. biotechnol. 33, 831–838 (2015).article google scholar senior, a. w. et al. nature 577, 706–710 (2020).article google scholar linder, j. et al. nat. mach. intell. 4, 41–54 (2022).article google scholar chen, j., song, l., wainwright, m. & jordan, m. int. conf. machine learning (883–892 (2018).yang, j. et al. proc. natl. acad. sci. usa 117, 4196–1503 (2020). google scholar download referencesbroad institute of mit and harvard, merkin building, cambridge, ma, usaahmed m. alaamassachusetts institute of technology, cambridge, ma, usaahmed m. alaayou can also search for this author in pubmed google scholarcorrespondence to ahmed m. alaa.the author declares no competing interests.reprints and permissionsalaa, a.m. mining for informative signals in biological sequences. nat mach intell (2022). https://doi.org/10.1038/s42256-022-00524-1download citationpublished: 03 august 2022doi: https://doi.org/10.1038/s42256-022-00524-1anyone you share the following link with will be able to read this content:sorry, a shareable link is not currently available for this article. provided by the springer nature sharedit content-sharing initiative 