constraining metabolic models by enzyme capacities greatly improves genotype–phenotype predictions. now, a method for estimating enzyme turnovers based on deep learning has been developed and used to reconstruct enzyme-constrained genome-scale metabolic models for more than 300 yeast species.you have full access to this article via your institution.genome-scale metabolic (gsm) models encode the complete metabolism of an organism through a list of gene-protein-reaction relations. ultimately, these relations help establish constraints on the magnitude, direction and catalytic resource needed for the network of metabolic reactions. flux balance analysis of gsm models (fig. 1a) aims to quantify the flow of metabolites through these networks under different environmental and genetic constraints1. however, such analysis does not account for the fact that the reaction flux of an enzyme inside a cell (in vivo) is ultimately constrained by its catalytic capacity, defined as the product of the intracellular concentration and the catalytic turnover rates (kcat) of the enzyme. imposing enzyme constraints in gsm models (fig. 1b) can be very informative for recapitulating important metabolic attributes such as maximum growth characteristics, metabolic shifts and proteome reallocations2 using flux balance analysis calculations. in the absence of enzyme capacity constraints, all reactions are allowed to reach their stoichiometrically allowable limits, without accounting for the trade-offs organisms need to establish under different physical and energy constraints.a, genome-scale metabolic (gsm) model formulated with mass and flux balance constraints. sij is the stoichiometry of the ith metabolite in the jth reaction, and vj is the metabolic flux of the jth reaction. lb and ub are the lower and upper bounds, respectively, for the reaction fluxes. b, enzyme-constrained gsm model formed by adding additional enzyme entries to the stoichiometric matrix using kcat values obtained from the dlkcat neural network model. for each enzyme, p, ep denotes its usage, and ep denotes its maximum possible usage. c, dlkcat uses the enzyme sequence and substrate structure as inputs and encodes them into mathematical representations using convolutional and graph neural networks, respectively. the final representations are combined using a fully connected layer to output kcat values. gpr, gene–protein–reaction.however, to date, genome-wide availability of in vivo kcat values is lacking, even for well-studied organisms. now, writing in nature catalysis, eduard j. kerkhoven and colleagues established a deep learning framework (dlkcat, fig. 1c) to estimate the kcat values of metabolic enzymes for any organism of choice2. the ability to predict kcat values (even with a certain level of approximation) can facilitate the automated construction of enzyme-constrained gsms, especially for non-model organisms.earlier efforts for constructing enzyme-constrained gsm models3 relied on tabulated values from databases such as brenda and sabio-rk. in most cases, organism-specific values were absent and thus were adopted from those of other organisms4. alternatively, heckmann et al5. trained regression models to predict enzyme turnover numbers from several hand-picked features spanning an enzyme’s network context and biochemical and structural properties. however, owing to the requirement of several detailed features, this method was only trained on a few hundred escherichia coli enzymes and hence had limited adaptability to other under-studied organisms. in dlkcat, kerkhoven and colleagues used a representation-based learning approach to automatically extract underlying features in the input data relevant for kcat prediction. in this approach, the enzymes’ amino acid sequences and the structures of their corresponding substrates are discretized into sets of sequence words and substrate substructures (fig. 1c), respectively, similar to a previously described procedure2.each discrete word and substructure hence created is assigned a unique vector of real numbers, called an embedding6. the mathematical representations for enzyme and substrate features are then obtained by combining the embeddings of the comprising words and sub-structures using a convolutional neural network and a graph neural network for each enzyme and substrate, respectively. obtained representations are then input into a fully connected neural network layer to predict kcat values. the resulting neural network architecture, dlkcat, was trained iteratively using the training data curated by collating all available kcat values in the brenda and sabio-rk databases summing up to 16,838 data points. during training, both the weights of the neural networks and the embeddings are updated to minimize the root mean squared error (r.m.s.e.) between the predicted and true kcat values for all enzyme–substrate pairs in the training data. the resulting model trained on this data accurately predicted kcat values with a r.m.s.e. of 1.06 in log10 scale when evaluated on an unseen test dataset (the evaluation gave a pearson’s correlation coefficient of r = 0.71). this is quite an achievement given that kcat values span a range of up to 10 to 14 orders of magnitude.however, it is important to stress that the predicted kcat values are only approximate in nature. given that the correlation agreement was achieved for a log-based scale, the predicted values could be off by an order of magnitude or more when establishing upper limits of reaction fluxes. the advantage, however, is that using this tool, one can readily impose constraints on almost all enzymes of gsm models. upon estimating a value for kcat, the stoichiometric matrix of the model is updated to yield constraints of the form ν ≤ kcat [e] for each enzyme-catalysed reaction (fig. 1b).as training data for the model are based on in vitro experiments, the researchers further developed a bayesian framework7 that can estimate in vivo-like kcat values starting from the dlkcat predicted kcat values by minimizing the distance between the predicted and experimentally observed growth data. the researchers put forth dlkcat and the bayesian framework, together, as an automated and convenient enzyme-constraint-based gsm (ecgsm) reconstruction pipeline, dl-ecgem. using dl-ecgem, the researchers demonstrated 80% coverage of enzymes for kcat values to define constraints for 90% of the enzymatic reactions for a set of 343 yeast/fungi species. in comparison, previous ecgsm models4 only covered kcat values for 40% of enzymes and generated constraints for 60% of annotated enzymatic reactions for the same set of species. in summary, dl-ecgem is a convenient pipeline for reconstructing enzyme-constrained gsm models with near-complete coverage for any genome-sequenced organisms.in addition to predicting kcat values, the current work showcases the advantage of attention weights derived from the neural network to identify regions of enzyme sequence that have the highest impact on catalytic activity. this capability can be further extended to identify specific amino acid residues that could potentially cause drastic changes in the kcat value of any given enzyme. amino acid residues identified as such can be modified to aid enzyme engineering efforts. recent applications in deep learning for structure generation, such as the alphafold2 (ref. 8), promise genome-wide prediction of protein structures at angstrom-level accuracy. although embeddings of the amino acid sequence words alone have proven to calculate accurate enzyme representations for kcat prediction, using structural features could potentially boost the model’s accuracy and interpretability, as demonstrated for similar applications9.one limitation of dlkcat is that it only incorporates the features of one primary substrate in the model, leaving cofactor/ion-dependent and/or non-unimolecular reactions approximately described. also, enzyme turnover numbers are known to vary by orders of magnitude owing to variations in the experimental conditions such as ph and temperature, which is beyond the scope of dlkcat. as in all such studies, there is always the potential danger of introducing systematic biases due to the over-representation of well-studied organisms in the training datasets. hence, any extrapolation of results for other organisms and enzymes needs to be cautiously executed.wang, h. et al. proc. natl acad. sci. usa 118, 30 (2021). google scholar li, f. et al. nat. catal. https://doi.org/10.1038/s41929-022-00798-z (2022).article google scholar chen, y. & nielsen, j. curr. opin. syst. biol. 25, 50–56 (2021).cas article google scholar domenzain, i. et al. nat. commun. 13, 3766 (2022).cas article google scholar heckmann, d. et al. nat. commun. 9, 5252 (2018).cas article google scholar tsubaki, m., tomii, k. & sese, j. bioinformatics 35, 309–318 (2019).cas article google scholar li, g. et al. nat. commun. 12, 190 (2021).cas article google scholar jumper, j. et al. nature 596, 583–489 (2021).cas article google scholar gligorijević, v. et al. nat. commun. 12, 3168 (2021).article google scholar download referencesdepartment of chemical engineering, the pennsylvania state university, university park, pa, usaveda sheersh boorla, vikas upadhyay & costas d. maranasyou can also search for this author in pubmed google scholaryou can also search for this author in pubmed google scholaryou can also search for this author in pubmed google scholarcorrespondence to costas d. maranas.the authors declare no competing interests.reprints and permissionsboorla, v.s., upadhyay, v. & maranas, c.d. ml helps predict enzyme turnover rates. nat catal 5, 655–657 (2022). https://doi.org/10.1038/s41929-022-00827-xdownload citationpublished: 19 august 2022issue date: august 2022doi: https://doi.org/10.1038/s41929-022-00827-xanyone you share the following link with will be able to read this content:sorry, a shareable link is not currently available for this article. provided by the springer nature sharedit content-sharing initiative 