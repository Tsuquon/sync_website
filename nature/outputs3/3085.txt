the problem of automatically determining state variables for physical systems is challenging, but essential in the modeling process of almost all scientific and engineering processes. a deep neural network-based approach is proposed to find state variables for systems whose data are given as video frames.you have full access to this article via your institution.the discovery of natural and physical laws from experimental data via the scientific method has challenged scientists for almost five centuries. this process has been rejuvenated by recent advances in machine learning alongside a tremendous increase in computing power, which together enable new avenues for scientific model discovery: advanced algorithms can find physical laws by sifting through large amounts of data1,2,3,4. once found, the governing equations describe functional relationships between physically meaningful variables. these algorithms are commonly fed labeled and measured data of physical variables in rather low dimensions. the preceding problem, namely to determine which variables are necessary to model a physical system, has seen much less progress, but is equally important for scientific discovery. in this issue of nature computational science, chen et al.5 present an approach to discover a set of variables that describes a physical process that is given in the form of a sequence of video frames.computational discovery of physical laws from data has a history going back to the 1970s when computers started to have the necessary processing power to implement somewhat intensive search and regression algorithms. the early work of langley and co-authors produced the bacon software1 that was able to determine physical laws — albeit from low-dimensional states — and to identify some functional relationships between independent and dependent variables. the identification process tested selective hypotheses, that is, it was looking for common heuristic laws such as constant and linear relationships, multiplicative and inverse correlations, and so forth. bacon was also able to add additional variables to an existing set, if it so discovered that a certain functional relationship existed. in 2009, schmidt and lipson2 proposed a genetic algorithm that would learn physical laws, including conservation laws of hamiltonian and lagrangian nature, from experimental data. their work would correctly identify many known physical laws and became inspirational to future algorithmic developments. however, the employed genetic algorithms could be unstable and quite sensitive to the data (and noise therein). moreover, they did not enforce what occam’s razor hypothesized: that natural laws often arise in a simple form. brunton, proctor and kutz4 used that very parsimonious principle to equip their symbolic regression algorithm with sparsity promoting regularization, which has been demonstrated to learn a variety of engineering and physical principles. recently, a multidimensional symbolic regression algorithm with additional physics-inspired heuristics4 correctly identified all 100 equations in the feynman lectures on physics. yet, despite all this progress, none of these approaches tackled the ‘upstream’ problem of identifying the proper physical variables to describe these systems. while alternative sets of physical variables to determine a dynamical system can be found manually6, algorithmic approaches are sorely needed.while other model identification methods typically rely on sensor data, which directly measure a specific variable, the proposed approach by chen et al. generalizes to any physical system and avoids expensive experimental setups. the key of the algorithm is to leverage the structure of deep neural networks, particularly of encoder–decoder type. the authors trained the neural network to correctly predict given video frames and then they use a strategy to find out how many internal neurons — which the authors call the intrinsic dimension — the network needs for accurate predictions. the intrinsic dimension informs the number of state variables needed, and after an additional network is leveraged, results in a set of variables to describe the evolution of the physical system. the authors demonstrated the applicability of their approach by successfully predicting system dynamics in a variety of dynamical systems including chaotic kinematics (rigid double pendulum, elastic double pendulum, swing stick), nonlinear waves (reaction–diffusion system), multi-phase flow (lava lamp), aeroelasticity (air dancer) and combustion (flame dynamics).more specifically, in the framework, chen et al.5 first fit a typical encoder–decoder architecture (ge and gd, as depicted in fig. 1a) to the rgb (red, green, blue) image data frames, to obtain a forward map that advances the current frame by one time step. in the next stage, they use the latent vector of this encoder–decoder architecture to estimate the intrinsic dimension (fig. 1b) — that is, the required number of state variables — of the system via the levina–bickel algorithm. once the number of state variables is estimated, the authors determine the actual variables to describe the system — they term them neural state variables — noting that those are not unique. this step of finding neural state variables is key to their algorithm as it effectively introduces another encoder–decoder architecture (he and hd, as depicted in fig. 1c) for the latent vectors and fixes the latent dimension of the second encoder–decoder network (fig. 1c) to the identified intrinsic dimension. this gives them a set of neural state variables, which they then advance through the system dynamics (fig. 1d) that were learned at the level of the rgb image data.a, the first step determines an encoder (ge)–decoder (gd) network that propagates the rgb images by one timestep. the latent vectors of the forward map are denoted lt→t+dt . b, the next step detects the intrinsic dimension (id), which is much smaller than the latent dimension (ld) and the input size (m), that is, m ≫ ld ≫ id. the intrinsic dimension is found through a nearest-neighbor-type argument in the nonlinear manifold of latent vectors l(i). c, a new encoder (he)–decoder (hd) network is fitted to the latent vectors l(i), where the internal dimension is fixed to id. the internal variables vk are thus interpreted as the ‘neural states’, that is, the minimal number of internal states needed to predict the model. d, the neural states are propagated through the learned dynamics \({\hat{f}}_v\).the identification of physical variables from data of dynamical systems is very challenging and has lacked automation and computationally tractable procedures. chen et al.5 present a framework that will help advance the field and close an important gap. however, it is important to note that the discovered variables do not have any units, and may not be interpreted as physical variables (such as pressure, temperature, and velocity) per se. at the moment, intuitive meaning must still be assigned by an application expert. it will be interesting to see their present work stimulate more research in this direction where, if possible, a complete set of physical variables with units could be learned from rgb data.bradshaw, g. f., langley, p. w. & simon, h. a. science 222, 971–975 (1983).article google scholar schmidt, m. & lipson, h. science 324, 81–85 (2009).article google scholar brunton, s. l., proctor, j. l. & kutz, j. n. proc. natl acad. sci. 113, 3932–3937 (2016).article google scholar udrescu, s.-m. & tegmark, m. sci. adv. 6, eaay2631 (2020).article google scholar chen, b. et al. nat. comput. sci. https://doi.org/10.1038/s43588-022-00281-6 (2022).article google scholar qian, e., kramer, b., peherstorfer, b. & willcox, k. physica d 406, 132401 (2020).mathscinet article google scholar download referencesdepartment of mechanical and aerospace engineering, university of california san diego, la jolla, ca, usaboris krameryou can also search for this author in pubmed google scholarcorrespondence to boris kramer.the author declares no competing interests.reprints and permissionskramer, b. learning state variables for physical systems. nat comput sci 2, 414–415 (2022). https://doi.org/10.1038/s43588-022-00283-4download citationpublished: 25 july 2022issue date: july 2022doi: https://doi.org/10.1038/s43588-022-00283-4anyone you share the following link with will be able to read this content:sorry, a shareable link is not currently available for this article. provided by the springer nature sharedit content-sharing initiative 