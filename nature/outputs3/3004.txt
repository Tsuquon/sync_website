a neuromorphic chip, key to the predicted ‘paradigm shift’ in computing performance.credit: seung hwan leesolid-state computing has had a long run since the 1950s, when transistors began replacing vacuum tubes as the key component of electronic circuits. generations of new solid-state devices that process and store information electronically at accelerated speeds came and went as germanium transistors were replaced by silicon transistors, followed by integrated circuits, then by increasingly complex chips filled with ever-higher counts of smaller transistors.since 1965, the industry has been guided by moore’s law — predictions made by gordon moore, co-founder of microprocessor giant intel — that ever-shrinking devices will lead to improved computing performance and energy efficiency. advances in nanotechnology have allowed the smallest features on today’s most advanced integrated circuits to be shrunk to an atomic scale, but this is incompatible with current devices. the next major step in computing not only requires new nanomaterials — it needs a new architecture. nature index 2022 nanoscience and nanotechnologycmos (complementary metal-oxide-semiconductor) transistors have been the standard building blocks for integrated circuits since the 1980s. cmos circuits, like generations of digital computers before them, rely on the fundamental architecture that john von neumann chose in the mid-twentieth century. his architecture was designed to separate the electronics that store data in computers from those that process digital information. the computer stored information in one place, then sent it to other circuits for processing. separating stored memory from the processor keeps the signals from interfering with each other and retains the accuracy needed for digital computing. however, the time spent moving data from memory to processors has become a bottleneck. developers are now seeking alternative non-von neumann architectures to perform calculations ‘within memory’ to avoid wasting time moving data around.another aim is to shift to neuromorphic systems, which use algorithms and network designs that emulate the high connectivity and parallel processing of the human brain. this means developing new artificial neurons and synapses that are compatible with electronic processing, but exceed the performance of cmos circuits, says chemical and materials science researcher, mark hersam. it’s no small feat, he adds, but would be well worth the cost. i’m more interested in neuromorphic computing than in-memory processing, since i believe that emulating the brain is a larger paradigm shift, with more potential upsides.the challenge, in both cases, is to identify the best technologies for the task, work that hersam is pursuing at northwestern university in evanston, illinois. in the nature index, which tracks articles in 82 selected natural-sciences journals, northwestern university is placed second in the united states for nano-related output, after the massachusetts institute of technology in cambridge.the first hints of a major change in computing emerged around 2012, as moore’s law began to stall out and developers of deep learning — where systems improve their performance based on past experience — realized that general-purpose central processing units (cpus) used in conventional computers could not meet their needs.the strength of cpus was their versatility, says wilfried haensch, who led a group developing concepts for computer memory at the ibm watson research center in yorktown heights, new york, until his retirement in 2020. whatever program you come up with, the cpu can execute it, says haensch. whether it can execute it efficiently is a different story.seeking better processors for deep learning, ibm developers turned to graphical processing units (gpus), designed to perform advanced mathematics used for high-speed, three-dimensional imaging in computer games. ibm found that gpus can run deep-learning algorithms much more efficiently than cpus, so the team hard-wired chips to run particular processes.in other machines, you load data and instructions, but in data-flow machines, certain instructions are hard-wired in the processor, so you don’t have to load the instructions, says haensch. this marked a break from the conventional von neumann model because data flowed through the hard-wired processor, as if operations were being performed in-memory. it also worked for the deep-learning algorithm, because about 80% of its operations used the same advanced mathematics as image processing.further fine-tuning of current materials only offers a short-term solution, says haensch. there are many new ideas, new devices and new nanostructures, he says, but none is ready to replace cmos. and there are no guarantees on whether, or when, they will be ready to deliver the transformation the industry needs.source: nature indexamong the most popular class of devices in development are memristors, which have both memory and electrical resistance. memristors resemble standard electrical resistors, but applying an electrical input to them can alter their resistance, changing what’s stored in memory. with three layers — two terminals that connect to other devices, separated by a storage layer — their structure allows them to store data and process information. the concept was proposed in 1971, but it was not until 2007 that r. stanley williams, a research scientist at hewlett-packard labs in palo alto, california, made the first thin-film solid-state memristor that was usable in a circuit.memristors can be made on a nanometre scale and can switch in less than a nanosecond. they have great potential for developing future computing systems past the von neumann and moore’s law eras, wei lu and his group at the university of michigan in ann arbor outlined in a 2018 review of memristor technology (m. a. zidan et al. nature electron. 1, 22–29; 2018). building a single system that combines all of the desired properties will not be easy.researchers are looking to new classes of materials to meet the needs of advanced computing. hersam and his colleague vinod k. sangwan, a materials science and engineering researcher at northwestern university, have catalogued an extensive list of potential neuromorphic electronic materials that includes zero-dimensional materials (quantum dots), one-dimensional and two-dimensional materials (graphene), and van der waals heterostructures (multiple two-dimensional layers of materials that adhere together) (v. k. sangwan and m. c. hersam nature nanotechnol. 15, 517–528; 2020).one-dimensional carbon nanotubes, for example, have attracted attention for their use in neuromorphic systems because they resemble the tubular axons through which nerve cells transmit electric signals in biological systems.opinions are divided on how these materials will factor into the future of computing. abu sebastian, the zurich-based technical leader of the ibm research ai hardware center in albany, new york, is focused on near-term gains, and sees opportunities to push further in both digital and neuromorphic computing.companies such as mythic [an artificial intelligence company based in austin, texas] are very close to commercialization, he says. on the research side, lu says there’s a lot to figure out. the complex computations adapted from imaging need to be made tighter and more accurate for neuromorphic computing to take full advantage, he says. haensch adds that there is so far no material to enable viable commercial production.intel and ibm, which is the leading corporate institution for nanoscience and nanotechnology-related output in the nature index, have large groups working on non-von neumann computing. hewlett-packard and paris-based artificial intelligence firm lights-on are among several companies that are focused on near-term applications.