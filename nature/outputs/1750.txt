two years ago, a new proteomic test was heralded as the future of cancer diagnostics. but since then, doubts about its effectiveness have begun to grow. erika check reports.seldom does a single piece of research prompt the us congress to pass a resolution urging continued funding to drive a new diagnostic test towards the clinic. but that's what happened in 2002, when the lancet published a paper1 claiming a breakthrough in the diagnosis of ovarian cancer.the paper described the use of mass spectrometry to analyse the pattern of proteins present in samples of blood serum. on the basis of these patterns, the test detected all the patients with ovarian cancers in a set of 50 samples, and falsely identified just three healthy patients as suffering from the disease from a total of 66 control samples.most encouragingly, the technique seemed to work well on patients with early-stage cancer — offering the prospect of earlier diagnosis, which improves the chances of successful treatment. the best current blood test, which relies on the detection of a single protein called ca125, misses at least half of patients in the earliest stages of the disease, and gives a high rate of false positives.the researchers, led by lance liotta and emanuel petricoin, who co-direct a proteomics programme run by the national cancer institute and the food and drug administration, based in bethesda, maryland, won immediate acclaim. in addition to the congressional resolution urging further funding for their research, the consumer magazine health named the test one of the top ten medical advances of the year.commercial rights to develop the test were quickly licensed from the us government to a company called correlogic systems, also based in bethesda, whose scientists collaborated with liotta and petricoin on the lancet paper. in november 2002, correlogic granted licences to two larger firms, quest diagnostics and the laboratory corporation of america, which are now hoping to market the test under the brand name ovacheck.but those plans could be thrown off track by reanalyses of liotta and petricoin's data by independent groups, which have raised serious doubts about ovacheck's reliability.these questions prompted the society of gynecologic oncologists to review all of the published work about ovacheck. on 7 february, the society declared that more research is needed to validate the test's effectiveness before offering it to the public. early warning critics warn that the episode illustrates the dangers of moving rapidly to the clinic with immature technologies such as those of proteomics. they say that scientists and regulators need to develop standards to ensure that such tests really work before they hit the market, because early detection is not without risks. women who get false positive results may undergo unnecessary surgery, and those who get false negatives may forgo further screening. early detection is not a benign undertaking, says martin mcintosh, who runs a cancer-detection effort at the fred hutchinson cancer research center in seattle, washington.the first criticisms of ovacheck hit the public domain in june 2003, when two biostatisticians at the university of maryland in baltimore, james sorace and min zhan, published a paper in the online journal bmc bioinformatics2. they had reanalysed a data set that liotta and petricoin's team posted online in august 2002. sorace and zhan similarly found numerous differences in the protein patterns that discriminated between the cancer patients and the healthy controls. the trouble, according to sorace and zahn, was that these looked more like experimental artefacts than real biological differences.the proteomics test relies on using gravity and electric fields to separate the proteins in a given sample. each protein is then given a number that represents the ratio of its charge and mass — called its m/z value. the test identifies patterns in these numbers to give a diagnosis. sorace and zahn were concerned because the most marked differences between the cancer patients and unaffected controls occurred for proteins with m/z values of less than 500. many spectrometry experts consider m/z values below 2,000 to be suspect, because they tend to include values generated by experimental artefacts or measurement error. to sorace and zahn, this suggested that the data for cancer patients and controls had been collected under such different conditions that it was impossible to reliably identify true biological differences.further questions emerged when biostatisticians led by keith baggerly of the md anderson cancer center in houston, texas, reanalysed the data in the lancet paper, plus two further data sets posted online. based on these data, we really can't tell if it's possible to use proteomics to separate normals from cancers, baggerly says.baggerly's first concern was that the values posted on the proteomics programme's website had already been processed in a way that made it impossible to reconstruct the raw data. but more specific concerns arose when his team analysed the overall characteristics of the data, looking at how closely the peaks for each of the proteins in the samples matched one another. each of the data sets was divided into three groups: cancer patients, unaffected controls and patients with benign tumours. in the first data set, the pattern of proteins detected in the benign-tumour group seemed very different from the patterns detected in both the cancer and normal groups. in contrast, the patterns generated by the cancer and normal groups looked highly similar to each other.furthermore, the patterns detected in the first benign-tumour group looked almost identical to the patterns detected in all three groups in the second data set — cancers, normals and patients with benign tumours. this indicated to baggerly that liotta and his colleagues inadvertently changed their experimental set-up midway through collecting the first data set — the one that they analysed for their lancet paper.although the second and third data sets seemed consistent with each other, the precise differences in protein patterns that separated cancers from controls in one set could not be used to make the same distinction in the other — which makes little sense if the test is uncovering fundamental biological differences between cancerous and control samples. baggerly's team also claimed that the data were collected on a machine that had not been properly calibrated, or adjusted for accuracy. in a paper first published online in january this year3, the researchers concluded that the test may be uncovering differences due to artifacts of sample processing, not to the underlying biology of cancer. divided over data baggerly says that he discussed his concerns privately with liotta and petricoin in december 2002. he decided to publish only after learning that a test might become commercially available.petricoin says there is no proof that m/z values below 2,000 always represent experimental noise or bias. he also denies that the team switched methods midway through the first experiment. and he explains that the second and third data sets were processed differently. the point is, the lancet paper shows feasibility of this approach, and the results derived from each of these data sets prove that there do indeed exist low-molecular-weight molecules in the circulation that can discriminate the disease states, says petricoin, who adds that the team has further refined its methods in a new paper4. he also notes that other groups have examined his data and supported his conclusions5.petricoin stresses that he and liotta are not directly involved with commercial development of the ovacheck test. peter levine, correlogic's chief executive, says that the company has also refined its data analysis techniques since the lancet paper. it seems to me a lot of people are sort of debating an issue that is pretty much of historical relevance only, he says. but correlogic has not released its data, so it is impossible to verify this claim.meanwhile, correlogic is now in dispute with liotta and petricoin over the pair's consulting work for a rival company, biospect of south san francisco. on 18 may, the two scientists were called before a congressional committee and asked whether this had slowed ovacheck's development — a charge that they roundly denied. growing pains but this dispute is secondary to the main issue of whether the technology works. sceptics want to see more evidence that it yields consistent results on samples from different labs. and some argue that the field needs to develop standards for how proteomics experiments should be done and reported.because the technology is so new, says epidemiologist david ransohoff of the university of north carolina in chapel hill, scientists are still learning how to cut out all the possible sources of bias in proteomics experiments. for instance, if the cancer samples are collected from women being tested because they are known to be at high risk of suffering from the disease, they might experience anxiety when sampled, unlike controls whose samples may be taken as part of a routine check-up. in this case, the first group of samples may be flooded with stress hormones that would be detected by a proteomic analysis, but have nothing to do with whether or not a woman has cancer.whether the test works or not, we will learn from this experience with ovacheck what rules of evidence we might apply in the future to find useful results more efficiently, says ransohoff.he hopes that the episode will lead to an established set of standards for evaluating the effectiveness of such tests. only then will we know whether proteomics-based diagnostic tools truly deserve the trust of scientists, doctors — and, most importantly, patients.→ http://ncifdaproteomics.competricoin, e. f. et al. lancet 359, 572–577 (2002).cas article google scholar sorace j. m., amp; zhan m., bmc bioinformatics 4, 24 (2003).article google scholar baggerly, k. a., morris, j. s. & coombes, k. r. bioinformatics 20, 777–785 (2004).cas article google scholar conrads, t. p. et al. endocr.-relat. cancer 11, 163–178 (2004).cas article google scholar zhu, w. et al. proc. natl acad. sci. usa 100, 14666–14671 (2003).ads mathscinet cas article google scholar download referencesyou can also search for this author in pubmed google scholarreprints and permissionscheck, e. running before we can walk?. nature 429, 496–497 (2004). https://doi.org/10.1038/429496adownload citationissue date: 03 june 2004doi: https://doi.org/10.1038/429496aanyone you share the following link with will be able to read this content:sorry, a shareable link is not currently available for this article. provided by the springer nature sharedit content-sharing initiative nature reviews nephrology (2015)diagnostic pathology (2011)pediatric nephrology (2010)