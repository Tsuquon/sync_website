upload your latest holiday photos to facebook, and there’s a chance they’ll end up stored in prineville, oregon, a small town where the firm has built three giant data centres and is planning two more. inside these vast factories, bigger than aircraft carriers, tens of thousands of circuit boards are racked row upon row, stretching down windowless halls so long that staff ride through the corridors on scooters.these huge buildings are the treasuries of the new industrial kings: the information traders. the five biggest global companies by market capitalization this year are currently apple, amazon, alphabet, microsoft and facebook, replacing titans such as shell and exxonmobil. although information factories might not spew out black smoke or grind greasy cogs, they are not bereft of environmental impact. as demand for internet and mobile-phone traffic skyrockets, the information industry could lead to an explosion in energy use (see ‘energy forecast’).source: ref. 1already, data centres use an estimated 200 terawatt hours (twh) each year. that is more than the national energy consumption of some countries, including iran, but half of the electricity used for transport worldwide, and just 1% of global electricity demand (see ‘energy scale’). data centres contribute around 0.3% to overall carbon emissions, whereas the information and communications technology (ict) ecosystem as a whole — under a sweeping definition that encompasses personal digital devices, mobile-phone networks and televisions — accounts for more than 2% of global emissions. that puts ict’s carbon footprint on a par with the aviation industry’s emissions from fuel. what could happen in the future is hard to forecast. but one of the most worrying models predicts that electricity use by ict could exceed 20% of the global total by the time a child born today reaches her teens, with data centres using more than one-third of that (see ‘energy forecast’)1. if the computationally intensive cryptocurrency bitcoin continues to grow, a sharp rise in energy demand could come sooner rather than later (see ‘the bitcoin bite’).for now, despite rising demand for data, ict’s electricity consumption is staying nearly flat, as increased internet traffic and data loads are countered by increased efficiencies — including shuttering older facilities in favour of ultra-efficient centres such as prineville’s. but those easy wins could end within a decade. the trend is good right now, but it’s questionable what it’s going to look like in 5–10 years, says dale sartor, who oversees the center of expertise for energy efficiency in data centers at the us department of energy’s lawrence berkeley national laboratory in berkeley, california.with the spectre of an energy-hungry future looming, scientists in academic labs and engineers at some of the world’s wealthiest companies are exploring ways to keep the industry’s environmental impact in check. they are streamlining computing processes, switching to renewables and investigating better ways to cool data centres and to recycle their waste heat. ict’s energy use must be vigilantly managed, says eric masanet, an engineer at northwestern university in evanston, illinois, who co-authored an international energy agency (iea) report2 last year on digitalization and energy — but if we stay on top of it, he says, we should keep future energy demand in check. perhaps the most startling forecast of ict’s future energy demand comes from anders andrae, who works on sustainable ict at huawei technologies sweden in kista; he predicts that data-centre electricity use is likely to increase about 15-fold by 2030, to 8% of projected global demand1. such dire numbers are controversial. there have been many alarmist predictions of growing ict energy use over the years, and all have proven to be bunk, says masanet. last year’s iea report estimated that although data-centre workloads will shoot up — tripling 2014 levels by 2020 — efficiency gains mean that their electricity demand might sneak up only by 3%2. ict’s carbon footprint as a whole might even drift downwards by 2020, as smartphones take over from larger devices, researchers have suggested3.since the cryptocurrency bitcoin was born in 2008, concerns have grown that the energy demand of its production will escalate rapidly. virtual coins are ‘minted’ by miners who buy specialized servers to crunch time-intensive computations in a growing blockchain that proves the validity of the new cryptocoins. by mid-2018, says alex de vries, a data consultant for international professional-services company pwc in amsterdam, bitcoin miners were probably using about 20 terawatt hours of electricity per year globally — less than 10% that of data centres, and less than 0.1% of total electricity use6. but estimates of how fast their usage is going up are contentious.de vries estimates that, by now, bitcoin is gobbling up at least 0.33% of global electricity. including other cryptocurrencies such as ethereum bumps that up to 0.5%. i think it’s shocking, he says. but others, including marc bevand, a cryptocurrency researcher in san diego, california, say that those numbers are inflated and based on gross assumptions. bevand estimates that energy use might be as low as half of de vries’ current numbers by january 2019. there is growth, but people are hyping this, says jonathan koomey, a california-based it consultant, who is gathering data on cryptocurrency electricity consumption.for now, bitcoin mining is only profitable in places where electricity is cheap (about half the global average, says bevand), which include china, iceland and areas along the columbia river in north america where hydroelectric power is plentiful. as bitcoin miners dig in to an area and stress the grid, energy companies respond by raising their fees. that might prompt miners either to shut down or to take steps to drastically improve the energy efficiency of their hardware or system cooling.bitcoin could possibly be migrated to a less-energy-intensive blockchain system, says bevand (as ethereum is planning). or, notes koomey, say bitcoin collapses for some reason; all those facilities will just go away.data-centre electricity demand has remained roughly level over the past half-decade, in part because of the ‘hyperscale shift’ — the rise of super-efficient information factories that use an organized, uniform computing architecture that easily scales up to hundreds of thousands of servers. hyperscale data centres emerged about a decade ago when companies such as amazon and google began to need fleets of a quarter of a million servers or more, says bill carter, chief technical officer at the open compute project. it was started by facebook in 2011 to share hardware and software solutions to make computing more energy-efficient. at that point, it made no sense to use off-the-shelf hardware from a computing firm, as companies had typically done.you had the opportunity to strip things down to just what you need, and make it specific to your application, carter says. the new hyperscalers made bare-bones servers designed for purpose. we stripped out video connectors, because there’s no video monitor. there’s no blinking lights because there’s no one walking the racks. there’s no screws, says carter. on average, one server in a hyperscale centre is said to be able to replace 3.75 servers in a conventional centre.information and communications technology accounts for more than 2% of global carbon emissions.credit: svteam/gettythe savings made by hyperscale centres can be seen in their power usage efficiency (pue), defined as the total energy needed for everything, including lights and cooling, divided by the energy used for computing (a pue of 1.0 would be a perfect score). conventional data centres typically have a pue of about 2.0; for hyperscale facilities, that’s been whittled down to about 1.2. google, for one, boasts a pue of 1.12 on average for all its centres.older or less technologically adept data centres can contain a mix of equipment that is hard to optimize — and some that is even useless. in 2017, jonathan koomey, a california-based consultant and leading international expert on it, surveyed with a colleague more than 16,000 servers tucked into corporate closets and basements and found that about one-quarter of them were zombies, sucking up power without doing any useful work — perhaps because someone simply forgot to turn them off. these are servers sitting around doing nothing except using electricity, and that’s outrageous, says koomey.in a 2016 report, the lawrence berkeley national laboratory estimated that if 80% of servers in small us data centres were moved over to hyperscale facilities, this would result in a 25% drop in energy use4. that move is under way. today, the world has around 400 hyperscale data centres, many of them mopping up services for small corporations or universities that in the past would have had their own servers. already they account for 20% of the world’s data-centre electricity usage. by 2020, hyperscale centres will account for almost half of it, the iea says (see ‘hyperscale shift’).source: ieaonce the hyperscalers have taken as much of the load as they can, further efficiencies will be harder to find. but corporations are trying. one emerging management technique is to make sure that servers are at full throttle as much of the time as possible, whereas others are turned off rather than being left idle. facebook invented a system called autoscale that reduces the number of servers that need to be on during low-traffic hours; in trials, this led to power savings of about 10–15%, the company reported in 2014.one big way the hyperscalers have whittled down their pue is by tackling cooling. in a conventional data centre, standard air conditioning can soak up 40% of the energy bill. the use of cooling towers, which evaporate water to drive the cooling of air, causes another environmental problem: us data centres are estimated to have used about 100 billion litres of water in 2014. getting rid of compression chillers and cooling towers helps to save both energy and water.one popular solution is to simply locate data centres in cool climes and blow the outside air into them. such centres don’t have to be in icy regions: prineville is cool enough to take advantage of so-called ‘free air cooling’, and so are many other data-centre locations, says ingmar meijer, a physicist at ibm research in zurich, switzerland.in this google-owned data centre in oregon, blue pipes supply cold water and red pipes return warm water to be cooled.credit: connie zhou/google/zumapiped water is an even better conductor of heat, allowing centres to be cooled using warm water, which is less energy intensive to produce and recycle in the cooling system. even in temperate climates, warm-water cooling has become the de facto solution for managing high-performance computers that are run fast and hot, including those in us department of energy labs and the bavarian academy of sciences’ supermuc supercomputer in garching, germany. commercial centres in warm climates sometimes also invest in these systems, such as ebay’s project mercury data centre in phoenix, arizona.for high-density, high-power computing, the most efficient thing to do is to immerse servers in a non-conductive oil or mineral bath. facebook trialled that in 2012 as a way to run its servers at higher speeds without overheating them. for now, immersion cooling is a specialist area with tricky maintenance, says meijer.in 2016, google turned its deepmind artificial intelligence (ai) research team to the task of tuning its data centre’s cooling system to match the weather and other factors. google says that, in tests, the team reduced its cooling energy bill by 40% and produced the lowest pue the site had ever seen. this august, the company announced it had turned cooling control at some data centres over to its ai algorithm.exploring innovative cooling solutions and making existing ones cheaper will become more important in coming years, says carter. as we connect the world, there’s areas that won’t be able to use free air cooling, he notes, pointing to africa and southern asia. and other developments will tax it infrastructure in new ways. if self-driving cars flood onto the roads, for example, small server installations at the base of mobile-phone towers, used to help those cars communicate and process data, will need high-power devices that can handle real-time ai workloads — and better cooling options. this year, the open compute project launched a project on advanced cooling, with the aim of making efficient cooling systems more accessible. the hyperscalers have it figured out; they’re extremely efficient, says carter. we’re trying to help the other guys.sources: iea/a. andrae/ref. 6hand-in-hand with better cooling is the idea of using the heat coming off the servers, thus saving electricity demand elsewhere. it’s like a free resource, says ibm researcher patrick ruch in zurich. there are a few examples: the condorcet data centre in paris sends its waste heat directly into a neighbouring climate change arboretum, where scientists study the impacts of high temperatures on vegetation. an ibm data centre in switzerland warms a nearby swimming pool. but heat doesn’t travel well, so waste-heat use tends to be limited to data centres located next to a convenient customer, or in a city that already uses piped hot water to heat homes.a scattering of players are aiming to make waste heat more generally usable, including preliminary efforts to turn it into electricity. others aim to use the waste heat to run cooling devices — ibm’s us$2-million project thrive, for instance, is developing new materials that can better soak up water vapour and release it when exposed to heat, to make more efficient ‘sorption heat pumps’ to keep data centres cool.at their core, data centres are only as good as the processors they’re made of — and there’s scope for improvement there, too. since the 1940s, the number of operations a computer can perform with each kilowatt hour (kwh) of energy has doubled about once every 1.6 years for peak performance and every 2.6 years for average performance. that’s a 10-billion-fold improvement over 50 years. by some measures, the rate of improvement has slowed since 2000, and the current generation of computing will run up against a physical barrier that limits the function of transistors in just a few decades, according to koomey’s calculations5.we’re up against the limits of shrinkage, says koomey. making comparable efficiency gains after that, he says, will require a revolution in how hardware is built and computing is done: perhaps through a switch to quantum computing. it’s basically impossible to predict, he says.although the core focus is on reducing ict’s energy usage, it’s worth remembering that the information industry could also make our power use elsewhere smarter and more efficient. the iea notes that if all vehicles become automated, for example, there is a utopian possibility that smoother traffic flow and easier carpooling would reduce the total energy demand of the transport industry by 60%. buildings, which have accounted for 60% of the increased global electricity demand over the past 25 years, have huge scope for energy-efficiency improvements: smart heating and cooling, hooked up to building sensors and weather reports, could save 10% of their future energy demand. chiara venturini, director of the global e-sustainability initiative, a brussels-based industry association, estimates that the it industry currently abates 1.5 times its own carbon footprint, and that could go up to almost 10 times by 2030.source: ieaict might also help to reduce global emissions by giving renewables a leg-up over fossil fuels. in 2010, the environmental group greenpeace published its first clickclean report, which ranked major companies and threw a spotlight on the environmental burden of it. in 2011, facebook made a commitment to using 100% renewable energy. google and apple followed in 2012. as of 2017, nearly 20 internet companies had done the same. (chinese internet giants such as baidu, tencent and alibaba, however, have not followed suit.) back in 2010, it companies were a negligible contributor to renewable-power purchase agreements with energy companies; by 2015, they accounted for more than half of such agreements (see ‘green growth’). google is the largest corporate purchaser of renewable energy on the planet.cutting back our thirst for data might be the ultimate way to prevent energy use going into hyperdrive. but it’s hard to see anyone agreeing to, say, limit their netflix use, which accounts for more than one-third of internet traffic in the united states. banning high-definition colour cameras on phones alone could reduce data traffic in europe by 40%, says ian bitterlin, a consulting engineer and data-centre expert in cheltenham, uk. but, he adds, no one seems likely to dare to institute such rules. we can’t put the lid back on pandora’s box, he says. but we could reduce data-centre power.