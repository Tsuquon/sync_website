the design of protein sequences that can precisely fold into pre-specified 3d structures is a challenging task. a recently proposed deep-learning algorithm improves such designs when compared with traditional, physics-based protein design approaches.you have full access to this article via your institution.de novo protein design —the creation of proteins tailored to functional goals without using natural proteins as a starting point — has broad potential to create new drugs and catalysts. however, it requires solving, among other challenges, the ‘sequence design’ problem: given a protein structure (a ‘backbone’), the goal is to find a sequence of amino acids (aas) that will fold into that structure. conventionally, this problem (fig. 1a) is addressed by monte carlo optimization of a physical and/or statistical energy function1 (fig. 1b,c). this optimization process can be computationally expensive, and sometimes inaccurate, because the energy has to be approximated as contributions from individual residues and pairs of residues, ignoring higher-order interactions, for the sake of computational tractability. as such, the ‘native sequence recovery’ of such methods — that is, the fraction of identical aas between a sequence designed from a natural protein backbone and the original natural sequence — rarely exceeds ~30%. a wave of recent deep-learning sequence-design methods2,3,4 have improved native sequence recovery to 35–40% by leveraging the ability of neural networks to model the nonlinear dependence of aa preferences on structural context (fig. 1c). writing in nature computational science, liu et al. present5 a deep-learning sequence designer, abacus-r (‘a backbone-based amino acid usage survey’), combining insights from multiple recent works2,3 to achieve 45% native sequence recovery on test sets. more importantly, they solve experimentally the structures of abacus-r-designed proteins and show that the experimental structures have sub-ångström root-mean-square deviation (rmsd) agreement with the design models.a, visual representation of the protein sequence design problem. the structure on the left is a backbone-only representation of a protein and can potentially be realized with many different sequences. the goal is to choose one particular sequence that will fold into this structure as accurately as possible in the wet lab. b, schematic of a subset of the energy terms in the rosetta energy function, which is widely used for protein sequence design1. eelec: electrostatic energy term, based on coulomb’s law; erotamer: sidechain rotamer energy, based on statistics from natural protein structures; ebbtorsion: backbone torsion angle energy, also based on statistics; ehbond: hydrogen-bonding energy, based on a simplified physical model. c, monte carlo sequence design using an energy function. on each step, a new sidechain conformation or identity is proposed and accepted or rejected based on its energy in the context of the entire structure. this requires a full-atom representation of the protein and millions of steps to converge to a low-energy sequence. d, neural-network based sequence design in abacus-r. a residue position (blue circle) is designed by predicting the distribution over amino acids (aas, represented by each of 20 letters) at that position, based on the geometry and aas of its neighbors, and then sampling a specific aa from the predicted distribution (p(aa)). abacus-r performs this design procedure in parallel at more than one residue position at a time. this does not require explicit representation of the side-chain atoms or conformations, and only requires a few hundred iterations for most proteins. although individual iterations are slower with neural networks than with energy functions, design runs are still overall 100–1,000 times faster.abacus-r is trained on the task of predicting the aa at a given residue, using information about that residue’s backbone structure, and the backbone and aa of neighboring residues in space. to do this, abacus-r uses the transformer neural network architecture6, which offers flexibility in representing and integrating information between different residues. although these aspects are similar to a previous network2, abacus-r adds auxiliary training tasks, such as predicting secondary structures, solvent exposure and sidechain torsion angles. these outputs aren’t needed during design but help with training and increase sequence recovery by about 6%. to design a protein sequence, abacus-r uses an iterative ‘denoising’ process (fig. 1d): first, aa probabilities are predicted for a randomly chosen residue; then, a particular aa is sampled from the predicted probability distribution and ‘fixed’ at that site, and another position is decoded given the backbone and the previously fixed positions; this process continues until the overall sequence probability is maximized. in principle, this process allows sub-optimal aa choices made in earlier steps to be ‘corrected’ by later steps. to increase computational efficiency, abacus-r also decodes multiple positions in parallel at each step. compared with previous works, abacus-r is the first to use a transformer architecture with a denoising sequence-design procedure. with the above technical innovations, abacus-r can achieve a native sequence recovery of 37.7% and 44.6% on two benchmark sets, which makes it one of the best published sequence designers.in silico benchmarks for protein design tasks are inherently limited as they can vary greatly depending on the details of dataset curation. therefore, one of the most important contributions of liu et al. is their rigorous experimental testing in the wet lab of whether the designed sequences fold into a stable protein with the intended structures. a common failure mode of de-novo-designed sequences is poor expression and unintended aggregation. on these measures, abacus-r is successful, with 49 of 57 tested sequences expressing as (non-aggregated) monomers in escherichia coli. previously, one deep-learning sequence-design method had been validated by x-ray crystal structure, but it did not agree closely with the design model until a manual mutation was introduced3. liu et al.5 provide multiple new crystal structures of their designs, all within 1 å rmsd to the original native backbone. this structural validation, which is laborious and rarely done for other sequence-design methods, convincingly indicates that the designed sequences are highly optimized to fold into the intended structure. interestingly, several of the crystal structures exhibited non-native polar networks, which are very difficult to design using energy-based methods1. a controllable version of this ability would benefit important design applications such as protein–protein interactions and molecular logic7.despite these successes, current deep-learning-based sequence designers have several limitations that warrant future research. first, they cannot model non-protein ligands, and poorly predict side-chain conformations; these are needed to design proteins for catalysis, small-molecule binding, or dna binding. second, current methods all assume a fixed backbone structure. although small perturbations of input backbone structures are tolerated (and even beneficial during training), the networks cannot generate good sequences with poor inputs, which leads to low success rates on some de novo backbone structures. a network that can improve the backbone while designing the sequence would further increase success rate and robustness. existing methods for this ‘flexible-backbone sequence design’, based on energy functions1 or ‘inverted’ structure-prediction networks8, have relatively poor performance. as a result, modified versions of a structure-to-sequence network like abacus-r may provide a potential alternative solution.leman, j. k. et al. nat. methods 17, 665–680 (2020).mathscinet article google scholar ingraham, j., garg, v. k., barzilay, r. & jaakkola, t. generative models for graph-based protein design. in 33rd conference on neural information processing systems (neurips 2019) 15741–15752 (curran associates, 2020).anand, n. et al. nat. commun. 13, 746 (2022).article google scholar strokach, a., becerra, d., corbi-verge, c., perez-riba, a. & kim, p. m. cell syst. 11, 402–411.e4 (2020).article google scholar liu, y. et al. nat. comput. sci. https://doi.org/10.1038/s43588-022-00273-6 (2022).article google scholar vaswani, a. et al. preprint at https://arxiv.org/abs/1706.03762 (2017).boyken, s. e. et al. science 352, 680–687 (2016).article google scholar norn, c. et al. proc. natl acad. sci. usa 118, e2017228118 (2021).article google scholar download referencesinstitute of protein design, university of washington, seattle, wa, usajue wangyou can also search for this author in pubmed google scholarcorrespondence to jue wang.the author declares no competing interests.reprints and permissionswang, j. protein sequence design by deep learning. nat comput sci 2, 416–417 (2022). https://doi.org/10.1038/s43588-022-00274-5download citationpublished: 21 july 2022issue date: july 2022doi: https://doi.org/10.1038/s43588-022-00274-5anyone you share the following link with will be able to read this content:sorry, a shareable link is not currently available for this article. provided by the springer nature sharedit content-sharing initiative 