prognostic information for patients with ovarian cancer is captured in clinico-genomic data, histopathology slides and computed tomography imaging; however, how to integrate these data is unclear. a study now presents a method for combining complementary data types to stratify risk and aid treatment selection in patients with ovarian cancer.you have full access to this article via your institution.genomic data, histology, radiology and clinical features all have independent relevance for clinical decision-making and are thus collected throughout the diagnosis and treatment of cancer. despite the ubiquitous clinical usage of these data, recent studies have demonstrated that the implementation of deep-learning methods can reveal additional prognostic information encoded in these modalities. more specifically, deep learning has been used to identify patients with distinct genomic and clinical features based on hematoxylin and eosin (h&e)-stained slides1,2 and computed tomography (ct) imaging alone3,4. what were once thought to be non-intersectional data modalities with a narrow purview are now understood to be different lenses for viewing a patient’s cancer, highlighting overlapping facets of the disease based on the data type. although previous work has demonstrated some degree of synergy in the information from genomic, histological and radiological data, the degree to which each data type offers unique, clinically relevant information remains unknown. in addition, there is no consensus on the optimal architecture for learning prognostic features from multi-modal data. in this issue of nature cancer, boehm et al.5 used a late-fusion machine learning model to integrate genomic, histopathological, radiomic and clinical data into a multi-modal model trained to stratify patients with high-grade serous ovarian cancer (hgsoc) by risk. their framework demonstrated that the features extracted from histological and radiological data contained complementary, rather than mutual, information to genomic and clinical data.deep learning has proven to be a valuable tool for extracting complex features from high-dimensional clinical data. convolutional neural networks are a deep-learning method that are well suited for identifying features in images and have therefore been applied to scanned h&e slides and ct images to learn both genomic features, such as microsatellite instability in colon1 and endometrial cancer3, as well as prognosis in lung cancer4. multi-modality fusion models offer an extension to these image classification models by combining other data types alongside pixel data from images into a feature vector6. this multi-modal feature vector can be fed into a machine learning model at various stages. in early fusion, the data is fused into a multi-modal feature at the input level, so the model learns to jointly handle the datatypes together (fig. 1a). by contrast, late fusion models leverage the predictions from multiple single-modal models to make a final decision (fig. 1b). this late addition enables greater interpretation of the contribution of each data modality.a, early fusion models combine multi-modal data into a single feature vector, which is then fed into a model to learn to stratify risk. glszm–sae, gray level size zone–small area emphasis; g, genomic; glrlm-glv, gray level run length matrix gray level variance; h, histopathological; hrp, homologous recombination proficient; r, radiomic. b, late fusion models learn to stratify single-modal data before the partial hazard are combined for multi-modal risk prediction. c, example patient dashboard for conveying model results to clinicians.to study the association of genomic, histological, radiological and clinical features with overall survival in hgsoc, boehm et al. assembled a multimodal dataset consisting of 444 patients. of these, 296 were treated at memorial sloan kettering cancer center (mskcc) and 148 were collected as a part of the cancer genome atlas (tcga-ov). these patients primarily had late-stage disease and were randomly split 404 to 40 for training and testing respectively. a subset of these patients had digitally scanned h&e-stained slides, pretreatment ct imaging of adnexal lesions or omental implants, treatment regimens noted, or genomic sequencing. these data types were further refined into distinct vectors of features, for example, homologous recombination deficiency (hrd) status was inferred from genomic sequencing, and tissue-type and cell-type features were identified by applying deep-learning methods to digital h&e slides.the interpretable, single-modality feature vectors were then fed into a cox model. this allowed the authors to identify features associated with shorter overall survival. among these features, the authors identified nuclear size in h&e images as a prognostic marker, which could be associated with events such as whole-genome doubling and warrants further investigation. in addition, radiological features corresponding to omental implant appearance in ct imaging, as well as other important genomic and clinical features, such as residual disease status after debulking surgery, and parp inhibitor maintenance therapy were found to be prognostic (fig. 1). these features were combined in a late-fusion model by taking each patient’s negative partial hazard from the single modality cox model, and then training a final multivariate cox model to integrate these inferred partial hazards.this study expanded on previous findings4,7,8,9, showing that combining features from h&e and ct images (termed radiomic–histopathological (rh) model) outperformed more traditional biomarkers, such as the hrd status-based model, as well as clinical and individual imaging models. in addition, the model that combined genomic, histological and radiological features (termed grh) was similar in performance to models including only histological and radiological features, but improved the separation of risk groups, in line with previous work1,2,3 that identified genomic features in these data types. despite the benefit of adding genomic information, combining all of the data modalities together corresponded with a decrease in overall performance, as did shrinking the dataset to include only patients with full information. the full ghrc model (including clinical information) performed worse than the rh and grh models, highlighting that multimodality is not guaranteeing improved performance.boehm et al. demonstrated that features from different data modalities selected distinct patient subgroups as those most likely to have a greater overall survival, highlighting the complementary information encoded into each data type. with the advance of targeted therapies, the identification of biomarkers to stratify patient subpopulations according to therapy success is becoming essential10. radiological and histological images are collected as part of the standard of care across a wide range of malignancies and the development of tools that fully exploit the information encoded in these ubiquitous data types is crucial for the future of precision medicine.another important step for clinical integration of machine learning tools is the visualization of the output for interpretation by clinicians, to inform action. late-fusion multi-modal models provide an opportunity to critically evaluate the overall conclusions in light of single-modality results. however, for clinicians to incorporate this information into clinical decision-making, an intuitive interface for visualizing model results is necessary. this could for example be achieved via the development of a patient dashboard that might include results from each single data modality, how much a data type influenced the final prediction, single-modality predictive features per patient and how these feature values fall into a broader distribution, or model uncertainty (fig. 1c). conveying these type of model results and limitations to clinicians is a critical step in the practical implementation of machine learning tools because it increases transparency, establishes trust where the model reinforces the clinician’s decision, and enables clinical contextualization for complex decisions.in conclusion, the identification of single-modality features associated with overall survival allowed the authors of this study to reduce their feature-space, which increased interpretability, but also constrained their multi-modal results. moreover, features across data types that are not significant on their own may be informative for predicting risk when combined. although the work by boehm et al. represents a step forward in both understanding the information encoded in clinical data and in practically combining data from multiple modalities, further work is needed to understand the relationships between features and when a combinatorial approach is warranted. nevertheless, the work presented here provides a strategy for combining multi-modal data to stratify patients with hgsoc by risk and thereby contributes to a growing body of work proposing that radiological and histological images contain rich information about a patient’s disease. further research using multi-modal machine learning models may lead to a deeper understanding of what within these data types is indicative of clinical outcomes and of how to best integrate multi-modal clinical data to improve precision medicine. in this context, the study of boehm et al. fuses complementary histopathological, radiological and clinical-genomic information and thereby provides a framework for the integration of multi-modal data that could pave the way for clinical application.kather, j. n. et al. nat. med. 25, 1054–1056 (2019).cas article google scholar kather, j. n. et al. nat. can. 1, 789–799 (2020).cas article google scholar veeraraghavan, h. et al. sci. rep. 10, 1–10 (2020).article google scholar mukherjee, p. et al. nat. mach. intell. 2, 274–282 (2020).article google scholar boehm, k. m., aherne, e. a., ellenson, l., soslow, r. & nikolovski, i. nat. can. https://doi.org/10.1038/s43018-022-00388-9 (2022).article google scholar huang, s. c., pareek, a., seyyedi, s. & banerjee, i. & lungren. m. p. digit. med. 3, 136 (2020).article google scholar wulczyn, e. et al. plos one 15, 1–18 (2020).article google scholar hosny, a. et al. plos med. 15, 1–25 (2018).article google scholar dercle, l. et al. jama oncol. 8, 385–392 (2022).article google scholar paul, s. m. et al. nat. rev. drug discov. 9, 203–214 (2010).cas article google scholar download referencesbiological sciences division, university of chicago, chicago, il, usahanna m. hieromnimon & alexander t. pearsonuniversity of chicago comprehensive cancer center, university of chicago, chicago, il, usaalexander t. pearsonyou can also search for this author in pubmed google scholaryou can also search for this author in pubmed google scholarcorrespondence to alexander t. pearson.a.t.p. serves on advisory boards for ayala, elevar and prelude therapeutics; has consulting roles for abbvie; and reports research funding from abbvie and kura oncology. a.t.p. reports effort support via grants from nih/nci u01-ca243075, nih/nidcr r56-de030958, nih/nci r25-ca240134, eu horizon 2021-sc1-bhc, su2c (stand up to cancer) fanconi anemia research fund – farrah fawcett foundation head and neck cancer research team grant during the conduct of the study. this has been funded in whole or in part with federal funding by the nci-doe collaboration established by the us department of energy (doe) and the national cancer institute (nci) of the national institutes of health (nih), cancer moonshot task order no. 75n91019f00134 and under frederick national laboratory for cancer research contract 75n91019d00024. this work was performed under the auspices of the us doe by argonne national laboratory under contract de-ac02-06-ch11357.reprints and permissionshieromnimon, h.m., pearson, a.t. ovarian cancer through a multi-modal lens. nat cancer 3, 662–664 (2022). https://doi.org/10.1038/s43018-022-00397-8download citationpublished: 28 june 2022issue date: june 2022doi: https://doi.org/10.1038/s43018-022-00397-8anyone you share the following link with will be able to read this content:sorry, a shareable link is not currently available for this article. provided by the springer nature sharedit content-sharing initiative 