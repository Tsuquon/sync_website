a new generation of sophisticated earth models is gearing up for its first major test. but added complexity may lead to greater uncertainty about the future climate, finds olive heffernan.you have full access to this article via your institution.the government building in the south of england looks open and airy with its three-storey glass facade. but security measures such as guards stationed at the front serve as a reminder that this ministry of defence property is carrying out sensitive work important to the nation's future. deep in the building's basement, two adjoining rooms house 27 big black boxes that churn through a million lines of computer code every hour to gaze into the future of earth and its 7 billion inhabitants.this massive supercomputer at the uk met office in exeter is home to what is possibly the world's most sophisticated climate model. developed by researchers at the hadley centre, the met office's climate-change branch, the newly finished model will be put to its first big test over the coming months. it will run a series of climate simulations out to the year 2100 for the next report of the intergovernmental panel on climate change (ipcc), on the physical-science basis of climate change, which is due out in 2013.four years in the making, the model is known as hadgem2-es, short for the hadley centre global environmental model, version two, with an added earth-system component. it is one of a dozen earth-system models under development worldwide that reach far beyond their distant forebears, which represented just the physical elements of the climate, such as air, sunlight and water. the new generation includes all that and much more: forests that can shrink or spread as conditions change; marine food webs that react as the oceans grow more acidic with carbon dioxide; aerosol particles in the atmosphere that interact with greenhouse gases, enhancing or sapping their warming power.the hadley centre is at the forefront of efforts around the world to develop such complex climate models. "it's really pushing the envelope", says andrew weaver, a climate modeller at the university of victoria in british columbia, canada.researchers hope that the added complexity will lead to more realistic predictions and help them to derive new insights about how elements of the climate interact with each other. but it is a bit of a gamble whether this monumental effort will actually help political leaders and researchers to plan for the future. because models are simulating more components and each one is subject to variation, the extra complexity could make error bars expand greatly in forecasts of how temperatures or precipitation will change with time. "it's very likely that the generation of models that will be assessed for the next ipcc report will have a wider spread of possible climate outcomes as we move into the future," says jim hurrell, who is heading development of the earth-system model at the national center for atmospheric research (ncar) in boulder, colorado.at its core, hadgem2-es is much like any climate model. it is a series of equations describing atmospheric circulation and thermodynamics, solved for a number of points that form a three-dimensional grid over the surface of earth.the grid for hadgem2-es has 192 evenly spaced points running east–west and 144 points in the north–south direction. each box represents an area of roughly one degree latitude by two degrees longitude, which is about 100 kilometres by 200 kilometres at the equator. the atmosphere has 38 unequally divided levels, about 20 metres deep near the land and ocean surface but getting progressively thicker with height.  from slime to leaf  as with all modern global climate models, hadgem2-es couples the atmospheric model with an ocean model. in the 1980s and early 1990s, the oceans in the earliest of the hadley coupled models were called swamps and were little more than large, shallow puddles connecting continents. they gradually evolved into 'slab oceans', which were rigid but could absorb and release heat (see 'model evolution'). the global ocean in hadgem2-es, by contrast, flows with currents and eddies. its 40 layers reach down to 5,000 metres, with the top layer measuring just 10 metres thick, an important advance that allows models to simulate more realistically how the ocean takes up and retains carbon dioxide at different depths.amid the stark white walls and uniform desks of the hadley centre, researchers are wrestling with how to mimic the clouds and currents, trees and tundra and the myriad other aspects of the planet that can amplify or diminish global warming. yet the only signs of the work that is done here are the lines of computer code scrolling across the computer monitors.the coding that makes up hadgem2-es includes several other components of the earth-system, such as a closed carbon cycle, which in theory can account for all of the carbon on earth that might affect climate over the next few centuries. the model does this with detailed atmospheric chemistry, dynamic vegetation that can grow and die depending on the climate, and ocean biological and geochemical systems that can respond independently to changing greenhouse-gas levels.on land, vegetation is split into nine categories from shrubs to needle-leaf trees — a major advance in how models represent the biosphere. the first attempts to include biology in a climate model gave rise to the 'green scum' model, which treated all of life as a uniform green layer from equator to pole. hadgem1, the immediate forerunner of hadgem2-es, included the same types of vegetation as hadgem2-es but as a static layer. now, the vegetation types can shift, and foliage can change with the seasonal cycle.merging these aspects together in a single model slows it considerably. simulating one month takes an hour of computing time. "that's the price you pay," says chris jones, a carbon-cycle specialist who has been instrumental in developing hadgem2-es. hadgem1 runs three times faster than the newer model.even with the fine resolution of hadgem2-es, there are climatically important processes that are simply too small for the model to simulate directly, such as clouds. to get around this, climate modellers use a technique called parameterization. rather than trying to simulate individual clouds within a grid box, for example, the generalized effect of clouds on climate is represented by a series of equations that describe the conditions under which clouds form and decay, and how they absorb or reflect radiation. these are then averaged for each grid box.parameterizations are a necessary evil that can introduce error into models. perhaps more bothersome, though, are the random errors. just one incorrectly entered bit of data can send the whole system haywire. so too can an imperfect representation of a seemingly simple aspect of the real world, as hadley centre scientists discovered when they put too little plant life in the world's arid regions, leaving insufficient vegetation to hold the soil in place. soon, dust levels in the model atmosphere surged out of control, reaching three times higher than average, which in turn fertilized the ocean, causing phytoplankton to flourish.  identifying culprits  model developers are only too familiar with the effects of errors. "that's not only common, that's the evolution that all of us go through," says hurrell. to identify these quirks, each model goes through a rigorous testing phase. this involves putting the model through a series of 'control runs' in which its skill is tested by simulating a stable climate. the model is also commonly 'hindcast' to determine its ability to match historical changes. that's what hadgem2-es is currently doing and will finish up next month."sometimes, you can get completely unrealistic results," says jones. when the results don't correlate with reality, modellers use their knowledge of the climate system to try to identify the likely culprits within the model and then correct them. and occasionally what starts as a problem can lead to insights. the unrealistic dust storms in hadgem2-es, for example, revealed that even a small amount of plant life in the world's deserts can influence the whole climate.once in a while you have to stop and think if this complexity is really warranted in light of our ignorance. , the testing phase usually gets the model to a point at which it can simulate past and current climate well, but often certain aspects continue to cause problems. in fact, every model has a weak spot. when the hadley centre scientists developed hadgem1, the model had difficulty simulating the el niño southern oscillation, a phenomenon that drives much of the climate variability across the pacific ocean. "this led many to harp on that the hadley centre had lost its footing at the lead of the pack," says bill collins, the scientist responsible for the hadgem2-es model development. but the model used by ncar scientists had an equally bad problem with el niño. and ronald stouffer, a climate researcher at the geophysical fluid dynamics laboratory (gdfl) in princeton, new jersey, says the gdfl model was "the worst in the world" for simulating north pacific sea ice in the last ipcc assessment.but when it came to reproducing past and current climates on a global scale, these models were on top of their game. "that's one of the joys of this business," stouffer says with a laugh. "you can make your model better overall but at any one point it might be worse." whether or not hadgem2-es has a blind spot should become apparent after running the simulations for the ipcc over the next few months.whatever flaws there are in hadgem2-es may be compensated for by its peers — the other earth-system models that will be running simulations for the ipcc forecasts. these models hail from the research groups at the gfdl, ncar and from centres in australia, canada, china, france, germany, japan and norway. "it is important that there's a good diversity of models," says weaver, because, when it comes to ways of modelling, "there's no one right answer". the advantage of having numerous groups developing models simultaneously is that they each approach the task differently.other institutes have not put as much work into their earth-system models as the hadley centre, so most of them are not relying entirely on the new models. "every one of these simulations is a challenge and we don't know until the last moment whether everything will work out as we would like," says christian reick, who is leading vegetation modelling for the max planck institute for meteorology in hamburg, germany. many groups will run a hierarchy of simpler models as a way of hedging their bets. if the earth-system models produce forecasts of, say, temperature change that have an unreasonably large uncertainty range, the teams can fall back on more familiar models for a simpler evaluation of what the future holds. that concern has led some to call for caution in increasing the complexity of models. "once in a while you have to stop and think if this complexity is really warranted in light of our ignorance," says syukuro manabe, one of the founders of modern climate models and a researcher at the gfdl. "you don't want to oversimplify," says manabe, adding that the key is to look at whether "the detail in the model is balanced with our knowledge of the process".  practical constraints  although the new models are better from an academic perspective, they do not necessarily produce results that are more useful for policy-makers wrestling with how to plan for the future. jones acknowledges that models are "only useful if we understand them to the point where we have confidence in them". but, he says, "we're going into a future that we can't constrain with observations, and climate models are the only tool we have for making projections".during the report-writing process, the ipcc authors will analyse all the model projections to develop their best estimates for the climate of the future. the estimates will include how quickly temperatures will rise, where rainfall will increase or decrease and how vegetation patterns will shift.but even as they run through the ipcc simulations with the current crop of models (see 'the range of future climates'), jones and his peers are thinking about how to improve them by adding more components. "we're keen that we don't stand still," he says. one addition they hope to make is the thawing of permafrost in the arctic tundra that could accelerate warming by releasing methane, a potent greenhouse gas. hadgem2-es has factored in methane release from wetlands, but doesn't account for permafrost. also in the pipeline for the hadley centre's next model is the nitrogen cycle and how it influences plant growth.but the point of all of this "is not to develop a perfect model", says hurrell. "we develop new representations of aspects of the system based on our best understanding of the system, and when we are confident that we can represent that process well, we include it in the model."for climatologists, models are not just tools that can give a glimpse of what the future holds; they are also an experimental playground — a replica world on which they can test their knowledge of the climate system. without the ability to conduct global-scale experiments in the lab or in the field, models are the only tool they have.at the hadley centre, some staff joke that modellers will eventually try to include panda bears in their simulations. the bit of hyperbole no doubt riles some of the modellers, perhaps because they are only too keenly aware that their creations can never fully represent the real thing. "part of our intellectual challenge is to find out what's important to include," says jones. "there'll always be some level of detail or complexity that we can't get in there." met officesource: met office/ipccmet officethe uk met office is home to the world's most sophisticated climate model.[image 2 left]in its 2007 report, the intergovernmental panel on climate change (ipcc) issued a range of projections of what might happen to the climate under various scenarios. some assumed that human population and consumption would grow quickly, whereas others envisioned the emergence of clean technologies or slower rates of development. but the panel did not examine deliberate efforts to rein in emissions of greenhouse gases.for the next ipcc assessment of the physical-basic of climate change, due out in 2013, models will simulate emission-controlling scenarios called representative concentration pathways (rcp). each rcp is named after the level of radiative forcing, or overall warming power of human activities, expected in 2100. the current radiative forcing from past emissions is about 1.6 watts per square metre and the net concentration of greenhouse gases is more than 400 parts per million in terms of carbon dioxide equivalents. the new scenarios are: rcp8.5 emissions rise indefinitely, radiative forcing reaches 8.5 w m–2 and concentrations of all anthropogenic greenhouse gases reach at least 1,370 p.p.m. in co2 equivalents by the end of the century. rcp6.0 emissions level off and radiative forcing reaches about 6 w m–2 sometime after 2100, with greenhouse-gas concentrations stabilizing at about 850 p.p.m. in co2 equivalents sometime after 2100. rcp4.5 emissions level off earlier and radiative forcing reaches roughly 4.5 w m–2. atmospheric greenhouse-gas concentrations stabilize at around 650 p.p.m. in co2 equivalents sometime after 2100. rcp3.0-pd radiative forcing peaks at about 3 w m–2 then declines; atmospheric greenhouse-gas concentrations reach around 490 p.p.m. in co2 equivalents and then decline.you can also search for this author in pubmed google scholarolive heffernan is the editor of nature reports climate change. nature reports climate change  nature geoscience  hadley centre  geophysical fluid dynamics laboratory  national center for atmospheric research reprints and permissionsheffernan, o. earth science: the climate machine. nature 463, 1014–1016 (2010). https://doi.org/10.1038/4631014adownload citationpublished: 24 february 2010issue date: 25 february 2010doi: https://doi.org/10.1038/4631014aanyone you share the following link with will be able to read this content:sorry, a shareable link is not currently available for this article. provided by the springer nature sharedit content-sharing initiative journal for general philosophy of science (2015)