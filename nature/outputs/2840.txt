the application of machine learning to big data, to make quantitative predictions about reaction outcomes, has been fraught with failure. this is because so many chemical-reaction data are not fit for purpose, but predictions would be less error-prone if synthetic chemists changed their reaction design and reporting practices.you have full access to this article via your institution.data-driven discoveries about materials and their chemical properties have become possible owing to the latest advances in ‘big data’ analytics that employ machine-learning (ml) algorithms. for example, scientists have successfully used ml to discover new materials for magnetic refrigeration1, and identify new photoactive compounds for light-emitting diodes2. however, making quantitative predictions about the outcome of chemical reactions — for example, predicting chemical yields — currently proves to be problematic.ml-based quantitative predictions about chemical reactions require more categories of experimental data (fig. 1a), which are currently lacking in availability. now, writing in angewandte chemie, frank glorius and co-workers have demonstrated that certain systematic errors can render datasets unbalanced and incomplete, critically limiting the utility of ml-based predictions3. yet, such errors could be avoided if chemists alter their synthetic design and reporting practices. glorius and co-workers simulated how the extent of different types of errors in chemical-reaction data can impact the level of success by which ml can predict a chemical yield, presenting results from four case studies for common reactions used in medicinal chemistry: buchwald–hartwig couplings, suzuki–miyaura cross-couplings, amide couplings, and nucleophilic aromatic substitutions.a, machine-learning (ml) predictions of new materials using structure and property data can lead to a new material, while ml predictions of reaction outcomes are problematic because they require higher-dimensional data. b, illustrations of the three types of error discussed. c, the very different sampling from data extracted from the scientific literature compared with hte data.three types of errors were simulated — random experimental noise, and two forms of systematic error caused by either selection bias or reporting bias (fig. 1b). random noise is omnipresent in any experiment, whereas selection bias and reporting bias refer to systematic errors in the data that are caused by the behaviour of a synthetic chemist. selection bias concerns a chemist’s natural tendency to select certain experimental parameters according to their personal preferences or ease of access; for instance, ligands that are familiar to them or happen to be available in their laboratory, an example being the use of triphenyl phosphine as a ligand in over 45,000 suzuki–miyaura couplings (∼60% of all reports). this type of bias generates highly unbalanced datasets. reporting bias pertains to a chemist’s selective presentation of successful results, such as the highest chemical yield obtained for a given reaction, or exclusive reporting of the chemical yield for only the major product. this bias results in missing data that can cause errors in judgement about the likely success of a given experiment, and a short-sighted view about the importance of reaction by-products.glorius and co-workers added simulated noise and error due to these systematic biases to the original chemical-reaction data in order to test the effect of progressively increasing the extent of each type of error on the success level of ml-based predictions of chemical yield. each yield was predicted in triplicate since three ml models were employed in these tests based on: one-hot encoding (statistical baseline model) with random forest; multiple fingerprint featurisation with random forest; and a deep-learning neural network that is incorporated within bidirectional encoder representations from transformers. first looking at the effect of noise, ml-based predictions of chemical yields were found to show satisfactory performance (accurate within approximately 10%) as long as experimental noise does not exceed levels above 25%.the effects of the systematic biases on the efficacy of these ml-based predictions were then assessed, using two types of data sources: the reaxys database (https://www.elsevier.com/solutions/reaxys) which contains chemical-reaction data that have been manually curated from the scientific literature; and databases that have resulted from high-throughput experiments (htes)4,5. the sample distributions of the two types of datasets will naturally differ (fig. 1c) and this difference underpins the tests for systematic bias. data from the scientific literature represent many independent reports of chemical yield as a singular value, which authors claim for a given reaction, thus containing selection and reporting biases. in contrast, data from htes will be intrinsically correlated and low in systematic biases, as the goal of hte is to determine the optimal experimental conditions to form a chemical product by repeating its synthesis many times, while systematically varying a range of reaction parameters: nature of the reactant, ligands, temperature and ph value. the systematic nature of hte limits selection bias and all results are reported, thereby circumventing reporting bias.ml-based predictions showed that selection bias in data from the scientific literature accounted for a 30% increase in prediction error relative to a control experiment, where hte data (low in selection bias) were sampled randomly. ml predictions are thus seriously limited by data sources that display uneven coverage of chemical space. the impact of reporting bias on ml-based predictions was then studied by effectively imposing the sample distribution of chemical yield data from the scientific literature onto corresponding hte-generated data. this artificially introduced reporting bias into the hte data whose resulting statistical distribution emulates that of corresponding data from the scientific literature. ml predictions based on such data revealed that this simulated reporting bias incurs an increase in prediction error by more than 50%, relative to the original hte dataset. thus, the resistance to record data on low-yielding chemical products in scientific publications appears to render untenable ml predictions of chemical yields.the results are quite striking, although one must be careful not to generalise them too readily, given that they were afforded using data from four specific chemical reactions. while one can mine chemical data from the literature, hte databases are needed for quantifying the bias. yet there is currently a dearth of hte databases, and each one that is available naturally covers such a small region of chemical space (i.e., specific to one compound). glorius and co-workers advocate that the reporting of all synthetic results, especially low-output findings, should be presented as a mandatory requirement in order to reduce reporting bias. overall, it would seem that the best way to achieve this would be to encourage publishers to regulate authors of scientific papers to disseminate all full-scale synthetic attempts rather than just their most successful results. however, such data would need to be extractable, not simply deposited in supplementary information which is the current home for excess data but from which data extraction is renowned to be difficult.glorius and co-workers also suggest that synthetic chemists need to change their entire philosophy regarding designing experiments, to become much more systematic in nature. it would certainly be great if hte-level data were available for every chemical reaction. however, such a dream is currently unrealistic without the injection of high-level resourcing to create major experimental facilities that automate synthesis for the physical sciences. such facilities would also require increased cooperation between synthetic chemists and ml scientists to ensure good experimental design.so, can chemists see the errors of their ways? we can perhaps hope, given the increasing variety of hte-based setups that have been demonstrated to synthesize optimized solar-cell materials6, photocatalyst mixtures7 and hole-transport materials in thin-film manufacturing8. meanwhile, the pace by which ml advances is currently so high that it might perversely be quicker to develop new ml algorithms that overcome these barriers to the prediction of quantitative reaction outcomes, owing to unbalanced and missing data.baptista de castro, p. et al. npg asia mater. 12, 35 (2020).article google scholar gómez-bombarelli, r. et al. nat. mater. 15, 1120–1127 (2016).article google scholar strieth-kalthoff, f. et al. angew. chem. int. ed. 61, e202204647 (2022).cas article google scholar perera, d. et al. science 359, 429–434 (2018).cas article google scholar ahneman, d. t., estrada, j. g., lin, s., dreher, s. d. & doyle, a. g. science 360, 186–190 (2018).cas article google scholar li, z. et al. chem. mater. 32, 5650–5663 (2020).cas article google scholar burger, b. et al. nature 583, 237–241 (2020).cas article google scholar macleod, b. p. et al. science adv. 6, eaaz8867 (2020).cas article google scholar download referencescavendish laboratory, university of cambridge, cambridge, ukjacqueline m. coleisis neutron and muon source, stfc rutherford appleton laboratory, didcot, ukjacqueline m. coleyou can also search for this author in pubmed google scholarcorrespondence to jacqueline m. cole.the author declares no competing interests.reprints and permissionscole, j.m. the chemistry of errors. nat. chem. 14, 973–975 (2022). https://doi.org/10.1038/s41557-022-01028-6download citationpublished: 26 august 2022issue date: september 2022doi: https://doi.org/10.1038/s41557-022-01028-6anyone you share the following link with will be able to read this content:sorry, a shareable link is not currently available for this article. provided by the springer nature sharedit content-sharing initiative 