illustration by ana kovain july 2000, david haussler remembers crying as he watched the first fully assembled human genome streaming across his computer screen. he and jim kent, a graduate student at the time, built the first-ever web-based tool for exploring the three billion letters of the human genome. they had published the rough draft of the genome on the internet a mere 11 days after finishing the herculean task of stitching it all together — a task assigned to them as part of the human genome project (hgp), the international collaboration that had been working towards this goal for a decade. it would still be several months before the group published its analysis of the genome in the pages of nature1, but the data were ready to share.there it was, going out into the whole world, recalls haussler, scientific director of the university of california santa cruz genomics institute. soon, every person in the world could explore it — chromosome by chromosome, gene by gene, base by base — on the web.it was a historic moment, says haussler. before the hgp launched in the early 1990s, there had not been a serious discussion about data sharing in biomedical research, haussler says. the standard was that a successful investigator held onto their own data as long as they could.that standard clearly wouldn’t work for such a large and collaborative effort. if countries or scientists hoarded the data they were producing, it would derail the project. so in 1996, the hgp researchers got together to lay out what became known as the bermuda principles, with all parties agreeing to make the human genome sequences available in public databases, ideally within 24 hours — no delays, no exceptions.fast-forward two decades, and the field is bursting with genomic data, thanks to improved technology both for sequencing whole genomes and for genotyping them by sequencing a few million select spots to quickly capture the variation within. these efforts have produced genetic readouts for tens of millions of individuals, and they sit in data repositories around the globe. the principles laid out during the hgp, and later adopted by journals and funding agencies, meant that anyone should be able to access the data created for published genome studies and use them to power new discoveries.in 2000, jim kent, a graduate student at the university of california, santa cruz, helped to assemble and share the results of the decade-long human genome project.credit: the regents of the university of california. courtesy special collections, university library, university of california, santa cruz. uc santa cruz photography services photographs.if only it were that simple.the explosion of data led governments, funding agencies, research institutes and private research consortia to develop their own custom-built databases for handling the complex and sometimes sensitive data sets. and the patchwork of repositories, with various rules for access and no standard data formatting, has led to a tower of babel situation, says haussler.although some researchers are reluctant to share genome data, the field is generally viewed as generous compared with other disciplines. still, the repositories meant to foster sharing often present barriers to those uploading and downloading data. researchers tell tales of spending months or years tracking down data sets, only to find dead ends or unusable files. and journal editors and funding agencies struggle to monitor whether scientists are sticking to their agreements.many scientists are pushing for change, but it can’t come fast enough.clinical genomicist heidi rehm says the field has come to recognize that big scientific advances require vast amounts of genomic data linked to disease and health-trait data. but it isn’t compatible and shareable, says rehm, based at massachusetts general hospital in boston and the broad institute in cambridge. how do we get everyone in the world — patients, clinicians and researchers — to share?sequencing the human genome made it easier to study diseases associated with mutations in a single gene — mendelian disorders such as non-syndromic hearing loss2. but identifying the genetic roots of more common complex diseases, including cardiovascular disease, cancer and other leading causes of death, required the identification of multiple genetic risk factors throughout the genome. to do this, researchers in the mid-2000s began comparing the genotypes of thousands to hundreds of thousands of individuals with and without a specific disease or condition, in an approach known as genome-wide association studies, or gwas. the next 20 years of human genomics must be more equitable and more openthe approach proved popular — more than 10,700 gwas have been conducted since 2005. and that has produced oceans of data, says chiea chuen khor, a group leader at the genome institute of singapore, who studies the genetic basis of glaucoma. a study with 10,000 people, looking at 1 million genetic markers in each, for example, says khor, would generate a spreadsheet with 10 billion entries.most of these individual-level genomic data now live in ‘controlled-access’ databases. these were set up to deal with the sticky legal and ethical concerns that come with genomic data that have been linked to personal information — ‘phenotype data’ that can include health-care records, disease status or lifestyle choices. even in anonymized data sets, it’s technically possible that individuals can be reidentified. so, controlled-access databases vet the researchers seeking access and ensure that the data are used only for the purposes that participants consented to.the us national institutes of health (nih) requires its grant recipients to place gwas data into its official repository, the database for genotypes and phenotypes, or dbgap. european researchers can deposit data into the european genome-phenome archive (ega) housed at the european bioinformatics institute (embl-ebi) in hinxton, uk. similarly, other large generators of genomic data, such as the for-profit company 23andme in sunnyvale, california, and the non-profit genomics england in london, operate their own controlled-access databases.a first draft of data sharing principles for the human genome project, written by john sulston on a whiteboard in bermuda, 1996.credit: richard myersbut uploading data into some of these repositories often takes a long time. as a result, says khor, the data are often minimal and sparse, because researchers are depositing just what’s required to be compliant.sometimes the data get stored in more than one place, and that creates other challenges. rasika mathias, a genetic epidemiologist at johns hopkins university in baltimore, maryland, who studies the genetics of asthma in people of african ancestry, says that decentralization is a huge problem. she is part of topmed, a precision-medicine programme run by the nih’s national heart, lung, and blood institute. it consists of more than 155,000 research participants across more than 80 studies and shares its data in several repositories, including dbgap and some university-based portals.it’s a remarkable resource, says mathias. but it’s cumbersome for an outsider to find all the pieces of available data and request access, she says. they must often provide detailed proposals and letters of support. it’s unnecessarily difficult.many look for workarounds. i personally do not download dbgap data, i just go straight to the researchers and ask if they want to collaborate, says ruth loos, a genetic epidemiologist at the icahn school of medicine at mount sinai in new york city. several years ago, she tried to access a dbgap data set, filing multiple rounds of digital paperwork, only to be rejected. even logging into dbgap can be a pain. it’s just not researcher-friendly, she says.stephen sherry, acting director of the nih’s national center for biotechnology information in bethesda, maryland, which runs the dbgap, acknowledges that the processes to submit and access data are imperfect and painful. and the complex, heterogeneous data require case-by-case review, which cannot simply be sped up by throwing more people at the crank to turn it faster.but, sherry says, the nih is investing in modernizing the system to make it more streamlined and flexible. carrie wolinetz, associate director for science policy at the nih, says it is yet to be determined whether the remedy will be a dbgap 2.0 or an alternative resource. do you put in a stop-gap measure, or is it time to invest in a whole bathroom renovation? she asks.for all the problems that controlled access causes in sharing genome data, many researchers say databases such as dbgap and the uk biobank, which holds genomic data on 500,000 people, are still invaluable. mathias is fiercely protective of the participants in topmed and sees merit in the protection that controlled access provides. like many, she would like to see the repositories better resourced. but, she says, i am an advocate for the checks and balances.craig venter (left) of celera genomics and francis collins (centre), then at the national human genome research institute, met in washington in 2000 to announce the completion of the first drafts of the human genome.credit: ron sachs/shutterstockand others are happy to have access, even if it is hard to obtain. it’s out of our scope to generate that amount of data, says melanie bahlo, who runs a statistical-genetics lab at the walter and eliza hall institute of medical research in melbourne, australia. her lab is more than willing to wade through the digital paperwork to use the dbgap, and has done so for more than ten projects. she also recently spent a fruitless six months chasing after a data set that was supposed to be publicly available through a research institute’s data portal, but wasn’t.nothing is harder than getting data out of dbgap and ega, says khor, unless it’s getting it from a researcher who is unwilling to share.twenty years on from the hgp, there is no specific universal policy that says research groups have to share their human-genome data, or share them in a particular format or database. that said, many journals have continued to abide by the bermuda principles, requiring that genomic data be shared in approved databases at the time of publication. enforcement of these policies can be hit or miss.michelle trenkmann, senior editor for genetics and genomics at nature in london, says that authors are often reluctant to share, citing concerns over participant privacy, consent or national or corporate rules governing who owns the data. what’s remarkable, is that, as a field, geneticists expect the data to be shared, but sometimes they do not want to share their own data, she says. trenkmann pushes back in such cases, and if the challenges can’t be overcome, the authors must spell out their reasons directly in the paper for transparency. (nature’s news team is editorially independent from its journal team.) a wealth of discovery built on the human genome project — by the numbersthe journal genome research, has a ‘no exceptions’ policy. executive editor hillary sussman explains that the journal’s editors will work through data-sharing obstacles with authors on a case-by-case basis to find solutions. this can go as far as asking authors to reapply for approval from their institutional review board, going back to participants to reobtain their consent or rerunning an analysis after removing unshareable data. the journal has turned away authors who state upfront that they cannot share data. the community and the funders demand this transparency and reproducibility, she says.but even when authors do agree to share data, editors and reviewers have limited ability to confirm that it is being done. they might not have the time — or the access to controlled-access databases — to check data quality, formatting or completeness.trenkmann says funders should require researchers to have a concrete data-sharing plan from the outset of a project. this could help to shift attitudes so that researchers see sharing as a duty, she says.an nih-wide data-sharing policy to be implemented in january 2023 does just that. it requires all nih grant applicants to put a data management and sharing (dms) plan into their grant proposals and allows researchers to allocate some of their budget to the task.this should ensure that data sharing is aligned both with ethical and privacy considerations, and with the fair principles — which mean that data must be findable, accessible, interoperable and reusable, says carolyn hutter, director of the national human genome research institute’s (nhgri) division of genome sciences in bethesda. that does not mean, i threw my data over a wall and hope someone caught it, she says.the enforcement part of it is tricky, hutter adds, because data sharing often comes at the very end of the project. and like journal editors, grant administrators can only do spot checks of any data-sharing accession numbers that appear in annual progress reports.there could be ways to share more simply without falling foul of proprietary or privacy issues. many genomic stakeholders agree that an aggregated form of gwas data, called gwas summary statistics, can and should be shared broadly and freely. these summaries include the aggregated scores for each genetic variant found to be associated with a disease or condition across multiple genomes. they are easier for researchers to work with, and they protect participant privacy.many research consortia do share these on their websites or portals. but an open-access collaboration between embl-ebi and the nhgri, called the gwas catalog3, is working towards a centralized, standardized solution.the rise of personal genomics companies such as 23andme, co-founded by anne wojcicki in 2006, added new wrinkles to genome data-sharing practices.credit: peter dasilva/nyt/redux/eyevinestarting in 2020, the gwas catalog gave researchers a way to submit their summary statistics along with metadata describing the study and participants. in return, researchers get a prepublication accession id to use in preprints and submitted manuscripts.but many researchers say that summary statistics are not sufficient for advancing genomic science. that’s a major threat to gwas, says chris amos, a genetic epidemiologist who studies lung cancer at baylor college of medicine in houston, texas. researchers need the individual-level genome data and the linked phenotypic trait data to reveal exactly how genetic variation plays out in disease. they also need the full data to check the science. if you don’t have the raw data, you can’t look at the quality. that is not good enough to make a reproducible finding, amos says.and the owners of the data for very large cohorts, such as 23andme and genomics england, don’t give unrestricted access to their summary statistics. they cite concerns over participants’ data privacy and the wish to retain ownership of their data. in effect, they run their own controlled-access databases, with custom processes for accessing and reanalysing their data. a precondition for working with much of their data is allowing the companies to share authorship of the resulting work. bahlo says these kinds of requirements set too high a bar for her and other bioinformaticians who wish to crunch data from genomics england’s 100,000 genomes project. milestones in genomic sequencinghutter acknowledges that not all the current growing pains of genomic data sharing can be fixed simply through improvements to the dbgap or by sharing summary statistics in the gwas catalog. the dbgap wasn’t positioned to evolve and handle every new type of data, she says. for example, the cost of storing data from whole genomes is very different from that for gwas data. as such, the nhgri has created a cloud-based infrastructure known as the analysis, visualization, and informatics lab-space (anvil), where researchers can share and analyse across large genomic data sets, including whole genome and exome sequences.another nih initiative is the researcher auth service (ras), which would authorize researchers to access anvil, the dbgap and several other data resources. the vision is that we’d push this out like a visa stamp, says sherry, allowing researchers to ultimately merge and analyse data at will in cloud-based systems. we’re building one of the first systems of library cards for researchers, says sherry.haussler and some other big-data wranglers also have ideas. as data-sharing frustrations were mounting in 2013, haussler, along with david altshuler, eric lander and other international colleagues laid the groundwork for the global alliance for genomics and health, or ga4gh (see go.nature.com/3app3xr). it started with the same ideals as the hgp. we’d get the world to share data on one big database, and we’d all agree on how we’d use that data, and kumbaya, says haussler. very quickly, it became evident that that was utterly impossible.instead, the ga4gh now focuses on creating standards for the multitude of genomic databases around the world. its working hypothesis is that it will be technically possible to harmonize data (like the gwas catalog on a grander scale) and to federate, or loosely link, the disparate data warehouses.ga4gh chief executive peter goodhand uses the analogy of global mobile-phone communications. there’s huge competition between mobile-phone makers and service providers, but at the end of the day, they all have to work on the same network. for true interoperability to take place, there have to be working relationships between the providers, says goodhand. you can set up the systems that permit the sharing and make it easier. sequence three million genomes across africascientists used a ga4gh standard to create the matchmaker exchange, for example. this service lets clinicians and researchers working on the rarest of rare diseases search a single federated network of eight international databases to find individuals with a similar genotype or phenotype to a case they’re working on. if a match is returned, both parties are connected in a way that protects both patient confidentiality and research ownership and authorship. the nih’s ras will also use a ga4gh standard, called the data repository service, a software interface that helps different repositories to communicate.bahlo and others say that data federation efforts become even more important as the field pivots to digging deeper into phenotype data, which have grown in scope and complexity. that data comes in all sorts of forms — environmental exposures, smoking status, medical imaging data, says bahlo.she and others see data federation as a great opportunity to inject global equity into genomic data sharing. researchers from developing countries could access and work with data sets without needing to generate their own data or have their own supercomputing resources. and better data sharing should also improve representation of non-white, non-european global ancestries. under-representation is especially stark for continental african ancestries, which make up less than 0.5% of all gwas participants4.haussler thinks that positive peer pressure should convince scientists to share in better ways. the need is only growing. twenty years after releasing the first human genome to the internet, his team has built a way for anyone to explore the sars-cov-2 viral genome5.data should be a living thing, says haussler. i want to click on it and play with it immediately. that should be the motivation. if you don’t share your data, you can’t do that.