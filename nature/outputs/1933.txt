drowning in data? new visualization techniques could help. philip ball discovers, among other things, how to plot a seven-dimensional graph.ralph kahn's office at nasa's jet propulsion laboratory in pasadena, california, has a unique line in interior decor. brightly coloured charts cover the walls. red is a favourite theme, streaked through with yellow and blue. the garish creations won't win any design awards, but kahn hopes that by staring at them for long enough, he could come up with new ideas about the earth's climate.kahn has created his unusual wallpaper because, like many other scientists, he is swamped with data. the figures illustrate measurements taken by a sensor on board the terra satellite, the flagship of nasa's earth-observation programme. the device produces data at a phenomenal rate — its nine cameras simultaneously monitor four frequencies of sunlight reflected by the earth. to understand a climate phenomenon involving multiple feedbacks, you really need to look almost everywhere, in detail, and often, explains kahn.by viewing plots of the terra data, kahn hopes to spot hints of patterns that are worthy of analysis. yet the possibilities for plotting data in revealing ways run far beyond kahn's two-dimensional coloured graphs. few people are aware of it, but sophisticated methods for visualizing large data sets are within the reach of most researchers. thanks to increases in computing power and improvements in graphical software, scientists can now make more and more use of the best pattern detector that we have at our disposal — the human visual system. digital deluge researchers studying everything from gene expression to financial risk assessment stand to benefit. many disciplines routinely produce more data than researchers know what to do with. in 1999, for example, an advisory committee to the us national institutes of health estimated that some biomedical laboratories can produce up to 100 terabytes (1014 bytes) of information a year — enough to fill a million encyclopaedias. digital technology is the main culprit. a 2,048 × 2,048-pixel colour image contains 16 megabytes of data, even if it's a picture of my kitchen floor, points out bill eddy, a statistician at carnegie mellon university in pittsburgh, pennsylvania.it's not just a question of having too much data — the kind of data also matters. the relationships between two variables can be examined using a simple graph. but experiments in many fields generate data using different instruments that simultaneously measure several parameters, which may or may not be related. the number of variables that could be paired in a two-dimensional plot is enormous — a problem known as the 'curse of dimensionality'.it is here that visual data-mining could help. before pinning down precise mathematical relationships within a data set, visual techniques provide a first line of attack that can suggest what kind of trends and patterns may lie within the numbers, and may thus help guide the focus of more detailed analysis. such an approach, known as exploratory data analysis, has long been used by statisticians, but sophisticated visualization techniques are now playing an increasingly important part in it.dna microarray experiments, which can simultaneously monitor the expression of thousands of genes, are already benefiting from visual analysis. david botstein, a geneticist at stanford university who helped to pioneer microarray studies, began working on the problem in 1998. together with colleagues, he described how data on the activity of thousands of genes can be displayed in a way that makes it clear which genes have similar patterns of expression1.in january, a team from the netherlands cancer institute in plesmanlaan and rosetta inpharmatics, a drug-development and technology company in kirkland, washington, used a similar plotting method in their work on predicting the course of breast cancers2. the team looked at tissue samples from the tumours of almost 100 patients, some of whom had gone on to develop secondary cancers in the five years after the samples were taken.the researchers measured expression levels for almost 5,000 genes in each tumour, and used a mathematical algorithm to create a list of tumours in which genes with similar patterns of expression were grouped together. plotting the list using colours to represent expression levels revealed that the tumours seemed to fall into two different groups (divided by the yellow line in the plot; see right). further investigation helped the team to identify a subset of 70 genes that could be used to predict whether a patient will develop secondary tumours.botstein feels that other important findings lie hidden in microarray data. there is more information in the data than we as a community have yet extracted, he says. i'm hopeful that better things may emerge. this is the very beginning. working in parallel other visualization techniques have longer histories, but are now benefiting from improvements in computing technology. one example, the method of parallel coordinates, was first proposed during the 1980s, and is particularly useful for tackling high-dimensional data.in the demonstration example shown in the plot below (bottom), the method is used to represent performance data for cars. each vehicle is described by seven variables, such as weight and acceleration. parallel axes are used to represent each variable, and the data for each vehicle form a line that links the values on each axes. trends can then be extracted by looking for sets of lines that cluster together. in the example given here, representing cars with high fuel efficiency in purple shows that these vehicles tend to have low weights and to be more recent models.more complex data can be mined by combining parallel coordinates with other visualization techniques. edward wegman, a statistician at george mason university in fairfax, virginia, has used these methods to identify correlations in data on bank customers in order to find out whether particular combinations of variables can be used to identify individuals who are at risk of financial problems3.each customer is classified by eight variables, such as age, occupation and profit status with the bank. plotting all of the records — over 130,000 in total — produces a solid mass of lines that is difficult to interpret, so wegman used a technique known as saturation brushing to reveal hidden patterns. each line was given a hint of colour: red for those with overdrafts, green for those in credit. individual lines that were brushed in this way still appeared black, but the colours added up when lines overlapped. if, for example, a group of overdrawn customers clustered around a point on one of the axes, the mass of lines appeared red. roughly even numbers of red and green lines mixed around points that did not correlate with credit status, and these points appeared yellow. the colour of money colour brushing revealed, unsurprisingly, that younger customers are more likely to be in debt (see plot, above top). but wegman suspected that more interesting correlations might be hidden in the data, and so he used a further method of analysis, known as a grand tour. this amounts to taking a virtual journey through the multidimensional data-space so that the points — or lines in parallel coordinates — are seen from many different perspectives.the process is easy to visualize in three dimensions. imagine a cloud of points, shaped like a doughnut, on a three-dimensional graph. if the graph is viewed through the plane of the doughnut, the hole is hidden. it is only when the data are seen from other angles that the overall structure becomes clear.a mathematical version of this process can be applied to data of any number of dimensions. in the case of parallel coordinates, the lines are viewed from a new position by replotting the data using axes that represent combinations of the original variables, rather than assigning a single variable to each axis.as the grand tour of the bank data progressed, interesting features emerged. wegman clarified these features by removing nearby points that did not appear to be part of potential trends, a process known as data-pruning. after several successive plots, the data revealed further risks associated with certain occupations, whether the customer owns or rents their home, and their marital status.because each axis now represents a combination of variables, risks cannot easily be related to the original variables, such as occupation and so on. but the risk associated with a new customer can still be calculated by combining that person's data in the appropriate way. and by not singling out particular factors, the bank can avoid accusations of discrimination.other visualization researchers stress the importance of interacting with data. jim thomas, an expert in data-mining technology at pacific northwest national laboratory in richland, washington, believes that the key to success lies in the dynamic aspect of modern computer graphics. interaction is as important as the visuals, he says. and if users want to get really interactive, they can no better than to enter a device known as the cave.developed in the early 1990s in the electronic visualization laboratory (evl) at the university of illinois at chicago, the cave is a cube-shaped room, around three metres square, with one open wall through which three-dimensional computer images are projected onto the other walls and floor.inside the cave, users can 'float' in data-space. sensors track head movements, and the on-screen graphics are adjusted so that the image changes just as it would if the user were moving around a real scene. a manual controller called the wand acts like a kind of three-dimensional mouse, allowing users to probe and modify the images.the cave allows you to understand very complicated structures because you can intuitively and easily manipulate your view, says paul rajlich, a cave researcher at the national center for supercomputing applications (ncsa) at the university of illinois at urbana-champaign. you can simply walk around the cave and examine the structure just as if it were a real object.there are now about a hundred caves in use around the world. most are used in virtual-reality research, automotive design and medical training. but earth scientists often use them to visualize geological structures, and biologists are also getting in on the act. timothy karr studies fertilization in fruitflies at the university of chicago. he has used a software system known as crumbs, developed at the ncsa and other departments at urbana-champaign, which allows users to explore paths through three-dimensional data-space by dropping 'crumbs' to mark their trails. gathering the crumbs karr is interested in the fate of fruitfly sperm after it enters the egg. he used a cave at the ncsa to display microscopic images of sperm tails inside the egg. by using crumbs to outline the tail on successive images, he was able to plot how the sperm changes once it is within the egg4. think about trying to follow an unravelling ball of yarn inside an egg, and you have an idea of how difficult this would be without software like crumbs, says karr.caves, and other less sophisticated methods, could help researchers get a handle on the vast, high-dimensional data sets that are becoming increasingly common. but the techniques are not without their problems. designing visualization methods is a multi-disciplinary task, drawing in statisticians, computer programmers and the researchers who produce the data, yet these different groups do not always communicate effectively. and even when techniques are well-designed, some researchers are wary of placing too much confidence in them.many of the potential problems stem from a lack of cooperation, an issue that botstein has experienced in his microarray work. biologists know too little about mathematics and computation, and computer scientists and statisticians know too little biology, he says. and as in any interdisciplinary endeavour, there are cultural and value differences that cause misunderstanding. biologists, says botstein, do not particularly care how they analyse their data, as long as it offers some insight. but mathematicians may feel that a 'trivial' analysis has no academic value, despite the fact that such methods often stretch biologists' analytical powers to the limit.there is also a divide between researchers who work on scientific images and those that use visualization as a precursor to statistical analysis. the two groups tend to have their own separate conferences, says jason leigh of the evl. there are so many techniques from both camps that are mutually beneficial to one another, but the opportunities for cross-fertilization are lost.others lament the fact that vision researchers are seldom involved in developing the techniques, and warn that statisticians could be placing too much confidence in the pattern-recognition abilities of the human brain. the human visual system is remarkably good at discovering real patterns, says eddy. but it is also remarkably good at discovering false patterns. the experts who work on visualization methods are, with a few notable exceptions, woefully ignorant of the relevant psychology and psychophysics. sadly, the cognitive psychologists have bigger fish to fry and are not especially interested.if the groups do start working together, they are likely to find that the speed with which data become redundant creates a demand for new methods of analysis. the time window for analysis is getting shorter and shorter, says daniel carr, a statistician at george mason university. it is hard to imagine that in five years scientists will be much interested in analysing today's microarray data.they will also find that with data piling up in many different disciplines, and becoming less useful increasingly quickly, there has never been a greater need for visualization methods. and for the advocates of visualization, the benefits are clear. science will continue to progress with or without more attention to visualization and data analysis, says eddy. but it will progress much faster with careful attention to these issues. to begin with there has to be a will to do it — a commitment to the analysis.eisen, m b., spellman, p. t., brown, p. o. & botstein, d. proc. natl acad. sci. usa 95, 14863–14868 (1998).ads cas article google scholar van 't veer, l. j. et al. nature 415, 530–536 (2002).cas article google scholar wegman, e. stat. med. (in the press).karr, t. l. & brady, r. trends genet. 16, 231–232 (2000).cas article google scholar download referencesconsultant editor of nature, philip ballyou can also search for this author in pubmed google scholarreprints and permissionsball, p. picture this. nature 418, 11–13 (2002). https://doi.org/10.1038/418011adownload citationissue date: 04 july 2002doi: https://doi.org/10.1038/418011aanyone you share the following link with will be able to read this content:sorry, a shareable link is not currently available for this article. provided by the springer nature sharedit content-sharing initiative microbial cell factories (2018)acta seismologica sinica (2004)nature (2003)