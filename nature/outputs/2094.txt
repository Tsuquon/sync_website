progress in neuroscience might be faster if researchers shared their results in a network of databases. but the technical challenges are huge, and reaching a consensus on what to archive won't be easy, says marina chicurel. stephen kuffler probably did not realize what he was starting when he established the department of neurobiology at harvard medical school in 1966. kuffler helped found the modern discipline of neuroscience by bringing together physiologists, biochemists and anatomists to focus their efforts on the nervous system.today, more than 50,000 neuroscientists worldwide study everything from individual molecules to complex behaviours in species from nematode worms to humans. generating enough data to fill more than 300 journals, they have created one of the largest, most unwieldy datasets in science. several prominent neuroscientists are now arguing that the time has come to tame this monster. they believe that progress could be boosted by creating interoperable databases, allowing researchers to share their results and make links between data from labs around the world. it's a topic that needs to be spotlighted, says dennis choi of the washington university school of medicine in st louis, who is president of the society for neuroscience.but there are three big obstacles. first, reaching a consensus on what is worth including in databases. second, the technical difficulty of collating and relating such disparate types of information. and third, the reluctance of researchers who have traditionally guarded their results jealously to embrace data sharing. it's a data-hugging community, observes michael arbib, director of the brain project at the university of southern california in los angeles, which is developing a variety of neuroscience databases. where to begin? bioinformatics is most advanced in genomics and proteomics, where dna and protein sequences are routinely archived in public databases. indeed, many journals will only consider papers if authors make their sequences public. but linear sequence data are easy to store in this way.technically, the genome and protein databases are relatively trivial, says stephen koslow, director of the office of neuroinformatics at the national institute of mental health in bethesda, maryland. neuroscience data are much more complex. but in the latest issue of nature neuroscience, koslow argues that the technical obstacles can be overcome, and makes a plea for more sharing of data1.koslow and others point to projects that are showing the way forward. many of these are supported by the human brain project, a multi-agency us government initiative established in 1993 to support research into databases and related tools for neuroscientists. bioinformatics enthusiasts note the scientific advances made possible by such databases (see ‘making the connections’, opposite). and they are encouraged by the organisation for economic cooperation and development's (oecd's) recent acceptance of a proposal to set up a working group on neuroinformatics, chaired by koslow, which aims to establish guidelines for neuroscience databases and software, and provide an internet portal for their dissemination2.but shovelling data into a database is only useful if they can be organized in meaningful ways. and in some areas, it is still unclear whether neuroscientists know enough to sort the wheat from the chaff. geneticists agreed long ago on the value of storing reproducibly generated dna sequences, but not images of their sequencing gels. many of neuroscience's subdisciplines have yet to reach a consensus on what is worth storing. in addition, techniques that neuroscientists use to generate and analyse data are often not standardized. part of this process must be to confront the devils early in the game, says choi. the devils include the possibility of prejudice overly influencing future thought, essentially by controlling the shape of the database or the priority given to certain data over others.most database developers agree that flexibility is the key. daniel gardner of the weill medical college of cornell university in new york is developing the cortical neuron net database, a repository for neurophysiological recordings from the brain's cortex. trying to ensure that the database keeps pace with the field, gardner's team is developing a hierarchical classification scheme that allows for new categories to be introduced without making earlier entries obsolete or restricting searches. it's never economically possible to go back and re-classify old databases, says bruce schatz, an information scientist at the university of illinois at urbana-champaign. it just costs too much.lack of standardization is a persistent problem, and the confused nomenclature of neuroanatomy is a particular bugbear. given that the same brain region can go by the name of the caudate nucleus or nucleus caudatus, or may be referred to as part of a larger structure known as the basal ganglia, only experts can reliably sort the data. worse still, different neuroanatomists sometimes use the same names to refer to different structures. the actual business of data collation is a hugely, hugely onerous thing, says malcolm young, a systems neuroscientist at the university of newcastle-upon-tyne in england. it's more onerous in my experience than doing experimental neuroscience. some scientists are turning to computers for help. researchers led by rolf kötter at the heinrich heine university in düsseldorf, germany, have developed an algorithm called objective relational transformation (ort)3 for their database of cortical connectivity in the macaque brain, known as cocomac. this algorithm transforms neuroanatomical data from one representational format, or mapping scheme, into another. by applying ort to data in their original formats, individual database users can convert the data into any format they wish to work with. besides reducing costs, the approach is more objective, reproducible, transparent and correctable than human labour, says kötter.there is also more disagreement over the interpretation of neuroscience data than there is for dna sequences. gordon shepherd and his colleagues at yale university in new haven, connecticut, are working on this problem. their senselab database integrates information on the olfactory system, including the sequences of olfactory receptor proteins and the electrical properties of the neurons involved in processing smells. senselab flags controversial data through annotations that describe conflicting studies and point users to the primary literature.other database architects are designing filters to let users decide how to weight the data. at the university of southern california, gully burns is constructing a database on neural connectivity in the rat brain called neuroscholar. future users will be able to score studies based on attributes such as the journal they appeared in or the techniques the investigators used. they will then be able to combine the weighted data from several studies to assess the strengths of hypotheses they might wish to test.other researchers agree that approaches such as this, which allow users to impose their own organizational schemes on the data are, in theory, the best solution. james bower at the california institute of technology in pasadena, for example, is developing model-based database systems whose organization relies almost exclusively on the user. bower's genesis database holds data that range from the characteristics of ion channels in cell membranes to the details of the connections between neurons. but these data only become usefully organized when a user creates a model of a neuron or network of neurons. super models for example, a user may devise a model of a purkinje cell — a type of neuron found in the cerebellum — to predict how electrical signals from its projections, or dendrites, will be transmitted to the cell body. the model will then sort relevant data from the database, and help the researcher spot inconsistencies. if the model is not compatible with known information, or if data are contradictory, the model will alert the researcher to the problem. and because the models are formal representations of hypotheses, they provide a way for neuroscientists to communicate and compare their ideas quantitatively. more problems are posed by data that vary in three dimensions. like a crumpled beach ball, the human brain's surface is covered with folds, serving as natural landmarks for neuroanatomists comparing results. but the distribution of folds varies between individuals, as does their location relative to functional brain areas. some researchers have resorted to shrinking or expanding their images to align them with different-sized brains. but brains often vary in more complicated ways than size. so several researchers have developed more sophisticated warping algorithms. david van essen of the washington university school of medicine, for example, has created algorithms that flatten the surface of the brain. these reduce the alignment problem to two dimensions, and have been incorporated into van essen's software toolkit and database of cortical structure and function, the surface management system.neurophysiologists, meanwhile, generate large amounts of time-series data as they record the electrical behaviour of neurons. michael gabriel and his colleagues at the university of illinois have developed a time series data protocol which has been incorporated into the neuronal time series analysis workbench, a set of informatics tools designed to help researchers who work with such data. like kötter's ort, the protocol is associated with translational filters that allow users to work with data in a variety of formats. another tool for neurophysiologists is a virtual oscilloscope, devised by gardner at cornell4. this allows electrophysiologists to see extended datasets — as opposed to the small, static excerpts that appear in published articles — as well as artificial traces predicted from simulations. neuroscientists often have to wrestle with huge computer files. an image of a single section of monkey brain can take up 80 megabytes, says edward jones of the university of california at davis, who is building brain atlases of such high resolution that they include individual neurons. a typical atlas of a monkey brain includes 1,500 sections, so his files often fill several gigabytes. functional magnetic resonance imaging (fmri) datasets pose similar problems. having access to servers that can handle such monstrous files and ship them across the internet is problematic. but computer experts such as schatz say that computer power and internet bandwidth should soon cease to be limiting factors.that should assist neuroscientists in tackling perhaps the most difficult challenge of all: linking their various databases into a seamless federation. ideally, says schatz, users should be able to navigate between databanks without noticing borders, as if interacting with a single, cohesive database. but this has proved difficult, even in fields with much simpler datasets. it took years to connect the european molecular biology laboratory's dna sequence database with the swissprot protein sequence database, because researchers could not agree on nomenclature. learning to share many database developers believe the best approach is to standardize the communication between databases, rather than standardizing the data they hold. one helpful software trick is to ‘wrap’ data with universal labels that describe their composition and format so they can be understood by different types of software, running on different computers — the html code that underlies web pages is one such wrapper. gardner is developing an interface to link different databases using the biophysical description markup language, a wrapper that he designed specifically to label neuroscience data, based on a code called xml. schatz believes another key tool will be concept-switching, a search strategy that relies on analysing the contextual relationships between phrases to identify underlying concepts5,6. concept-switching algorithms, for instance, would identify the term ‘nucleus caudatus’ used in one database as occurring in similar contexts as ‘basal ganglia’ in another, and link them together.but the technical issues are not the only ones that need to be ironed out. in a highly competitive field, in which data are often hard-won, many researchers are reluctant to release their data to be exploited by others. some neuroscientists, such as peter fox at the university of texas health science center in san antonio, also argue that the complexity of the data poses problems. when fox sent primary brain-imaging data to a collaborating lab, he says he had to be in almost continuous communication to explain how he had calibrated his machines, subtracted noise and displayed the data. if primary data were freely available, fox fears that some researchers might be led to the wrong conclusions. the submission of truly raw data would lead to a nightmare of misinterpretation, he says.the sensitivity of the data-sharing issue was underlined last month, with the opening of a new repository for fmri data at dartmouth college in hanover, new hampshire. a letter sent to leading researchers in the field announcing the national fmri data center by its director michael gazzaniga raised fears that, in the future, journals might require primary data relating to papers on fmri to be deposited in the database7. gazzaniga sent the letter in his capacity as editor of the journal of cognitive neuroscience , which had decided to adopt such a policy. that prompted several dozen fmri specialists to sign a letter arguing against mandatory data submission, which was sent to the data centre's financial backers and to the editors of 14 leading journals.although it may not be universally applicable, yale's shepherd has devised a way of encouraging neuroscientists to share without losing control of their data. the olfactory receptor database within shepherd's senselab allows users to deposit unpublished sequences, which are then kept hidden from other users’ view. when a search for sequence homologies finds a match with one of these unpublished sequences, the database provides the searcher with the contact information for the researchers who submitted the sequences. in this way, researchers can avoid revealing their primary data, yet benefit from identifying potential collaborators. the task ahead koslow accepts that it may take some time before neuroscientists adjust to a culture of data sharing. but he argues that the complexity of data do not pose insurmountable problems. for example, he says, details of the experimental conditions and variables needed to interpret primary data could be incorporated into databases. koslow believes that, ultimately, neuroscientists must be allowed to come to a consensus on the desirability of data sharing, and that attempts by funding agencies or journals to force the pace could be counterproductive. when it comes to building the basic infrastructure, however, koslow is keen to push the pace. but experts disagree on how to move forward. koslow wants to expand the existing approach of funding a wide variety of small projects, and then building links between those that prove most successful — and is pushing this model in the oecd working group. schatz is in favour of more centralization. in his view, small projects based in individual labs, often relying on graduate students who are amateurs in informatics, are likely to fail. he thinks the big problems, such as how to form database federations, have to be tackled by experts working in dedicated informatics centres. based on his experience in the early days of genomics, schatz proposes getting professional informaticians to build a network of databases focusing on a particular organism and a set of well-studied brain regions. he also believes the field needs definite targets against which to measure progress.as the experts debate the way forward, some neuroscientists are sceptical about investing heavily in databases. says rodney douglas, a computational neuroscientist at the institute of neuroinformaticsfootnote 1 in zurich: is it really a good thing for us to spend a lot of money on cataloguing everything in sight? douglas argues that the priority in neuroscience should be to understand how nervous systems deal with information. the nervous system doesn't collect a lot of data, it collects relevant data, and i think we can learn from that, he says. the best answer to the sceptics would be to demonstrate the practical advantages that databases can bring. but koslow, who chairs the human brain project's coordinating committee, concedes that the project's overall impact has so far been modest. the challenge for the bioinformaticians is to produce some tools that can make a real difference to working neuroscientists. i became convinced early on of the need to build databases, says van essen. but i must admit i didn't realize how challenging a task that would be.for an online debate on data sharing in neuroscience, see http://www.nature.comhuman brain project →http://www.nimh.nih.gov/neuroinformatics/index.cfm university of southern california brain project →http://www-hbp.usc.edu cortical neuron net database →http://cortex.med.cornell.edu cocomac →http://www.cocomac.org senselab →http://ycmi.med.yale.edu/senselab neuroscholar →http://www-hbp.usc.edu/projects/neuroscholar_connx.htm genesis database →http://www.bbb.caltech.edu/hbp surface management system →http://stp.wustl.edu/sums neuronal time series analysis workbench →http://soma.npa.uiuc.edu/isnpa/isnpa.html international brain mapping consortium →http://www.loni.ucla.edu/icbm/index.html neurosys →http://nervana.montana.edu/projects/neurosyswhy bother to build neuroscience databases? ask a bioinformatics enthusiast, and he or she will fire back a handful of success stories that provide a glimpse of the future.the international consortium for brain mapping, which has built a database of brain images from 7,000 people8, is one example. as human brains vary with age and between individuals, it is impossible to create an absolute atlas of structure and function. but by using sophisticated algorithms to align a large numbers of images, the consortium has generated atlases that show the probability that any given region of the brain is involved in the performance of a certain task, or is disrupted by a particular disease. atlases that highlight regions commonly affected by alzheimer's disease and stroke, for example, are becoming powerful tools for tracking the effects of candidate therapies in clinical trials. john mazziotta of the university of california at los angeles, who heads the consortium, argues that the database promises to become even more powerful as researchers begin to match up the brain images with additional information such as genomic, neurochemical and behavioural data.meanwhile, malcolm young at the university of newcastle-upon-tyne is gaining new insight into how the brain is hooked up by mathematically sorting through thousands of anatomical studies. it started in 1990, when young set eyes on a global diagram of the hundreds of pathways that interconnect the visual cortex, drawn up by david van essen of the washington university school of medicine in st louis9. he soon realized that by applying mathematical analyses to the collated data, he could mine it for valuable nuggets of information10. since then, young has developed additional methods, successfully predicted new relationships between structure and function in the brain, and is helping to spawn a new field of research in which the analysis of anatomical data is helping researchers to localize neurons with specific electrophysiological properties11.bringing together electrophysiological and anatomical data on more than 200 neurons, gwen jacobs at montana state university in bozeman is discovering how networks of neurons encode sensory information. neuroscientists are often limited to studying small numbers of cells at a time, rarely seeing the big picture of how larger neural networks transmit complex messages. but by focusing on a simple system and setting up neurosys, a network of databases that allows her to integrate data from multiple neurons, jacobs is overcoming this limitation. it came out of the frustration of trying to hold ten different variables in your head at the same time, she says. jacobs has collected data on the behaviour and shape of neurons that are involved in detecting air motion in crickets. plugging her findings into neurosys, she has discovered that the coding system used to specify the direction that a puff of air has come from seems to depend on the branching patterns of a small set of key neurons12.*in the united states, the term neuroinformatics generally refers to the application of databases and related tools to neuroscience. in europe, it is sometimes used to describe the discipline of computational neuroscience, which models the behaviour of neurons and neural networks.koslow, s. h. nature neurosci. 3, 863– 866 (2000).cas article google scholar smaglik, p. nature 405, 603 (2000).cas article google scholar stephan, k. e., zilles, k. & kötter, r. phil. trans r. soc. lond. b 355, 37– 54 (2000).cas article google scholar gardner, d., abato, m., knuth, k. h., debellis, r. & erde, s. m. phil. trans. r. soc. lond. b (in the press).butler, d. nature 405, 112– 115 ( 2000).cas pubmed google scholar schatz, b. r. et al. ieee computer 32, 51– 59 (1999).article google scholar aldhous, p. nature 406, 445– 446 ( 2000).cas article google scholar mazziotta, j. c. et al. neuroimage 2, 89– 101 (1995).cas article google scholar felleman, d. j. & van essen, d. c. cereb. cortex 1, 1– 47 (1991 ).cas article google scholar young, m. p. nature 358, 152– 154 ( 1992).ads cas article google scholar young, m. p. &. scannell, j. w. phil. trans r. soc. lond. b 355, 3– 6 (2000 ).cas article google scholar jacobs, g. a. & theunissen, f. e. j. neurosci. 20 , 2934– 2943 (2000).cas article google scholar download referencesyou can also search for this author in pubmed google scholarreprints and permissionschicurel, m. databasing the brain. nature 406, 822–825 (2000). https://doi.org/10.1038/35022659download citationissue date: 24 august 2000doi: https://doi.org/10.1038/35022659anyone you share the following link with will be able to read this content:sorry, a shareable link is not currently available for this article. provided by the springer nature sharedit content-sharing initiative science and engineering ethics (2010)neuroinformatics (2003)nature reviews neuroscience (2002)nature reviews neuroscience (2002)