most researchers agree that open access to data is the scientific ideal, so what is stopping it happening? bryn nelson investigates why many researchers choose not to share.you have full access to this article via your institution.in 2003, the university of rochester in new york launched a digital archive designed to preserve and share dissertations, preprints, working papers, photographs, music scores — just about any kind of digital data the university's investigators could produce. six months of research and marketing had convinced the university that a publicly accessible online archive would be well received. at the time of the launch, the university librarians were worried that a flood of uploaded data might swamp the available storage space.six years later, the us$200,000 repository lies mostly empty.researchers had been very supportive of the archive idea, recalls susan gibbons, vice-provost and dean of the university's river campus libraries — especially as the alternative was to keep on scattering their data and dissertations across an ever-proliferating array of unintegrated computers and websites. "so we spent all this money, we spent all this time, we got the software up and running, and then we said, 'ok, here it is. we're ready. give us your stuff'," she says. "and that's where we hit the wall." when the time came, scientists couldn't find their data, or didn't understand how to use the archive, or lamented that they just didn't have any more hours left in the day to spend on this business.as gibbons and anthropologist nancy fried foster observed in their 2005 postmortem1, "the phrase 'if you build it, they will come' does not yet apply to irs [institutional repositories]." a similar reality check has greeted other data-sharing efforts. most researchers happily embrace the idea of sharing. it opens up observations to independent scrutiny, fosters new collaborations and encourages further discoveries in old data sets (see pages 168 and 171). but in practice those advantages often fail to outweigh researchers' concerns. what will keep work from being scooped, poached or misused? what rights will the scientists have to relinquish? where will they get the hours and money to find and format everything?some communities have been quite open to sharing, and their repositories are bulging with data. physicists, mathematicians and computer scientists use http://arxiv.org, operated by cornell university in ithaca, new york; the international council for science's world data system holds data for fields such as geophysics and biodiversity; and molecular biologists use the protein data bank, genbank and dozens of other sites. the astronomy community has the international virtual observatory alliance, geoscientists and environmental researchers have germany's publishing network for geoscientific & environmental data (pangaea), and the dryad repository recently launched in north carolina for ecology and evolution research.we got the software up and running and said 'give us your stuff'. that's when we hit the wall. susan gibbons but those discipline-specific successes are the exception rather than the rule in science. all too many observations lie isolated and forgotten on personal hard drives and cds, trapped by technical, legal and cultural barriers — a problem that open-data advocates are only just beginning to solve.one of those advocates is mark parsons at the national snow and ice data center at the university of colorado in boulder. parsons manages a global programme to preserve and organize the data produced by the international polar year (ipy) that ran from march 2007 to march 2009 and included an estimated 50,000 collaborators from more than 60 countries.the ipy policy calls for data to be made available fully, freely, openly and on the shortest feasible timescale. "part of what is driving that is the rapidness of change in the poles," says parsons. "if we're going to wait five years for data to be released, the arctic is going to be a completely different place."  reality bites  but reality is forcing a longer timescale. as soon as they began implementing the data policy, parsons and his team encountered a staggering diversity of incoming information, as well as wide variations in the culture of data sharing. fields such as atmospheric science and oceanography, parsons says, have well-developed traditions of free and open access, and robust databases. but fields such as wildlife ecology and many of the social sciences do not. "what we discovered was that this infrastructure to share the data doesn't really exist, so we need to start creating that," parsons says.but his programme lacks the resources required to create that infrastructure on a large scale. so the team has resorted to preserving as much data as it can. it has delegated much of that job to national coordinators, or "data wranglers", as parsons calls them, who contact investigators and, "get the data branded and put in the ipy corral".one of the most successful data-wrangling countries has been sweden, which formed a subcommittee to correct its early lag in collecting and then received national funding for its own ipy data archive. national coordinator håkan olsson, a specialist in remote sensing at the swedish university of agricultural sciences in umeå, says that the country's archive is helping to house data from smaller, independent projects that would never reach large international databanks.nevertheless, he says, many swedish researchers still don't archive their data, or don't put data in formats that make them easily searchable and retrievable. he faults the funding agencies too. "unlike some other countries," he says, "the research councils in sweden do not yet have a practice to grant funds with the condition that data from the project is sent to a data centre."even when wranglers can identify the data, it is not always obvious where the data should go. for example, says parsons, "you would think that any snow and ice data would go into the national snow and ice data centre". but the centre's funding is generally tied to specific data streams, he says, which means it can find itself in the position of accepting glacial data from a programme it has money for, while being forced to turn away similar glacial data from programmes where it does not.despite the launch earlier this year of the paris-based polar information commons to make polar data more accessible, parsons says, that with all the "naive assumptions", the lack of planning and other unanticipated obstacles, properly managing the ipy data will require another decade of work.we don't just have to analyse the data, we need to make sure the data are right. szabocs márka in other fields, however, the main barriers to data sharing are concerns about quantity and quality. the us national science foundation's (nsf's) laser interferometer gravitational-wave observatory (ligo), for example, uses giant detectors in louisiana and washington to search for gravitational waves that might indicate the presence of rare phenomena such as colliding black holes or merging stars. ligo is also working with the virgo consortium, which operates a similar detector near pisa, italy.neither team has detected the signal they are looking for yet — but that's not surprising: gravitational waves are expected to be extraordinarily faint. the key to detecting them is to eliminate every possible source of spurious vibration in the detectors, whether from seismic events, electrical storms, road traffic or even from the surf on distant beaches. it requires what szabolcs márka, a physicist at columbia university in new york and the university's lead scientist for ligo, calls "a really paranoid monitoring of the environment".the question of what data should be shared has provoked strong debate within the ligo and virgo teams. should they open up all their terabytes of data to outside scientists, including the torrents of environmental data? or should they release just the cleaned-up data stream most likely to reveal a gravity wave? would naive outsiders fail to process the raw data adequately, leading to premature announcement of gravitational wave 'discoveries' that would hurt everyone's credibility? or would the extra eyes bring fresh perspective to the search?"i'm torn," says márka, who says that the precise terms of data sharing are being negotiated with the project's funders. "we don't just have to analyse the data, we need to make sure the data are right."how data should be shared is also a substantial problem. a prime example is the issue of data standards: the conventions that spell out exactly how the digital information is formatted, and exactly how the contextual information (metadata) is listed.in some disciplines it is comparatively easy to agree on standards, says clifford lynch, executive director of the coalition for networked information based in washington dc, which represents academia on data and networking issues. "if you look at something like the sequencing of a genome, there's a whole lot of tacit stuff that's already settled," he says. "sequencing one genome is very similar to sequencing another." but for other groups — say, environmental scientists trying to understand the spread of a pollutant — the choice of common standards is far less obvious.the all-too-frequent result is fragmented and often mutually incomprehensible scientific information. and that, in turn, stifles innovation, says james boyle, a law professor at duke university in durham, north carolina, and a founding board member of creative commons, a non-profit organization that supports creative content sharing.  always somebody smarter  "researchers generally create their own formats because they believe that they know how their users want to use the data," says boyle. but there are roughly a billion people with internet access, he says "and at least one of them has a smarter idea about what to do with your content than you do". for example, web users are using applications such as google earth to plot the spread of pandemics2 or to collect information on the effects of climate change. all that is needed, says boyle, are common languages and formats for data.perhaps not surprisingly, data-sharing advocates say, the power to prod researchers towards openness and consistency rests largely with those who have always had the most clout in science: the funding agencies, which can demand data sharing in return for support; the scientific societies, which can establish it as a precedent; and the journals, which can make sharing a condition of publication.the trick is to wield that power effectively. the nsf, for example, has funded ground-breaking research into digital archiving, search and networking technologies. but its data-sharing policies for standard research grants, for example, have come under fire for being scattered and ad hoc; they are often stipulated on a per-project basis. gibbons says she is especially disappointed with a 2003 mandate by the us national institutes of health (nih), which could have dramatically changed the culture of data sharing. the mandate does require a data-sharing plan for any grant worth $500,000 or more in direct annual costs or an explanation of why sharing isn't possible. but details about how to make the data available were so vague, says gibbons, that researchers soon stopped paying attention, content to sit back until someone got in trouble for not playing by the rules.officials at the nih office of extramural research reply that the data-sharing policy's 'vagueness' is, in fact, flexibility, an attempt to avoid forcing every research programme into a one-size-fits-all straightjacket. they note that the policy also recognizes that there may be valid reasons for not sharing, including concerns about patient privacy and informed consent.  the chicken or the egg?  nonetheless, until data sharing becomes a requirement for every grant, says daniel gardner, a physiologist and biophysicist at the weill medical college of cornell university, "people aren't going to do it in as widespread of a way as we would like". right now, he says, "you can't ask large numbers of people to do it, because it's a lot of work and because in many cases the databases don't exist for it. so there is kind of a chicken and egg problem here."one solution would be for agencies to invest in the infrastructure necessary to meet their archiving requirements. that can be difficult to arrange, says boyle. "infrastructure is the thing that we always fail to fund because it's kind of everybody's problem, and therefore it's nobody's problem." yet some agencies have been pioneers in this area. one often-cited example is the wellcome trust, the largest non-governmental uk funder of biomedical research. since 1992, its sanger institute near cambridge has been developing and housing some of the world's leading databases in genomics, proteomics and other areas.at least one of the people out there has a smarter idea about what to do with your content than you do. james boyle another prominent example is the nih's national library of medicine, which in 1988 established the national center for biotechnology information (ncbi) to manage its own collection of molecular biology databases, including the genbank repository. james ostell, chief of the ncbi's information engineering branch, likes to show a colour-coded timeline of contributions to genbank since its founding in 1982 — a progression that dramatizes the fast-evolving history of genetic sequencing. ostell points out thick waves of colours flowing from the left side of the chart. representing traditional sequence divisions such as viruses, rodents, primates, plants and bacteria, they dominated genbank's contents for years. other sequences, produced by faster techniques, began to put in appearances in the mid 1990s. then in late 2001 a sudden surge of green, representing dna snippets derived from whole-genome shotgun sequencing, quickly took over. by 2006, the green accounted for more than half of the database's contents.keeping up with ever-shifting technology has created its own set of challenges, says ostell. "nobody has infinite resources. and storing electronic information over time is a dynamic process. if you try to look at a file that you wrote with a word processor 20 years ago, good luck." in the same way, if a data set isn't readable by the latest version of a database, it isn't usable. so an archive may well have to choose between tossing old data out, and paying to preserve the out-of-date software required to make sense of them.even more challenging are the legal minefields surrounding personal data and privacy. the need to protect human subjects has led to starkly different approaches. some projects openly share data, whereas others require researchers to navigate a labyrinthine approval process before granting access. the ncbi has tried to build such requirements into its newer databases. a case in point is its database of genotype and phenotype (dbgap), which archives and distributes the results of genome-wide association studies, medical dna sequencing, molecular diagnostic assays and almost anything else that relates people's traits and behaviours to their genetic makeup. the dbgap allows open access to summaries and other forms of information that have been stripped of personal identifiers. but it grants controlled access to personal health information only after a researcher has been approved by a formal review committee.  novel meaning  such measures can be cumbersome, says ostell. yet the benefits of sharing far outweigh the costs. some of genbank's early sequences, for example, included genes from yeast and escherichia coli labelled as dna repair enzymes. years later, researchers studying human colon cancer made a link between mutations in patients and those same enzymes3. "if you just did a literature search, you would never make that connection," ostell says. "but when you search on the basis of their genes, suddenly you connect meaning in a way that's novel, which is the basis of discovery."sharing is obviously easier when the expectations are clear, and many scientists point to a 1996 meeting in bermuda as a defining moment for genomics. at the meeting, leaders working on the human genome project hammered out a set of agreements known as the bermuda principles. chief among them was the stipulation that sequences longer than 1,000 base pairs be made publicly available, preferably within 24 hours.we need to change the culture of science to one that equally values publications and data. william michener the bermuda principles, in turn, built on the foundations laid a decade earlier by the editors of journals such as nucleic acids research, who spurred the early development of genbank and other genomic repositories by requiring researchers to deposit their data there as a precondition for publishing. newer journals, such as the open-access public library of science journals, have made publication contingent on making the data "freely available without restriction, provided that appropriate attribution is given and that suitable mechanisms exist for sharing the data used in a manuscript". the journal neuroinformatics devoted its september 2008 issue to data sharing through the nih neuroscience information framework. ecological archives publishes appendices, supplements and data — related to studies appearing in other ecology journals — which include the metadata needed to interpret them. (nature journals require authors "to make materials, data and associated protocols promptly available to readers without preconditions".)yet the journals' power to compel data sharing and scientific culture change is not absolute. in march 2009, for example, the journal epidemiology felt able to call only for a "small step" towards more openness. "we invite our authors to share their data and computer code when the burden is minimal," said an editorial4 in that issue."we believe that data sharing is a matter of time," says miguel hernán, an epidemiologist at harvard university and a co-author of the editorial. but prematurely forcing a sharing requirement on authors "would be suicidal", he warns, especially with unresolved concerns over patient confidentiality. they would simply submit their papers somewhere else.another issue facing journals and data banks is how to ensure proper citations for data sets. "the one thing that people clearly care about in the sciences is attribution," says boyle. without an agreed-on way of assigning credit for original data falling beyond the parameters of a publication, however, it's no wonder that scientists are reluctant to share: their hard work may never be recognized by their employers or by granting agencies. worse yet, it could be poached or scooped.this is one place that technology might help, says boyle. he points to a music site associated with creative commons known as ccmixter, in which users can upload an a capella chorus, a bass line, a trumpet solo or other musical samples. users are free to remix the samples into new tracks. but when they do, the program automatically keeps a continuous credit record.so why not implement a similar system that would add a link back to a database every time a researcher repurposed some data? it wouldn't necessarily solve the problem of scooping, boyle says, "but it aligns the social incentives with the individual incentives". it could also provide a feasible way for universities or funding agencies to track the value of a researcher's data.  international agreement  other creative commons tools are already making their way into international scientific agreements. in may, for example, creative commons' cc0 licence was endorsed by participants at a meeting in rome on resource and data sharing within the mouse functional genomics community. the licence, which allows its users to "waive all copyrights and related or neighbouring rights" and thereby share more of their work, has been translated into dozens of languages.as welcome as such developments are, however, boyle points out that the creation of the legal and technical infrastructure to accommodate researchers' data-sharing concerns is a huge task, and should not be left solely to non-profit organizations and individual universities. nor should it be left to the funding agencies' grant-by-grant allocations for data sharing. it will require major government investments, starting with demonstration projects to explore how sharing can best be done. "what we need is a working example that you can point to," he says.if william michener has his way, a virtual data centre funded by the nsf and hosted by his university will be one of those examples. dataone (data observation network for earth) exists only on paper, but a five-year, $20-million grant through the nsf's datanet programme will help to turn it into an open-access database focusing on biology, ecology and environmental science data. four other $20-million archives are planned under datanet's first phase.michener, director of e-science initiatives for university libraries at the university of new mexico, albuquerque, and a leader of dataone, says that the archive is designed to accommodate many of the orphan data sets that have yet to find a home, and will target resource-strapped colleges, field stations, and individual or small teams of scientists. in the longer term, the dataone consortium, which encompasses two dozen partner institutions in the united states, the united kingdom, south africa, australia and taiwan, will explore business models that could sustain the archive well beyond its initial grant and potential five-year renewal. among the plans under consideration are a fee-for-service set up, a membership requirement for participating entities and the solicitation of external grants for education and outreach.dataone's success, however, may depend on overcoming the same ambivalence among researchers that has bedevilled the university of rochester and other builders of public databases. although a strategy is still being worked out, michener envisions a combination of workshops, seminars, websites and other educational tools to help clarify the how and why of sharing. but one archive can only do so much. larger efforts will be required to tackle what michener sees as the overriding challenge: "changing the culture of science from one where publications were viewed as the primary product of the scientific enterprise to one that also equally values data."without that cultural shift, says gibbons, many digital archives are likely to remain little more than stacks of empty shelves.foster, n. f. & gibbons, g. d-lib magazine doi:10.1045/january2005-foster (2005). http://www.nature.com/avianflu/google-earth/index.html marra, g. & boland, c. r. gastroenterol. clin. north am. 25, 755-772 (1996).cas article google scholar hernán, m. a. & wilcox, a. j. epidemiology 20, 167-168 (2009).article google scholar download referencesyou can also search for this author in pubmed google scholar bryn nelson is a freelance science and medical writer based in seattle, washington. see opinion, pages 168 and 171 and online special nature neuroscience  nature reviews genetics  nature reviews drug discovery  molecular systems biology  science commons  joint information systems committee  nih data sharing policy  wellcome trust data sharing information  dspace open-source digital repository reprints and permissionsnelson, b. data sharing: empty archives. nature 461, 160–163 (2009). https://doi.org/10.1038/461160adownload citationpublished: 09 september 2009issue date: 10 september 2009doi: https://doi.org/10.1038/461160aanyone you share the following link with will be able to read this content:sorry, a shareable link is not currently available for this article. provided by the springer nature sharedit content-sharing initiative earth science informatics (2022)neuromolecular medicine (2022)science china life sciences (2022)scientific data (2019)nature communications (2018)