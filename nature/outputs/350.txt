carl malamud is on a crusade to liberate information locked up behind paywalls — and his campaigns have scored many victories. he has spent decades publishing copyrighted legal documents, from building codes to court records, and then arguing that such texts represent public-domain law that ought to be available to any citizen online. sometimes, he has won those arguments in court. now, the 60-year-old american technologist is turning his sights on a new objective: freeing paywalled scientific literature. and he thinks he has a legal way to do it.over the past year, malamud has — without asking publishers — teamed up with indian researchers to build a gigantic store of text and images extracted from 73 million journal articles dating from 1847 up to the present day. the cache, which is still being created, will be kept on a 576-terabyte storage facility at jawaharlal nehru university (jnu) in new delhi. this is not every journal article ever written, but it’s a lot, malamud says. it’s comparable to the size of the core collection in the web of science database, for instance. malamud and his jnu collaborator, bioinformatician andrew lynn, call their facility the jnu data depot.no one will be allowed to read or download work from the repository, because that would breach publishers’ copyright. instead, malamud envisages, researchers could crawl over its text and data with computer software, scanning through the world’s scientific literature to pull out insights without actually reading the text.the unprecedented project is generating much excitement because it could, for the first time, open up vast swathes of the paywalled literature for easy computerized analysis. dozens of research groups already mine papers to build databases of genes and chemicals, map associations between proteins and diseases, and generate useful scientific hypotheses. but publishers control — and often limit — the speed and scope of such projects, which typically confine themselves to abstracts, not full text. researchers in india, the united states and the united kingdom are already making plans to use the jnu store instead. malamud and lynn have held workshops at indian government laboratories and universities to explain the idea. we bring in professors and explain what we are doing. they get all excited and they say, ‘oh gosh, this is wonderful’, says malamud.but the depot’s legal status isn’t yet clear. malamud, who contacted several intellectual-property (ip) lawyers before starting work on the depot, hopes to avoid a lawsuit. our position is that what we are doing is perfectly legal, he says. for the moment, he is proceeding with caution: the jnu data depot is air-gapped, meaning that no one can access it from the internet. users have to physically visit the facility, and only researchers who want to mine for non-commercial purposes are currently allowed in. malamud says his team does plan to allow remote access in the future. the hope is to do this slowly and deliberately. we are not throwing this open right away, he says. the jnu data store could sweep aside barriers that still deter scientists from using software to analyse research, says max häussler, a bioinformatics researcher at the university of california, santa cruz (ucsc). text mining of academic papers is close to impossible right now, he says — even for someone like him who already has institutional access to paywalled articles.since 2009, häussler and his colleagues have been building the online ucsc genome browser, which links dna sequences in the human genome to parts of research papers that mention the same sequences. to do that, the researchers have contacted more than 40 publishers to ask permission to use software to rifle through research to find mentions of dna. but 15 publishers have not responded or have denied permission. häussler is unsure whether he can legally mine papers without permission, so he isn’t trying. in the past, he has found his access blocked by publishers who have spotted his software crawling over their sites. i spend 90% of my time just contacting publishers or writing software to download papers, says häussler.chris hartgerink, a statistician who works part-time at berlin’s quest center for transforming biomedical research, says he now restricts himself to text-mining work from open-access publishers only, because the hassles of dealing with these closed publishers are too much. a few years ago, when hartgerink was pursuing his phd in the netherlands, three publishers blocked his access to their journals after he tried to download articles in bulk for mining.some countries have changed their laws to affirm that researchers on non-commercial projects don’t need a copyright-holder’s permission to mine whatever they can legally access. the united kingdom passed such a law in 2014, and the european union voted through a similar provision this year. that doesn’t help academics in poor nations who don’t have legal access to papers. and even in the united kingdom, publishers can legally place ‘reasonable’ restrictions on the process, such as channelling scientists through publisher-specific interfaces and limiting the speed of electronic searching or bulk downloading to protect servers from overload. such limits are a big problem, says john mcnaught, deputy director of the national centre for text mining at the university of manchester, uk. a limit of, say, one article every five seconds, which sounds fast for a human, is painfully slow for a machine. it would take a year to download around six million articles, and five years to download all published articles concerning just biomedicine, he says.wealthy pharmaceutical firms often pay extra to negotiate special text-mining access because their work has a commercial purpose, says mcnaught. in some cases, publishers allow these firms to download papers in bulk, thus avoiding rate limits, according to a researcher at a pharmaceutical firm who did not want to be identified because they were not authorized to talk to the media. university academics, however, frequently restrict themselves to mining article abstracts from databases such as pubmed. that provides some information, but full texts are much more useful. in 2018, a team led by computational biologist søren brunak at the technical university of denmark in lyngby showed that full-text searches throw up many more gene–disease links than do searches of abstracts (d. westergaard et al. plos comput. biol. 14, e1005962; 2018).carl malamud and andrew lynn oversee the project at jawaharlal nehru university in new delhi to extract text and images from 73 million research papers.credit: smita sharma for naturescientists must also overcome technical barriers when mining articles. it is hard to extract text from the various layouts that publishers use — something that the jnu team is struggling with right now. tools to convert pdfs to plain text don’t always distinguish clearly between paragraphs, footnotes and images, for instance. once the jnu team has done it, however, others will be saved the effort. the team is close to completing the first round of extraction from the corpus of 73 million papers, malamud says — although they will need to check for errors, so he expects the database won’t be ready until the end of the year.early enthusiasts are already gearing up to use the jnu depot. one is gitanjali yadav, a computational biologist at delhi’s national institute of plant genome research (nipgr) and a lecturer at the university of cambridge, uk. in 2006, yadav led an effort at nipgr to build a database of chemicals secreted by plants. called essoildb, this database is today scoured by groups from drug developers to perfumeries looking for leads. yadav thinks that carl’s compendium, as she calls it, could give her database a leg-up.to make essoildb, yadav’s team had to trawl pubmed and google scholar for relevant papers, extract data from full texts where they could, and manually visit libraries to copy out tables from rare journals for the rest. the depot could fast-forward this work, says yadav, whose team is currently writing the queries they will use to extract the data.srinivasan ramachandran, a bioinformatics researcher at delhi’s institute of genomics and integrative biology, is also excited by malamud’s plan. his team runs a database of genes linked to type 2 diabetes; they’ve been crawling pubmed abstracts to find papers. now he hopes the depot could widen his mining net.and at the massachusetts institute of technology (mit) in cambridge, a team called the knowledge futures group says it wants to mine the depot to map how academic publishing has evolved over time. the group hopes to forecast emerging areas of research and identify alternatives to conventional metrics for measuring research impact, says team member james weis, a doctoral student at mit media lab.malamud only recently had the idea of extending his activism to academic publishing. the founder of a non-profit corporation called public resource, based in sebastopol, california, malamud has focused on buying up government-owned legal works and publishing them. these include, for instance, the state of georgia’s annotated legal code, european toy-safety standards and more than 19,000 indian standards for everything from buildings and pesticides to surgical equipment.because these documents are often a source of revenue for government agencies, some of them have sued malamud, who has argued back that documents which have the force of the law cannot be locked behind copyright. in the georgia case, a us appeals court cleared him of infringement charges in 2018, but the state appealed, and the case is with the us supreme court. meanwhile, a german court ruled in 2017 that the publication of toy standards by public resource, including a standard on baby dummies (pacifiers), was illegal.but malamud has enjoyed victories, too. in 2013, he filed a lawsuit in a us federal court asking the internal revenue service (irs) to publish the forms it collected from tax-exempt non-profit organizations — data that could help to hold these organizations to account. here, the court ruled in malamud’s favour, prompting the irs to release the financial information of thousands of non-profit organizations in a machine-readable format.in early 2017, aided by the arcadia fund, a london-based charity that promotes open access, malamud turned his attention to research articles. under us law, works by us federal government employees cannot be copyrighted, and public resource says it has found hundreds of thousands of academic articles that are us government works and seem to defy this rule. malamud has called for such articles to be freed from copyright assertions, but it’s not clear whether that would hold up in court. he has posted his preliminary results online, but has put further campaigning on hold, because the project prompted him to take on a wider mission: democratizing access to all scientific literature.a trigger for this mission came from a landmark delhi high court judgment in 2016. the case revolved around rameshwari photocopy services, a shop on the campus of the university of delhi. for years, the business had been preparing course packs for students by photocopying pages from expensive textbooks. with prices ranging between 500 and 19,000 rupees (us$7–277), these textbooks were out of reach for many students.rameshwari photocopy services in new delhi was taken to court for copying parts of textbooks, and won.credit: sajjad hussain/afp/gettyin 2012, oxford university press, cambridge university press and taylor and francis filed a lawsuit against the university, demanding that it buy a license to reproduce a portion of each text. but the delhi high court dismissed the suit. in its judgment, the court cited section 52 of india’s 1957 copyright act, which allows the reproduction of copyrighted works for education. another provision in the same section allows reproduction for research purposes.malamud has a long association with india: he first travelled there as a tourist in the 1980s, and he wrote one of his first books, on database design, on a houseboat in srinagar. and around the same time that he heard about the rameshwari judgment, he had come into possession (he won’t say how) of eight hard drives containing millions of journal articles from sci-hub, the pirate website that distributes paywalled papers for anyone to read. sci-hub itself has lost two lawsuits against publishers in us courts over its copyright infringements, but despite those judgments, some of its domains are still working today.malamud began to wonder whether he could legally use the sci-hub drives to benefit indian students. in a 2018 book about his work called code swaraj, co-authored with indian tech entrepreneur sam pitroda, malamud writes that he imagined showing up on indian campuses in the equivalent of an american taco truck, ready to serve the articles up to those who wanted them.ultimately, he zeroed in on the idea of the jnu text-mining depot instead. (malamud has also helped to set up another mining facility with 250 terabytes of data at the indian institute of technology delhi, which isn’t in use yet.) but he is cagey about where the depot’s articles come from. asked directly whether some of the text-mining depot’s articles come from sci-hub, he said he wouldn’t comment, and named only sources that provide free-to-download versions of papers (such as pubmed central and the ‘unpaywall’ tool). but he does say that he does not have contracts with publishers to access the journals in the depot.malamud says that where he got the articles from shouldn’t matter anyway. the data mining, he says, is non-consumptive: a technical term meaning that researchers don’t read or display large portions of the works they are analysing. you cannot punch in a doi [article identifier] and pull out the article, he says. malamud argues that it is legally permissible to do such mining on copyrighted content in countries such as the united states. in 2015, for instance, a us court cleared google books of copyright infringement charges after it did something similar to the jnu depot: scanning thousands of copyrighted books without buying the rights to do so, and displaying snippets from these books as part of its search service, but not allowing them to be downloaded or read in their entirety by a human.the google books case was a test of non-consumptive data mining, says joseph gratz, an ip lawyer at the law firm durie tangri in san francisco, california, who represented google in the case and has previously represented public resource. even though google was displaying snippets, the court ruled that the text was too limited to amount to infringement. google was scanning authorized copies of books (from libraries in many cases), even though it did not ask permission. copyright holders might argue that if sci-hub or other unauthorized sources supplied the jnu depot, the situation would be different from the google books case, gratz says. but a case involving unauthorized sources has never been argued in american courts, making it hard to predict the outcome. there are good reasons why the source shouldn’t matter, but there may be arguments that it should, says gratz.the question of the facility’s legality in the united states might not even be relevant, because international researchers would be getting results from a depot that sits in india, even if they are accessing it remotely. so indian law is likely to apply to the question of whether it is legal to create the corpus, says michael w. carroll, a professor at the american university’s washington college of law in washington dc.here, india’s copyright laws might help malamud — another reason why the facility is in new delhi. the research exemption in section 52 means that the jnu data depot’s actions would be considered fair under indian law, argues arul george scaria, an assistant professor at delhi’s national law university. not everyone agrees with this interpretation, however. section 52 allows researchers to photocopy a journal article for personal use, but doesn’t necessarily allow the blanket reproduction of journals as the jnu depot has done, says t. prashant reddy, a legal researcher at the vidhi centre for legal policy in new delhi. that entire articles aren’t shared with users does help, but the mass reproduction of text used to create the database puts the facility in a legal grey zone, reddy says.when nature contacted 15 publishers about the jnu data depot, the six who responded said that this was the first time they had heard of the project, and that they couldn’t comment on its legality without further information. but all six — elsevier, bmj, the american chemical society, springer nature, the american association for the advancement of sciences and the us national academy of sciences — stated that researchers looking to mine their papers needed their authorization. (springer nature publishes this journal; nature’s news team is editorially independent of its publisher.)malamud acknowledges that there is some risk in what he is doing. but he argues that it is morally crucial to do it, especially in india. indian universities and government labs spend heavily on journal subscriptions, he says, and still don’t have all the publications they need. data released by sci-hub indicate that indians are among the world’s biggest users of their website, suggesting that university licences don’t go far enough. although open-access movements in europe and the united states are valuable, india needs to lead the way in liberating access to scientific knowledge, malamud says. i don’t think we can wait for europe and the united states to solve that problem because the need is so pressing here.