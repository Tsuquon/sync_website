implants in the brain could one day help paralysed people move robotic arms and legs. but first, scientists need to work out how our brains know where our limbs are, says alison abbott.shut your eyes. now, touch your nose. chances are you can do this without even thinking about it. for this you can thank your sense of proprioception, which is so much a part of us that most of us are unaware that it exists. this 'sixth sense' lets our brain know the relative positions in space of different parts of our bodies. without it, our brains are lost.ian waterman knows how that loss feels. more than 30 years ago he lost this sense almost overnight, when a flu-like virus damaged the required sensory nerves. his muscles worked perfectly, but he could not control them. i lost ownership of my body, he says. he could no longer stand, or even sit up by himself, and doctors said he would never be able to do so again. waterman's condition arose from a disease called acute sensory neuropathy and is so rare that only a dozen or so similar cases are known to the medical literature.some neuroscientists are taking a cue from waterman's experiences and starting to investigate whether robotic devices controlled by thought alone could be integrated with an artificial sense of proprioception. if so, they reason, these 'neuroprosthetics' could be made to work in a much more life-like way. what's more, they hope to gain a deeper understanding of how proprioception works, and how they might be able to manipulate it.some months after his virus attacked, waterman, only 19 years old, was lying in bed applying all his mental energy to the fight for control of his body. he tensed his stomach muscles, lifted his head and stared down at the limbs that seemed no longer to belong to him. he willed himself to sit up. concentrated effort later, he realized that it was the visual feedback that allowed his body to unexpectedly obey the mental instruction. but the euphoria of the moment made me lose concentration and i nearly fell out of bed, he remembers.from then on he learnt to compensate for his deficit in proprioception with other forms of sensory feedback to help him understand where his limbs are, and thus control them. it requires constant, intense concentration, but now, despite his profound impairments, he can manage fairly normal movements. most of the input that he relies on is visual — standing up with his eyes closed is still nearly impossible — but he can also tune in to the tug of a jacket sleeve to work out the direction his arm is moving. or to the cool air on his armpit when he raises his arm in a loose shirt. neuroprosthetic engineers are realizing that many sensory feedback signals could be similarly harnessed.a neuroprosthetic is more accurately called a brain–machine interface. hundreds of electrodes, fixed into tiny arrays, are placed in or on the surface of the cortex, the thin, folded outer surface of the brain that controls complex functions including the organization of movement. the electrodes record the electrical signals from the cortex's neurons and these are translated by a computer algorithm and used to drive specific actions — the movement of a cursor on a computer screen, for example, or of an artificial limb.in this issue of nature, two papers1,2,3 demonstrate dramatic progress in the area. a team consisting of john donoghue's group, based at brown university in rhode island and cybernetics neurotechnology systems in foxborough, massachusetts, implanted 96 electrodes into matt nagle's motor cortex, the brain region that processes information about movement. nagle is a quadriplegic patient and the first human volunteer to reach this advanced stage of testing (see picture, above). hooked up to computers and attended by a team of technicians, nagle could move a cursor to issue different instructions — for example, to open e-mails or turn down the television.krishna shenoy's group at stanford university, california, has done similar work in a non-paralysed monkey's premotor cortex, the area of brain where the animal's movement-related 'intentions' are generated. using a new algorithm, the team's brain–computer interface produced results four times faster and more accurate than previously seen. closing the loop the two papers show how closely neuroprosthetics are approaching medical reality. but although moving a computer cursor by thought alone may be dazzling, scientists have long-term ambitions to make neuroprosthetics reproduce more complex functions. could patients direct a robotic arm to pick up a coffee cup, for example? for this, the devices need to deliver feedback to the brain — we need to close the loop, says daofen chen, director of the neural prosthesis programme at the us national institute of neurological disorders and stroke in bethesda, maryland.the brain's sensory cortex receives signals — proprioception, touch, pain and so on — from the body (see graphic), and in response constantly modifies its movement-related commands. the current generation of output-only neuroprosthetics are open-loop systems — with more limitations than ian waterman, who can at least use visual, temperature and tactile feedback. brain–machine interfaces will have to become interactive, says chen. but now that we would like to exploit it, we realize we know next to nothing about sensory input.a handful of researchers is starting to try to work out where and how to stimulate the sensory nervous system to reproduce the sorts of information that a limb might send to the sensory cortex. it is early days: none of their work is published. and as so little is known about the system, there is no obvious place to start.theoretically, the 'where' could be the nerves running from the limb into the spinal cord, or the spinal cord itself (see graphic). or it could be higher — in the brain's thalamus, where incoming sensory signals are integrated and redirected to the appropriate part of the cortex, or the sensory cortex itself.the 'how' refers to the design of the electrical signals to be fed into the cortex. these could mimic the sensory system's natural nerve impulses, based on parameters such as frequency and amplitude. or they could involve creating artificial signals that the sensory cortex is able to distinguish, in the hope that the brain can be trained to associate particular signals with particular parameters.once scientists have worked out how best to encode the signals, the idea would be to place sensors on artificial limbs to generate signals representing proprioceptive information such as angle of joint, vibration, force of grip — and other sensory information that waterman has found helpful, such as temperature. trained brain most in the field have a hunch that the signal will not have to mimic neural activity perfectly. the brain can, after all, cope with the very unphysiological signals generated by the most successful brain–machine interface to date: the cochlear transplant. already, some 110,000 profoundly deaf people have been implanted with the device, according to the us national institutes of health. the implant sits in the inner ear and interfaces with the auditory nerve. its signals are totally artificial, and, at first, recipients can make nothing of the noise. but the auditory cortex, it turns out, is highly adaptable. with appropriate training, it can quickly learn to associate particular codes with particular sounds, so that transplant recipients can learn to follow conversations with ease.when the concept of stimulating the auditory nerve emerged in the 1970s, people said it would be impossible to generate the right electrical signal to the brain, says shenoy. but it turned out that you don't have to get it perfect, just close enough for the brain to do its own fine-tuning. on the other hand, one does not want to burden patients with having to learn too much, says john chapin a physiologist at the state university of new york health science center in brooklyn, and a pioneer in using neural activity to control robots. ideally we should aim to mimic the natural signal as closely as possible, he says (see 'voyagers in the cortex').for now, whatever works will be good. we don't know if it will turn out to be possible to incorporate sensory information but we are going to try, says neuroscientist andrew schwartz of the university of pittsburgh, an expert in brain–computer interface technology for the control of robotic arm movement.schwartz is working with douglas weber, a bioengineer at pittsburgh who is developing a model for studying sensory input. this involves using electrodes to stimulate the sensory nerves from the limbs of an anaesthetized cat at the point just before they enter the spinal cord, and simultaneously recording from neurons in the sensory cortex. weber will then repeat the recording, only this time manually moving the cat's limbs, instead of stimulating their nerves with electrodes. he will then compare the pattern of neural activity in the cortex in the two situations and see whether he can mimic the patterns he sees in response to passive movements with artificial stimulation.not everyone agrees, but my gut feeling is that we will be more successful if we stimulate outside of the central nervous system, says weber. at more central points there will be greater convergence of different inputs and i guess it would be hard to get clean signals.lee miller, a neurophysiologist from northwestern university in chicago, agrees that weber could be right, but is nevertheless approaching the problem from the top. although useful for study, stimulating nerves from peripheral areas of the body such as limbs will not work for a patient whose spinal cord is severed.working in monkeys, miller's group is electrically stimulating the part of the cortex that processes proprioception, and recording neuronal activity in the motor cortex at the same time. miller hopes this will eventually let him design stimulation patterns that can imitate the brain's own processing of proprioceptive signals, much in the way that weber is designing signals to imitate processing of movement. monkeys will be trained to move a 'virtual' arm, created on a computer screen by computer algorithms fed by both recordings from the motor cortex and the simulated proprioceptive feedback.it's a complex experiment, he admits, which will probably take up to five years to get working optimally. but he draws hope from his group's finding, presented at the 2005 society of neuroscience meeting in washington dc, that the monkeys can recognize and distinguish between high- and low-frequency stimulation.chapin's set-up is equally ambitious. he also works on monkeys but his chosen target is the thalamus, the brain's junction box for sensory input. the higher you go in the brain, the more complex and abstract things become, he says. it is hard to know if you are stimulating something precise. in his experimental system, chapin electrically stimulates the area of the thalamus that relays touch-related signals. simultaneously he records in the areas of the sensory cortex that process tactile information. the monkeys meanwhile, have one arm strapped down and one free. they have been taught to point with their free hand to an area on their immobilized arm that they 'feel' is being touched. we have found that we can produce a sort of 'natural response' in the cortex when we stimulate in the thalamus, he says. the response matches that produced normally when a specific part of the monkey's arm is touched. chapin plans to extend his investigations to study proprioception in the same way.the papers on brain-machine interfaces by donoghue and shenoy1,2 seem like science fiction becoming reality. the next step — trying to introduce sensory input into brain– machine interfaces — may appear at first glance to be as fanciful as the six million dollar man. but few neuroscientists could seriously doubt their theoretical potential. as experience with the first generation of neuroprosthetics shows — it's a question of understanding how the system works.if paralysed people can now use electronic implants to move a computer cursor by thought, could similar implants make the deaf hear, or the blind see? not any time soon, say researchers. the brain's visual and auditory cortices, which process vision and hearing, respectively, are extremely complex — and early experience has been discouraging.a few decades ago neurologists tried stimulating combinations of neurons, in the hope that patients would perceive complex shapes or sounds. the patients perceived strong signals, but they were unstructured — flashing light spots, or hissing sounds4,5.frank ohl of the leibniz institute for neurobiology in magdeburg, germany, believes that earlier studies may have failed because the cortex is too complicated to be stimulated using the same simple signalling that works on the peripheral nerves that feed into it. it is constantly active, and 96% of that activity is internal — different parts of the cortex exchanging information with each other, he explains.ohl's team has been monitoring this internal chatter while experimenting with different patterns of electrical stimulation. he hopes to find a way of delivering just the right kinds of signals at just the right point amid the chatter when the cortex is able to 'listen' to them. fiendishly complicated, but ohl has already shown that his chosen subjects, gerbils, can discriminate between rising and falling tones whether they are presented as sound, or as direct stimulations to the cortex that create the same pattern of cortical activity as the sound produces6. he also has unpublished evidence that the gerbils become more accurate in distinguishing the signals when he takes the chatter into account.a few weeks ago, ohl's university signed an agreement with neurosurgeon volker sturm from the university of cologne to move the project on to humans. epileptics sometimes have electrodes implanted in the areas of their brain believed to generate their seizures. if these areas happen to be in or close to the auditory cortex, ohl will attend the operations and repeat some of the animal experiments on patients willing to take part.ed tehovnik, a neuroscientist at the massachusetts institute of technology, is taking a similar approach in the visual cortex in monkeys. he thinks earlier attempts may have failed because they did not consider how important the cortex's detailed architecture is to function. where and how you interface with this part of the brain is critical, he says.this may all be a long way from restoring sight or hearing, but it will at least establish the physiological principles on which sensory neuroprostheses may be based. in the meantime we will find out a lot about how the brain works, says ohl. a.a. hochberg, l. r. et al. nature 442, 164–171 (2006).ads cas article google scholar santhanam, g., ryu, s. i., yu, b. m., afshar, a. & shenoy, k. v. nature 442, 195–198 (2006).ads cas article google scholar scott, s. h. nature 442, 141–142 (2006).ads cas article google scholar brindley, g. s. & lewin, w. s. j. physiol. (lond.) 196, 479–493 (1968).cas article google scholar dobelle, w. h., stensaas, s. s., mladejovsky, m. g. & smith, j. b. rhinol. laryngol. 82, 445–463 (1973).cas article google scholar ohl, f. w. et al. nature 412, 733–736 (2001).ads cas article google scholar download referencesnature's senior european correspondent, alison abbottyou can also search for this author in pubmed google scholar sixth sense can come from within  nerve chip goes live  see no evil, feel no evil reprints and permissionsabbott, a. in search of the sixth sense. nature 442, 125–127 (2006). https://doi.org/10.1038/442125adownload citationpublished: 12 july 2006issue date: 13 july 2006doi: https://doi.org/10.1038/442125aanyone you share the following link with will be able to read this content:sorry, a shareable link is not currently available for this article. provided by the springer nature sharedit content-sharing initiative micro and nano systems letters (2017)american journal of criminal justice (2013)ai & society (2008)journal of neuroengineering and rehabilitation (2007)cognitive neurodynamics (2007)