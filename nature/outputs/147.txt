individual differences in brain functional organization track a range of traits, symptoms and behaviours1,2,3,4,5,6,7,8,9,10,11,12. so far, work modelling linear brain–phenotype relationships has assumed that a single such relationship generalizes across all individuals, but models do not work equally well in all participants13,14. a better understanding of in whom models fail and why is crucial to revealing robust, useful and unbiased brain–phenotype relationships. to this end, here we related brain activity to phenotype using predictive models—trained and tested on independent data to ensure generalizability15—and examined model failure. we applied this data-driven approach to a range of neurocognitive measures in a new, clinically and demographically heterogeneous dataset, with the results replicated in two independent, publicly available datasets16,17. across all three datasets, we find that models reflect not unitary cognitive constructs, but rather neurocognitive scores intertwined with sociodemographic and clinical covariates; that is, models reflect stereotypical profiles, and fail when applied to individuals who defy them. model failure is reliable, phenotype specific and generalizable across datasets. together, these results highlight the pitfalls of a one-size-fits-all modelling approach and the effect of biased phenotypic measures18,19,20 on the interpretation and utility of resulting brain–phenotype models. we present a framework to address these issues so that such models may reveal the neural circuits that underlie specific phenotypes and ultimately identify individualized neural targets for clinical intervention.relating individual differences in brain activity to complex phenotypes is a long-standing aim of human neuroscience, which has been advanced by the application of machine learning algorithms to neuroimaging and phenotypic data. such work has revealed patterns of brain activity that are associated with a range of traits1,2,5, behaviours6,7,8,9, psychopathology10,11,12, clinical risk factors3 and treatment outcomes4 across operationalizations, datasets, age groups and diagnoses. together, this reflects a paradigm shift in human neuroscience research from a focus on the group to a focus on the individual, with important potential applications to clinical practice21,22,23.to deliver on this promise, however, these approaches must identify patterns of brain activity that are relevant to the phenotype of interest in a given individual—the patient sitting before their clinician, for example. previous linear modelling work has relied on the assumptions that (1) a single brain network is associated with a given phenotype, with patterns of activity within that network varying across individuals10,24,25; and (2) larger, more heterogeneous samples will more accurately and reliably capture this single model26,27. but although many published models have demonstrated impressive generalizability6,9,10, they do not account for brain–phenotype relationships in all individuals13,14. this raises the crucial question of in whom models fail, and why.the existence of structured model failure—some individuals who are better fit by a model than others14,24,26—would suggest that one brain–phenotype relationship does not fit all, and that systematic bias may determine who is fit and who is not. this, in turn, may engender imprecise, misleading and in some cases harmful model interpretations. that is, a brain network that is found to be associated with a given phenotype may only apply to a specific subset of the population at large, limiting its practical utility14,26,28, or may not represent the phenotype of interest. indeed, factors that interfere with adequate phenotypic characterization have been documented for many widely used neurocognitive tests18,29, and may include the fallacy of universalism (construct bias), the application of inappropriate norms, discordance between primary and assessment language and the presence of instrument, administration (method) or interpretation bias18,19,20,30,31. related concerns about the ethical implications of data and model bias are receiving increasing attention in the machine learning literature32 (for example, racial disparities33 or unrelated attribute sensitivity34 in facial recognition, and the reflection of biased input data in algorithmic predictions, from criminal justice35,36 to healthcare37,38). however, whether brain–phenotype models are affected by bias in phenotype measurement and, if so, how this bias governs model failure remain open questions. answering them is a prerequisite for discovering precise and useful brain–phenotype relationships.to do so, we trained models to use brain activity to classify neurocognitive test performance and investigated the failure of these models. across a range of data-processing and analytical approaches applied to three independent datasets, we found that model failure is systematic, reliable, phenotype specific and generalizable across datasets, and that the scores of individuals are poorly classified when they ‘surprise’ the model, performing in a way that is inconsistent with the consensus covariate profile of high and low scorers. together, these findings suggest that brain-based models often represent not unitary neurocognitive constructs, but rather constructs of interest intertwined with clinical and sociodemographic factors. these factors comprise a stereotypical profile that does not fit all individuals in the study sample, and may generalize worse still to the population at large39. models that predict this profile will fail in those who defy it. model failure is thus informative, both because it identifies subtypes that require distinct predictive models, and because it offers insight into data and model biases that should guide model interpretation.to examine in whom models fail, we first developed a pipeline to train and test models, using functional connectivity (fc) to predict performance on tests that represent a range of cognitive domains. primary results were derived from a new dataset collected at yale (supplementary tables 1–3 and ‘datasets’ in methods) using four distinct prediction algorithms (fig. 1a, extended data fig. 1, supplementary table 11 and ‘phenotype classification’ in methods), and validated in two independent, publicly available datasets (ucla consortium for neuropsychiatric phenomics17 and the human connectome project (hcp)16; supplementary table 3).a, schematic illustration of the main classification pipeline. classification was performed using leave-one-out (loo) cross-validation. the training set was subsampled from the remaining participants to balance classes and was submitted to a linear support vector machine (svm), using summed fc of selected edges as features. this trained model was then applied to the left-out test participant to classify their score as high or low from their fc. participants who were successfully classified are termed ‘correctly classified participants’ (ccp), and participants who were misclassified are termed ‘misclassified participants’ (mcp). this procedure was repeated iteratively, with each participant used as the test participant, and this, in turn, was repeated 100 times with different training set subsamples selected on each iteration. this pipeline was repeated for each in-scanner condition and neurocognitive measure (numbers correspond to yale study; comparable approach for ucla and hcp). to ensure that the results are robust to these choices, analyses were repeated with alternative algorithms (bagging72 and neural networks); with 10-fold cross-validation; with an alternative parcellation of functional magnetic resonance imaging (fmri) data; with an alternative threshold for score binarization; and with continuous phenotypic measures. see methods, extended data fig. 1 and supplementary table 11 for comparable results. b, classification accuracy for each phenotypic measure, shown separately for high and low scorers and compared to the distribution of accuracy from 100 iterations of permutation tests (‘perm’). significance was determined using the fraction of iterations on which the null classifier performed as well as or better than the median accuracy of unpermuted classifiers (across the whole sample) and resulting one-tailed p values were adjusted for multiple comparisons using the false discovery rate (fdr; 16 tests). distributions and significance testing reflect accuracy across iterations for the best-performing in-scanner conditions, each noted in the plot title. for abbreviations and more on tasks and phenotypic measures, see supplementary tables 1 and 2. for sample sizes, see supplementary table 4.source datafc from the best-performing condition for each measure classified scores on 14 out of 16 phenotypic measures with above-chance performance (mean accuracy = 0.51–0.88; fig. 1b). performance varied across in-scanner conditions that were used to calculate fc (mean accuracy across iterations and measures, using only fc calculated from the best-performing condition = 0.68; mean accuracy across all conditions, iterations, and measures = 0.60; fig. 1b and supplementary table 4). fc also significantly classified phenotype in the ucla data (extended data fig. 2a and supplementary table 5) and hcp data (with family members assigned to the same fold, and permutations respecting family-related limits on exchangeability40,41; extended data fig. 3a and supplementary table 6). however, although the classification accuracy of most phenotypic measures was significantly better than chance, many participants were misclassified (for example, 12–59% of participants across all conditions and measures in the yale dataset; see supplementary tables 4–6 for model performance in each dataset). we turn next to an investigation of structure in these model failures.first, we examined whether misclassification demonstrates non-random structure. we reasoned that if misclassification were random, misclassification frequency (that is, the fraction of iterations on which a given participant was misclassified; see ‘internal validation analyses’ in methods) would be approximately normally distributed around a mean of 0.5. indeed, when phenotypic labels were permuted, the mean misclassification frequency did not differ from 0.5 in 14 out of 16 cases (p > 0.05, fdr adjusted, by two-tailed, one-sample t-test for all measures except symbol search (mean = 0.505, pfdr = 0.02) and letter–number (mean = 0.507, pfdr = 0.0004)). conversely, misclassification frequency using original (unpermuted) data has a markedly different, u-shaped distribution; for most measures, most participants were consistently correctly classified, whereas a smaller subset of participants was consistently misclassified across iterations (fig. 2a). for every measure, the mean and median of the distribution significantly differed from 0.5 (all p < 0.05, fdr adjusted (32 tests), by two-tailed t-test and wilcoxon signed-rank test), and the distributions of misclassification frequency for original and permuted analyses significantly differed for every measure (all p < 0.0001, fdr adjusted, by two-tailed, two-sample kolmogorov–smirnov test).a, histogram of misclassification frequency for each phenotypic measure. each histogram represents misclassification frequency (mf) for each participant, concatenated across in-scanner conditions and presented for analyses using original (that is, unpermuted) data (red) and permuted data (grey). b, condition-by-condition correlation of misclassification frequency for analyses using original (top triangle) and permuted (bottom triangle) data, presented for each phenotypic measure. condition order for individual phenotypic measures as in ‘average’. *significantly different from permuted result correlations, by paired, one-tailed wilcoxon signed-rank test; all p < 0.0001, fdr adjusted (16 tests). r1, rest 1; r2, rest 2; gfc, general fc42; grad, gradcpt73. c, relationship between phenotypic measure similarity (spearman correlation) and misclassification frequency similarity (spearman correlation). each point represents a measure pair (given different participants excluded for intermediate, missing or outlier scores for each measure; number of correlated participants for each measure pair ranges from misclassification frequency: 63 to 114, measure: 105 to 129). d, alternative visualization of misclassification frequency similarity, using a hierarchical linkage tree to reveal that measures that tap into similar constructs yield similar sets of misclassified participants.source datanext, we tested the consistency of misclassification across in-scanner conditions. for all neurocognitive measures except cancellation, correlation of misclassification frequency across conditions for a given measure was significantly greater than chance (that is, than correlation of misclassification frequency derived from permuted-label analyses, by paired, one-tailed wilcoxon signed-rank test; all p < 0.0001, fdr adjusted; fig. 2b), suggesting that the tendency to be misclassified is consistent regardless of the in-scanner condition during which fmri data were acquired.finally, we tested the phenotype specificity of misclassification—that is, whether similar phenotypic measures yield similar sets of misclassified participants. indeed, the more similar the phenotypic measure scores, the more similar the misclassification frequencies of participants for those measures (measure versus misclassification frequency correlation: rs = 0.49, p < 0.0001; fig. 2c). hierarchical linkage demonstrates which measures have a similar misclassification frequency, revealing a logical organization by cognitive construct (fig. 2d).these results were replicated in the ucla and hcp datasets (extended data figs. 2 and 3). in the hcp sample, misclassification frequency did not significantly differ for individuals with and without siblings in the sample (crystallized intelligence (ciq) p = 0.77, fluid intelligence (fiq) p = 0.36, uncorrected, by mann–whitney u test). misclassification frequency and overall classification performance were comparable with additional motion controls (supplementary fig. 1 and supplementary table 7), as well as with different supervised learning algorithms, brain parcellation, cross-validation approach and phenotype binarization threshold (extended data fig. 1). together, these results show that model failure is reliable and phenotype specific.for misclassification to be a meaningful organizing principle and not simply the product of relatively small (yale and ucla) or idiosyncratic samples, it must generalize across datasets. to ensure that this is the case, we trained a model to use gfc42 to classify performance on three neurocognitive measures that are included in both the yale and ucla datasets (letter–number, vocabulary and matrix reasoning). models were trained on all participants, only on within-sample ccp or only on within-sample mcp, and all analyses were performed twice, once with each dataset as the training set and all participants in the other dataset as the test set (‘cross-dataset analysis’ in methods).first, we found that models generalize across dataset. that is, whole-sample-based models classified neurocognitive scores in the whole test sample with above-chance accuracy, as did models that were trained only on ccp (p < 0.0001, fdr adjusted, by nested anova; fig. 3a, ‘train: all and train: correct; test: all’).a, for each of the three measures common to both datasets (ln, mr, vocabulary), six models were trained: one using all yale participants (training set n: 80, 58, 58 for ln, mr, vocabulary, respectively), one using yale ccp (50, 40, 40), one using yale mcp (30, 18, 18), one using all ucla participants (100, 78, 74), one using ucla ccp (64, 48, 50) and one using ucla mcp (36, 30, 24). each model was applied to all high and low scorers in the test dataset (see supplementary tables 4 and 5 for test-set sizes), and the results are displayed as accuracy in all test participants, only in test ccp (test: correct), and only in test mcp (test: misclassified). *significantly different from chance (mean accuracy using permuted data; dotted line presented for visualization only) by two-tailed, nested anova; all p < 0.0001, fdr adjusted (nine tests). bar height, grand mean; error bars, s.d. b, similarity of model pairs, with similarity = 1 − jaccard distance, thresholded at p < 0.05, by the hypergeometric cumulative distribution function. models are divided into edges that are positively and negatively correlated with phenotype to facilitate interpretation. larger, darker circles indicate increased similarity. number of edges in each model (that is, selected on at least 75% of iterations): 30–374. cells shaded on the basis of predicted patterns of similarity. c, each model’s highest-degree node and its incident edges are visualized in all models. models for which the depicted node is the highest-degree node are enclosed in grey rectangles. red, positive relationship with phenotype; blue, negative relationship with phenotype. node size scales with degree, and nodes are coloured red if, of the edges incident to that node, the number of edges positively related to phenotype is greater than or equal to the number of edges negatively related to phenotype; blue otherwise. p, edges positively correlated with phenotype; n, edges negatively correlated with phenotype; um, yale mcp train, ucla test; uc, yale ccp train, ucla test; yc, ucla ccp train, yale test; ym, ucla mcp train, yale test. node number in the shen atlas (mni coordinates).source datanext, across all measures and both datasets, we found that classification outcome (correct versus misclassified) generalizes across dataset. ccp in one dataset were classified with above-chance accuracy by a model trained on all participants or only on ccp in the other dataset (both p < 0.0001, fdr adjusted, by nested anova (see supplementary table 8 for a note on multiple comparison adjustment); fig. 3a, ‘train: all and train: correct; test: correct’). conversely, a model trained on mcp performed with below-chance accuracy on the other dataset’s ccp (p < 0.0001, fdr adjusted, by nested anova; fig. 3a, ‘train: misclassified; test: correct’). similarly, mcp in one dataset were classified with above-chance accuracy only by a model trained on the other dataset’s mcp (p < 0.0001, fdr adjusted, by nested anova), but were classified with below-chance accuracy by models that were trained on the whole training dataset or only on its ccp (both p < 0.0001, fdr adjusted, by nested anova; fig. 3a, ‘test: misclassified’). notably, building group-specific models of phenotypic score (that is, training a model on ccp and applying it to ccp, or training a model on mcp and applying it to mcp, rather than training and testing on the whole datasets) improved classification accuracy (ccp versus whole and mcp versus whole; both p < 0.0001 by paired, one-tailed wilcoxon signed-rank test).in sum, ccp-based models work on another dataset’s ccp, and mcp-based models work on another dataset’s mcp, but ccp-based models in one dataset do not work for mcp in the other, and vice versa. all results were replicated in analogous analyses using the hcp dataset (extended data fig. 3d), and individual models tested for significance using permutation tests yield comparable trends (supplementary table 8). together, these results demonstrate that what it means to be misclassified is consistent across datasets.the significantly below-chance performance when a model was trained on ccp and tested on mcp and vice versa motivated us to further investigate model similarity across groups. first, we trained the model on ccp, as described previously, switched positively and negatively correlated edge sums in the test set and calculated the classification accuracy in test-set mcp, as before, using this ‘inverted model’. this was then repeated with mcp as the training data and the inverted-model accuracy calculated in test-set ccp. in both yale–ucla and yale–hcp analyses, the mean classification accuracy was significantly above chance in both cases (all p < 0.0001, fdr adjusted (18 tests), by nested anova; yale–ucla ccp model applied to inverted mcp: overall mean (s.d.) = 0.64 (0.06); mcp model applied to inverted ccp = 0.64 (0.09); yale–hcp ccp model applied to inverted mcp = 0.76 (0.05); mcp model applied to inverted ccp = 0.76 (0.05)). together, these results show that edges that are positively correlated with phenotype in ccp are negatively correlated with phenotype in mcp, and vice versa.the success of these inverted models suggested that models trained in ccp and mcp would overlap substantially for a given phenotype, albeit with opposite relationships between fc and phenotype. we also expected similarity of a given model across phenotypic measures and datasets. to test these predictions, we calculated the similarity of every cross-dataset model pair as 1 − jaccard distance (‘cross-dataset analysis’ in methods) and visualized the resulting model-by-model similarity matrix, thresholded for significance at p < 0.05, by the hypergeometric cumulative distribution function (fig. 3b). as expected, for a given phenotypic measure, significant similarity is primarily observed across datasets and measures for the same model type (for example, edges positively correlated with phenotype in ccp-based models; ‘pc’ in fig. 3b), and opposite-sign edges in the other classification outcome group (for example, edges negatively correlated with phenotype in yale ccp-based models and positively correlated with phenotype in yale mcp-based models; unc and upm in fig. 3b).model similarity across datasets as well as opposite relationships between fc and phenotype in correct and misclassified groups are also apparent when the selected edges incident to each model’s highest-degree node are visualized (fig. 3c and extended data fig. 4). for example, node 166 is the highest-degree node in both the ucla-test mcp and ucla-test ccp letter–number models, and exhibits similar incident edges, but with largely opposite relationships to phenotype (for example, edges that are positively correlated with letter–number performance in ccp are negatively correlated with performance in mcp). across datasets, similarities can also be observed. for example, similar edges between node 166 and cerebellum were selected for their negative correlations with phenotype in mcp in both ucla and yale data (fig. 3c). however, the inverted nature of the mcp and ccp models does not entirely account for their selected edges, as evidenced by differences in patterns of fc across models. this suggests that mcp and ccp models comprise a core set of overlapping edges as well as sets of distinct edges.given the accumulated evidence that model failure is systematic, with a subset of participants reliably misclassified and misclassification generalizing across measures and datasets, we turn finally to the question of who these misclassified individuals are. to characterize misclassified yale participants, misclassification frequency was related to 15 covariates (characterizing participant demographics, clinically relevant experiences, in-scanner head motion and overall cognitive ability; for a description of how these covariates were selected and their relationships to each other, see ‘exploring contributors to misclassification’ in methods and supplementary fig. 2). misclassification frequency was averaged separately for measures on which participants scored low and high, to yield, for each participant, two mean misclassification frequencies. we note that many of these covariates, particularly race, are proxies that were available, but are neither biological nor causal and obscure much heterogeneity of culture, identity and experience (see ‘causes and implications of model failure’ (below) and ‘additional limitations and future directions’ in supplementary discussion). of these covariates, five were significantly related to misclassification frequency in both low and high scorers (fig. 4a): age (low rs = −0.38, p < 0.0001; high rs = 0.32, p = 0.001), race (low group median difference = −0.14, p = 0.02; high group median difference = 0.19, p = 0.003), education (low rs = 0.32, p = 0.001; high rs = −0.33, p = 0.001), overall cognitive ability (low rs = 0.31, p = 0.007; high rs = −0.45, p < 0.0001) and motion (low rs = −0.40, high rs = 0.48, both p < 0.0001; all p values fdr adjusted).a,b, data are shown for all covariates that were found to have significant pairwise relationships with misclassification frequency by two-tailed rank correlation and mann–whitney u test. a, relationship with misclassification frequency, averaged separately across measures on which participants scored high (‘high scorer’) and low (‘low scorer’). *significant (p < 0.05) in corresponding full regression of low- or high-scorer misclassification frequency on these covariates. b, relationship with mean scores, averaged separately across measures on which participants were frequently correctly (top) and incorrectly (bottom) classified. *significant (p < 0.05) in full regression of mean (correct or misclassified) score on these covariates. lines and shading: best-fit line from simple linear regression with 95% confidence bands. violin plot lines represent median and quartiles. box plot centre line and hinges represent median and quartiles, respectively; whiskers extend to most extreme non-outliers. all reported p values fdr adjusted (a, 30 tests; b: 8 tests). see supplementary tables 9 and 10 for relationships between misclassification frequency, mean score and all tested covariates, as well as sample sizes. rg, racialized groups.source datatogether, these covariates reflect a stereotypical profile that the model detected and used, with a high misclassification frequency in participants who defied this profile. for example, in ccp, an increased amount of education was associated with an increased mean score on the neurocognitive battery (rs = 0.48, p < 0.0001, fdr adjusted; fig. 4b). correspondingly, individuals with low neurocognitive scores and high education, or with high scores and low education, were frequently misclassified. in keeping with this stereotype-defying profile, the relationship between education and mean score was substantially diminished among mcp, and the relationships between mean score and age, race and motion—all significant in ccp (fig. 4b, ‘correct’)—were abolished in mcp (fig. 4b, ‘misclassified’). together, these results suggest that models reflect stereotypical demographic profiles of high and low scorers that, when violated, result in misclassifications.trends in the ucla and hcp data using comparable, available covariates are similar (extended data figs. 2e and 3e and supplementary figs. 3 and 4). notably, in the ucla sample, self-reported symptom severity and use of psychiatric medication were related to misclassification frequency (supplementary table 9), and low scorers who did not have a mental health diagnosis (assessed by diagnostic interview) were slightly, although not significantly, more likely to be misclassified than low scorers who did (misclassification frequency median difference = 0.25, p = 0.07). symptom severity, medication status and diagnosis in yale and ucla datasets were in many cases related to mean score in ccp, with illness tracking worse performance (supplementary table 10). there was no evidence of a relationship between symptom severity and mean score in the hcp sample (both p > 0.1; supplementary table 10), which focused recruitment on healthy individuals. together, these results reflect both the potential relevance of mental illness to stereotypical profiles and misclassification frequency, and the dataset-specificity of these profiles and who defies them (see ‘causes and implications of model failure’, below). relationships between misclassification frequency, mean score and all tested covariates are presented in supplementary tables 9 and 10.finally, we considered two additional questions raised by these findings. first, we compared the fc patterns of ccp and mcp groups to identify any group differences in functional brain organization that may explain misclassification. we found no consistent differences between groups at either the edge or the network levels (extended data fig. 5). second, these findings raise the concern that models reflect only demographic and clinical variables, not the neurocognitive constructs of interest. to investigate this, we regressed gfc edge summary scores on phenotypic scores and covariates. in all cases, the full model was preferred to a reduced model that included only score (all p < 0.0005 by extra sum-of-squares f-test). score was in most cases significantly associated with edge summary score after controlling for all covariates, and covariates associated with edge summary score were, as expected, highly overlapping with those that track misclassification frequency (extended data table 1).in this work, we interrogate model failure to better understand group differences in brain–phenotype relationships. across three datasets, a wide range of predicted measures and various analysis approaches, we find notably consistent results: model failure occurs reliably in a subset of individuals, generalizes across phenotypic measures and datasets and is associated with phenotype scores that do not fit the sample’s stereotypical profile for high and low scorers.together, these results show that one model does not fit all; model failure identifies subgroups that require distinct predictive models (see ‘model failure as a tool for subtyping’ in supplementary discussion). furthermore, they show us that we are often predicting not unitary outcomes, but rather outcomes of interest intertwined with constellations of covariates. this is crucial to acknowledge, both because these stereotypical profiles can teach us about the predicted construct and its potential biases, and because these profiles have practical and conceptual implications for model generalizability (stereotypes, and thus models, do not fit all individuals) and model interpretation (identified brain activity patterns may represent elements or consequences of this profile, not the phenotype of interest, per se). model failure is thus inextricably intertwined with biases in input data, and these issues must be jointly addressed if brain–phenotype models are to yield useful neuroscientific and clinical insights. we turn next to a discussion of these points, and close with a proposed framework for future work.our results suggest that brain activity-based models are often predicting complex profiles rather than unitary cognitive processes, highlighting the need to consider these profiles and the influence of sample representation on them. for example, a sample of same-age individuals would not demonstrate a relationship between age and performance, whereas targeted recruitment of groups with distinct medical health histories may render this variable relevant to performance and misclassification. in keeping with this intuition, the hcp sample, which selectively recruited healthy individuals, did not show a relationship between psychopathology and test performance, a relationship that exists in both the yale and the ucla samples.other effects of sample representation on phenotypic score profiles are suggested by many intersecting literatures. cultural influences on task strategies and test performance are well documented43,44, and neuropsychological test performance differs by factors such as life course epidemiology, education quality, acculturation and physical health45,46,47. many tests are thus composite measures20, and it is these composites that our models are predicting.furthermore, relationships between covariates and the outcome of interest may be complex, and differentially affect brain–phenotype relationships (see ‘covariate–outcome relationships may be varied and complex’ in supplementary discussion). verbal memory offers an illustrative example of group differences in covariate–outcome relationships and a use-case for the utility of subgroup-specific modelling. in native english speakers, the articulation rate of subvocal rehearsal tracks digit and word span performance. this relationship is substantially attenuated in native mandarin speakers, who also perform better overall than english speakers on the task48. together, these findings suggest that there is a meaningful difference in the cognitive processes that are associated with verbal memory across cultural groups. to account for this, brain–verbal memory modellers may build separate models for these groups. this would be likely to increase model performance for each group, consistent with our results (fig. 3) and with the previous finding that matching training and test data for confounding relationships maximizes classification accuracy49. it would also reveal whether these group-specific models track distinct processes (that is, neural correlates of articulation rate in english speakers, and of other factors—such as nonverbal rehearsal processes or increased capacity of the phonological store—in mandarin speakers48). by situating the interpretation of verbal memory test scores—and of corresponding brain-based predictive models—in the context of this existing literature, more appropriate models may be built for each group and a more nuanced interpretation of these models achieved.the existence of factors that track performance in one group but not in another is consistent with our finding that, overall, the same covariates that track misclassification also track score in ccp but not in mcp. because models are predicting profiles of which these covariates are a part, individuals who defy the profile will require a different brain–phenotype model. in all three studied samples, for example, individuals with more education tended to score higher on neurocognitive tests, but this correlation was not perfect. a substantial number of individuals with low education scored high, and vice versa, and these individuals were frequently misclassified in the yale and hcp samples (fig. 4a, extended data fig. 3e and supplementary tables 9 and 10). the observation of this pattern in both the clinically heterogeneous yale and healthy hcp samples suggests that it cannot be exclusively explained by disease processes. these cases thus present an opportunity to study potential correlates of resilience, obstacles to performance and alternative cognitive strategies.equally important and not mutually exclusive is the opportunity to use such cases, and the profiles they defy, to explore sources of bias encoded in input data. that is, if test scores are themselves biased, the models may be as well. such model bias has been described in applications of machine learning algorithms ranging from criminal justice35,36 to healthcare37,38. care must be taken to interpret results accordingly. for example, african american and hispanic or latino american individuals tend to score lower than white americans without hispanic or latino ancestry on neuropsychological tests46. these group differences are complex, often reductionist and non-causal; efforts to explain them have focused on differences in factors such as education quality50, acculturation47, neighbourhood disadvantage51 and research methodology18,52. although consensus causal explanations remain an open question, the pervasiveness of such bias in commonly used phenotypic measures18,20 is a call to action to carefully consider what brain-based models are truly predicting. indeed, race tracked neuropsychological test performance in all three studied samples. and despite the fact that our models had no access to information about race, race was related to misclassification frequency in the yale and hcp samples, such that high-scoring participants who identify with racialized groups (see methods) were frequently misclassified as low scoring, and vice versa for white participants. this finding is reminiscent of the errors made by the correctional offender management profiling for alternative sanctions (compas, now ‘equivant’) system36 and of recent evidence for ‘prediction shift’ in african american individuals14.we seek to avoid overinterpretation of these findings and note again that race is a non-causal, non-biological proxy for unmeasured variables that obscures much heterogeneity in these samples (see ‘additional limitations and future directions’ in supplementary discussion). what our results do reveal is unintended and easily missed bias in both model inputs (that is, phenotypic measures limited by available assessment tools18,20, such as those comprising the nih toolbox53,54) and model outputs (that is, the profiles to which models correspond). this bias matters for two reasons: (1) it may yield the right predictions for the wrong reasons; researchers may interpret the model as the neural representation of a unitary phenotypic construct or may acknowledge the role of covariates but wrongly assume causality; and (2) it determines the limits of model generalizability, which in turn guide the practical applications of the models. given, as we demonstrate, that models represent a composite profile, and that model generalizability will be limited to the group that fits this profile, it is critical for modellers to characterize these stereotypical profiles in their samples along multiple and intersecting dimensions (see specific recommendations in ‘limitations and future directions’, below).notably, we show that misclassified individuals—those who defy the consensus score profile—do not have a distinct brain organization; rather, it is the relationship between brain and phenotype that differs between ccp and mcp. specifically, mcp do not require an entirely distinct model to classify their phenotypes. this finding ran contrary to our expectation that edges relevant to phenotype in ccp would not be systematically related to phenotype in mcp, and that misclassification would thus identify groups with a different macroscale neural circuitry underlying a given phenotype. rather, mcp positively correlated edges overlap significantly with ccp negatively correlated edges and vice versa, and simply inverting the model trained on one group yielded successful classification of the other, suggesting a stereotype-based ingroup–outgroup dichotomy for each model. it is possible, however, that in a more demographically homogeneous sample, the influence of these stereotypical profiles would be minimized and more nuanced group differences in phenotype-relevant circuitry would be observable.in addition, we do not seek to suggest that these predictive models reflect only a constellation of covariates; there is likely to be variance in phenotype and shared variance between brain and phenotype that are attributable to the cognitive processes of interest. indeed, there is an extensive psychometric literature describing the construct validity of neuropsychological measures55, and several recent studies have demonstrated the phenotypic specificity of fc-based predictive models8,56. in the yale, ucla and hcp data, even after controlling for all included covariates, the relationship between brain and phenotype remained significant in most cases (extended data table 1). the macroscale circuits revealed by fc-based predictive modelling can thus be interpreted as the neural representation of a complicated mixture of the construct of interest and a range of sample-dependent demographic and clinical variables.disentangling these relationships presents an important and broad opportunity for future research. these issues are relevant to work that relates neural and phenotypic data at all levels of analysis, and thus will not be limited to human neuroscience; as individual differences gain increasing attention in cellular and systems neuroscience57, precise phenotypic characterization and model interpretation will be paramount. our results thus encourage each modeller to collect the data necessary to identify and, to the extent possible, correct stereotypical profiles for a given phenotype in their sample.doing so must begin with study design. given the importance of sociodemographic and clinical covariates to brain–phenotype modelling analyses, future work should further characterize score profiles, looking to best-practice guidelines to collect more expansive and inclusive demographic data58, increase the enrolment of underrepresented groups and exchange proxies such as race for more meaningful causal or explanatory variables59,60,61,62,63,64 (see ‘additional limitations and future directions’ in supplementary discussion for more on the use and characterization of race in this work). in the service of result generalizability and as a proof of principle, we present the covariates that are related to model failure across all studied phenotypic measures, but such future work will permit the identification of more precise and phenotype-specific stereotypical profiles (‘additional limitations and future directions’ in supplementary discussion). in parallel, phenotypic measures must be carefully selected and administered to maximize their validity18,52. these choices may be guided by tools to evaluate the risk of bias in study design (for example, probast, step 365).then, once data are collected, they must be used. that is, modelling analyses must be adapted to ask what combination of factors our outcome of interest measures, and how we can interpret related patterns of brain activity. first, statistical tools may be used to isolate, to the extent possible, the phenotype of interest. when standardizing phenotypic measures, norms should be carefully considered to ensure appropriateness (for example, for the nih toolbox66,67, but see refs. 53,54). furthermore, data may be corrected for identified confounds. many approaches to confound correction rely on the assumption that a single covariate–outcome relationship holds for all individuals in the sample. if this is not the case, as we show here, then such correction will fail, and may even induce a confounding relationship where in truth none exists68. to address this issue, more sophisticated correction approaches that account for sample-specific stereotypes will be necessary (for example, the use of crossed-term confounds, confound-based sample splitting68, inverse probability weighting69 or post-hoc confound control70). inevitably, however, confounds will remain. it is incumbent on the modeller to use the previously collected, comprehensive sociodemographic data to precisely characterize these persistent confounds and interpret resulting models accordingly: as group-specific neural representations of composite phenotypes. see extended data fig. 6 for a summary of these steps.to ignore these issues is to risk missing structured model failure, and the development of models that only apply to a specific—but uncharacterized—slice of the population. only by integrating standard model evaluation criteria (for example, accuracy, sensitivity and specificity) with more thorough investigations of model failure can we hope to define the population to which each model generalizes71 and move away from the limitations of a one-size-fits-all approach. that models pick up on and use stereotypical profiles is not always, in itself, a problem for data-driven studies of brain–phenotype relationships. however, we must characterize these profiles to identify potentially harmful biases and to know whether and how a given model applies to the individual sitting before us. doing so opens a world of possible applications for brain-based predictive models, chief among them the identification of neuromarkers that both shed light on the biological basis of disease and guide intervention.three datasets were used in these analyses. primary analyses use data collected at yale from february 2018–march 2021, and external validation analyses use the ucla consortium for neuropsychiatric phenomics (cnp)17, which is of comparable size to the yale dataset and includes individuals with mental health diagnoses, and the human connectome project (hcp)16, which includes only healthy participants and is substantially larger (thus addressing concerns about mean and variance of predictive model accuracy in small samples74,75; although it is still not as large as samples called for in recent work27, we note that the concerns raised in that study are not directly relevant to our work, as the former focuses on within-sample brain–phenotype associations, whereas our analyses rely exclusively on prediction, presenting brain–phenotype relationships that generalize to unseen data). each dataset is described below, with demographic and clinical information for each reported in supplementary table 3 and extended data fig. 7. together, these datasets comprise nearly 1,000 individuals, and each is of comparable size to or larger than datasets that are commonly used in brain–phenotype modelling work (for example, refs. 1,6,10).participants completed an mri scan followed by a post-scan neuropsychological and self-report battery; the scan and post-scan battery were each approximately 2 h in length, and, to cover a broad cognitive landscape, were designed to correspond to research domain criteria (rdoc)76 domains and constructs (supplementary tables 1 and 2).all imaging data were acquired on three harmonized siemens 3t prisma scanners with a 64-channel head coil at the yale magnetic resonance research center. for alignment to common space, a high-resolution t1-weighted three-dimensional anatomical scan was acquired using a magnetization-prepared rapid acquisition with gradient-echo (mprage) sequence (208 slices acquired in the sagittal plane, repetition time (tr) = 2,400 ms, echo time (te) = 1.22 ms, flip angle = 8°, slice thickness = 1 mm, in-plane resolution = 1 mm × 1 mm). functional data were acquired using a multiband gradient-echo-planar imaging (epi) sequence (75 slices acquired in the axial-oblique plane parallel to the ac–pc line, tr = 1,000 ms, te = 30 ms, flip angle = 55°, slice thickness = 2 mm, multiband factor = 5, in-plane resolution = 2 mm × 2 mm).participants completed eight functional scans—two resting-state runs and six task runs—each 6 min, 49 s long (including an initial shim). the first and last functional scans were resting-state runs; participants were asked to rest with their eyes open, and a fixation cross was displayed. participants completed each of the six tasks during the remaining runs, with task order counterbalanced across participants. tasks were presented using psychtoolbox-377.for a detailed description of the design of each task, see supplementary table 1 and previously published work78. in brief, the tasks included adaptations of: (1) 2-back79 (working memory; adapted from a previous study80; one block of scene stimuli and one block of emotional face stimuli81,82); (2) stop signal (response inhibition)83; (3) card guessing (reward)84,85; (4) gradual-onset continuous performance (gradcpt; sustained attention)73; (5) reading the mind in the eyes (social)86; and (6) movie watching tasks. each task, with the exception of movie watching, was preceded by instructions and practice, after which the participant had an opportunity to ask questions about the task. responses were recorded on a 2 × 2 button box. a fixation cross was displayed between tasks.after the scan, participants completed a battery of scales selected from extensively validated neuropsychological and self-report measures87,88,89,90,91,92,93,94,95,96,97,98,99,100 (supplementary table 2), as well as a structured diagnostic interview101, all administered by research assistants and graduate students trained by a clinical neuropsychologist. all phenotypic data were hand-scored by two independent raters, and age-adjusted, normed neuropsychological test scores (iq, scaled, t and z scores, as relevant), derived from corresponding test manuals, were used in subsequent analyses. further demographic adjustments were not used given their variable availability and utility in a research setting18,54,102,103, but we note that such adjustments—which are often used in clinical settings—can increase sensitivity to neurocognitive deficits, control for variance associated with premorbid factors and improve performance interpretation, making them an important area for future investigation, particularly in the study of acquired cognitive changes67 (see ‘limitations and future directions’ in ‘causes and implications of model failure’). a small subset of participants was recruited for a related study of bipolar disorder; their study sessions were comparable with the exception of an additional in-scanner task (not included in these analyses) and the use of the structured clinical interview for dsm-5104 instead of the mini.standard preprocessing procedures were applied to imaging data. structural scans were skull stripped using an optimized version of the fmrib’s software library (fsl)105 pipeline (optibet)106. motion correction was performed in spm12107. nonlinear registration of the mprage to the mni template was performed in bioimage suite108, and linear registration of the functional to the structural images was performed using a combination of fsl and bioimage suite linear registration tools to optimize registration quality. all additional preprocessing steps were performed in bioimage suite, and included regression of mean time courses in white matter, cerebrospinal fluid and grey matter; high-pass filtering to correct linear, quadratic, and cubic drift; regression of 24 motion parameters109; and low-pass filtering (gaussian filter, σ = 1.55). all registered data were visually inspected to ensure whole-brain coverage, adequate registration and absence of anomalies, artefact or other data quality concern. subsequent analyses and visualizations were performed in bioimage suite, matlab (mathworks), graphpad prism and r110 (packages: ggpairs111, corrplot112 and r.matlab113).participants were recruited through broadly distributed community advertisements and referrals from yale clinics, with an emphasis on recruiting a clinically naturalistic and demographically diverse sample. that is, participants experienced a range of symptom severity and frequently had multiple psychiatric diagnoses, ages were distributed broadly across the lifespan and the sample was enriched for mental illness and identification with racialized groups (supplementary table 3 and extended data fig. 7). note that we use this phrase to describe participants who did not identify as white, in keeping with the literature describing ‘racialization’ as the process of categorizing, marginalizing, or regarding according to race with socialized tendencies to view white race as the default, and in recognition that such categorization is without meaningful biological distinction114. all participants provided written informed consent in accordance with a protocol approved by the yale institutional review board.we restricted our analyses to those participants who completed all fmri scans (six task, two rest), whose grand mean frame-to-frame displacement was less than 0.15 mm and whose maximum mean frame-to-frame displacement was less than 0.2 mm. several additional participants were excluded because they were found to have an anatomical anomaly that interfered with registration or because of technical issues during their session that compromised data quality. several participants did not complete all neuropsychological and demographic measures; they were included in the sample but excluded from specific analyses for which they were missing data. similarly, for several participants, a slightly shorter protocol (25 s shorter than the standard protocol) was used for all or a subset of functional scans; given the known explanation for the missing time points, these participants were not excluded. in total, the sample includes 129 participants (supplementary table 3 and extended data fig. 7).participants completed both resting-state and task-based fmri scans, as well as an out-of-scanner battery of neurocognitive and self-report measures. in this study, we used imaging data from all cognitive tasks and rest, data from three of the out-of-scanner neurocognitive measures98 and sociodemographic and clinical covariates115,116. all data were accessed through openfmri (accession number ds000030).details of the study design and in-scanner tasks are described in previously published work17. in brief, imaging data were acquired on two 3t siemens trio scanners. from the imaging data, as for the yale data, we used the mprage (176 slices acquired in the sagittal plane, tr = 1,900 ms, te = 2.26 ms, slice thickness = 1 mm, field of view (fov) = 250 mm, matrix size = 256 × 256) and epi (34 slices acquired in the oblique plane, tr = 2,000 ms, te = 30 ms, flip angle = 90°, slice thickness = 4 mm, fov = 192 mm, matrix size = 64 × 64) scans.in this work, we used fmri data acquired during seven runs: eyes-open rest and six tasks performed over two, counterbalanced scan sessions. runs varied in length. cognitive tasks included: (1) balloon analogue risk task (bart); (2) paired-associate memory task (2 scans, one each for the encoding (pamenc) and retrieval (pamret) components of the task); (3) spatial working memory capacity (scap) task; (4) stop signal task (sst); and (5) task-switching (ts) task17. participants received training on each task immediately before the scan. responses were recorded on a button box.participants also underwent a neuropsychological and clinical assessment. from these tests, we used three phenotypic measures that are the same as or comparable to measures used in the yale dataset: wais-iv vocabulary, wais-iv matrix reasoning and wais-iv letter–number sequencing. age-adjusted scaled scores obtained from the wais-iv manual were used for all analyses.preprocessing procedures were similar to those used on the yale dataset (minor differences include use of spm8 and the middle, rather than the first, scan as reference for motion correction; an earlier version of the fsl and bioimage suite linear registration protocol; and a gaussian low-pass filter with σ = 1), and have been described previously9. all registered data were visually inspected to ensure whole-brain coverage, adequate registration and absence of anomalies, artefact or other data quality concern.participants were recruited through community advertisements and outreach to clinics; the sample was enriched for mental illness, with some comorbidities being grounds for exclusion17. all participants provided written informed consent consistent with procedures approved by the institutional review boards at ucla and the los angeles county department of mental health. we restricted analyses to participants with complete rest, bart, pamenc, pamret, scap, sst and ts runs, with grand mean frame-to-frame displacement less than 0.15 mm and maximum mean frame-to-frame displacement less than 0.2 mm, and without nodes entirely lacking coverage (see ‘functional parcellation and network definition’), leaving a final sample of 163 participants (supplementary table 3 and extended data fig. 7).as in the yale and ucla datasets, participants completed both resting-state and task-based fmri scans, as well as an out-of-scanner battery of neurocognitive and self-report measures. in this study, we used imaging data from all tasks and rest, as well as summary scores from the nih toolbox assessments117, and sociodemographic and clinical covariates matched, to the extent possible, to those used in primary analyses95,117,118. all data were released as part of the hcp 900 subjects release and are publicly available on the connectomedb database (https://db.humanconnectome.org).details of the study design, imaging protocol and in-scanner tasks have been extensively described16,119,120,121. in brief, all mri data were acquired on a 3t siemens skyra using a slice-accelerated, multiband, gradient-echo, epi sequence (72 slices acquired in the axial-oblique plane, tr = 720 ms, te = 33.1 ms, flip angle = 52°, slice thickness = 2 mm, in-plane resolution = 2 mm × 2 mm, multiband factor = 8) and a mprage (256 slices acquired in the sagittal plane, tr = 2,400 ms, te = 2.14 ms, flip angle = 8°, slice thickness = 0.7 mm, in-plane resolution = 0.7 mm × 0.7 mm).in total, 18 fmri scans were conducted for each participant (working memory (wm) task, incentive processing (gambling) task, motor task, language processing task, social cognition task, relational processing task, emotion processing task and two resting-state scans; two runs per condition (one left-to-right (lr) phase encoding run and one right-to-left (rl) phase encoding run))120,121 split between two sessions. participants received instructions for each task outside of the scanner, as well as a brief reminder of the task instructions and button box response mappings immediately prior to each task.participants also completed an extensive out-of-scanner battery, including most of the nih toolbox measures121. given the lack of exact correspondence between the nih toolbox and the neuropsychological tests used in the yale and ucla datasets, we used the toolbox composite age-adjusted scaled scores (variables: cogfluidcomp_ageadj and cogcrystalcomp_ageadj), corresponding broadly to fluid cognitive functions and verbal reasoning, respectively.preprocessing procedures have been described previously24. the hcp minimal preprocessing pipeline was used on these data122, which includes artefact removal, motion correction and registration to standard space. all subsequent preprocessing was performed in bioimage suite108 and included standard preprocessing procedures1, including removal of motion-related components of the signal; regression of mean time courses in white matter, cerebrospinal fluid and grey matter; removal of the linear trend; and low-pass filtering. mean frame-to-frame displacement was averaged for the lr and rl runs, yielding nine motion values per participant; these were used for participant exclusion and motion analyses.participants were recruited from families containing twins in missouri. participants were healthy, broadly defined, and reflective of the ethnic and racial composition of the usa as documented in the 2000 census16. the scanning protocol (as well as procedures for obtaining informed consent from all participants) was approved by the institutional review board at washington university in st louis. we restricted analyses to participants with complete rest and task runs, with complete zygosity data (necessary to respect family-related limits on exchangeability for permutation testing40,41), with grand mean frame-to-frame displacement less than 0.15 mm and maximum mean frame-to-frame displacement less than 0.2 mm, and without nodes entirely lacking coverage. we further excluded participants with identified quality control issues b–d or other major issues described on the hcp wiki as of october 2021 (for example, rl runs processed with the incorrect phase encoding direction), or who failed preprocessing, leaving a final sample of 664 participants (supplementary table 3 and extended data fig. 7).the shen 268-node atlas derived from an independent dataset using a group-wise spectral clustering algorithm123 was applied1 to the preprocessed yale, ucla and hcp data. after parcellating the data into 268, functionally coherent nodes, the mean time courses of each node pair were correlated and correlation coefficients were fisher transformed, generating eight 268 × 268 connectivity matrices per yale participant, seven 268 × 268 connectivity matrices per ucla participant (one per fmri run), and nine 268 × 268 connectivity matrices per hcp participant (averaged across rl and lr runs for each in-scanner condition).to ensure that results are robust to parcellation choice, we repeated analyses using the shen 368-node atlas, derived using a combined approach: data-driven parcellation of cortical areas, anatomic delineation of subcortical regions and a cerebellum parcellation based on the yeo 17-network parcellation124 (extended data fig. 1).a modified version of connectome-based predictive modelling25—which crucially tests models on previously unseen data (unlike explanatory models13,15)—was used to classify phenotypic scores as high or low using fc. first, phenotypic scores were binarized to generate unambiguous classification outcomes and avoid common sources of bias in comparisons of observed and predicted outcomes125. to ensure that all participants understood and performed the neurocognitive tests as intended, we used normative means and standard deviations for each measure to exclude outlier extremely low scorers (score ≤ mean – 3 × s.d.)126, as is common and recommended practice127,128,129. scores less than or equal to the normative mean – 1/3 × s.d. were considered low (label = −1); scores greater than or equal to the normative mean + 1/3 × s.d. were considered high (label = 1); thresholds were rounded as relevant. to ensure that results are robust to this thresholding choice, we repeated analyses using the normative mean as the cut-off (that is, high if greater than mean; low if less than mean). for results, see extended data fig. 1 (‘phenotype mean split’). leave-one-out cross-validation was used, such that one participant was left out as test data and training data were selected from the remaining participants (for results using 10-fold cross-validation, see extended data figs. 1 and 3). only high and low scorers were classified, and from the training set, only high and low scorers were considered. from that subset, the larger class was pseudorandomly undersampled to enforce balanced classes15, and the resulting subset was used to train the classifier.to do so, each edge across all training participants was correlated with their labels for the given phenotypic measure. using a significance threshold (p < 0.05, uncorrected25), two sets of edges were selected—one positively correlated with labels, and one negatively correlated with labels. those edges were separately summed to derive two edge summary scores for each training participant, one each for the positively and negatively correlated edge sets. these scores were in turn normalized (z-scored) and submitted to a linear svm to classify low versus high scores in the training set. selected positively and negatively correlated edges were then summed and normalized in the test data, and the trained classifier was applied to these scores to classify the test participant(s) as high or low scoring. to ensure that results are robust to classification algorithm, we repeated these analyses using two additional, commonly used algorithms for supervised learning that, together with linear svm and linear regression (see below), reflect the full range of model interpretability and complexity130: an ensemble of weak learners and a fully connected neural network, both implemented in matlab (mathworks). in both cases, we used a subset of phenotypic measures for hyperparameter optimization (ensemble learners: ensemble aggregation method, number of learners, learning rate (where relevant) and minimum leaf size; neural network: activation functions, standardization, regularization term strength and layer sizes). we used all available high and low scorers’ gfc and phenotypic data, undersampled to balance class size, in a leave-one-out manner for each optimization analysis, and used the best consensus hyperparameters (ensemble learners: method = bagging, number of learners = 150, minimum leaf size = 1; neural network: activation function = none; standardization = true; lambda = 1.34 × 10−5; layer sizes = 8, 200 and 8) to classify all 16 phenotypic measures as in main analyses, using all selected edges (correlation p < 0.05, as in main analyses) as features. see extended data fig. 1 for results.test participants were considered misclassified (mcp) versus correctly classified (ccp) if their predicted label did not match their observed label. classification was repeated iteratively, with each participant excluded once, and overall accuracy was calculated as the number of correctly classified participants divided by the number of classified participants. accuracy was also calculated separately for high and low scorers to show that performance was comparable in both groups.this process was repeated 100 times, with distinct subsampling on each iteration (for 10-fold analyses, 50 iterations of subsampling were performed for each 10-fold partition with 20 partitions, yielding 1,000 iterations). the entire pipeline was repeated for every combination of in-scanner condition (rest, tasks and gfc42 (without regression of task structure131)) and phenotypic measure.the significance of classifier performance was assessed by 100 iterations of permutation testing (or, in the case of hcp 10-fold analyses, 1,000 iterations; see above). that is, the classification pipeline was repeated with one modification: high and low phenotypic scores were permuted on each iteration. permutations were fixed across in-scanner conditions, and respected family-related limits on exchangeability for the hcp dataset40,41. p values were calculated as: pi = #{ai,null ≥ ai,median} + 1/(no. iters + 1), in which i indexes the phenotypic measure and ai is the classification accuracy for measure i, with #{ai,null ≥ ai,median} indicating the number of iterations on which the permutation-based classifiers performed as well as or better than the median accuracy of unpermuted data-based classifiers. the resulting p values were fdr adjusted for multiple comparisons (number of tests = number of phenotypic measures in the given dataset; one in-scanner condition per measure; for all conditions, see supplementary tables 4–6).finally, to ensure that dichotomization of continuous neurocognitive scores did not bias results, leave-one-out cross-validated connectome-based predictive modelling was performed1,25, using fc from each in-scanner condition to predict performance on each of the 16 yale phenotypic measures. the difference between predicted and observed scores was deconfounded by regressing it on observed scores125,132, and this residualized model fit metric, averaged across in-scanner conditions and phenotypic measures, was related to covariates in place of mean misclassification frequency (see ‘exploring contributors to misclassification’). results were comparable to those from main analyses and are presented in supplementary table 11.the consistency of mcp was explored at several levels of analysis. first, misclassification frequency for a given phenotypic measure and in-scanner condition was calculated for each classified participant as the number of misclassifications divided by the number of iterations. for each phenotypic measure, the original and permuted distributions of misclassification frequency, concatenated across all in-scanner conditions, were compared (fig. 2a).the results for each in-scanner condition were then examined separately by rank correlating misclassification frequency for each condition pair for a given phenotypic measure (fig. 2b). given the directional hypothesis, original and permuted correlations were compared by paired, one-tailed wilcoxon signed-rank test; p values were fdr adjusted for multiple comparisons across the phenotypic measures (for example, 16 tests for the yale dataset).finally, misclassification frequency across phenotypic measures was compared. to do so, misclassification frequency for a given in-scanner condition was rank correlated across phenotypic measures. the resulting phenotype × phenotype misclassification frequency correlation matrices were averaged across in-scanner conditions. separately, normed scores for each phenotype pair were rank correlated, yielding a phenotype × phenotype similarity matrix. the lower triangles of these matrices were plotted and rank correlated (fig. 2c), and misclassification frequency correlations were submitted to a hierarchical clustering algorithm (single linkage), with distance = 1 − corr (fig. 2d). in all cases, nan values (that is, participants with intermediate (not labelled high or low), low outlier, or missing scores on the relevant measures) were excluded.results were replicated in the ucla and hcp datasets, as relevant (extended data figs. 2 and 3).to explore the generalizability of misclassification, the classification analysis was adapted such that one dataset was used to train the classifier, and another to test it. in primary analyses using the yale and ucla datasets, letter–number, vocabulary and matrix reasoning tests were used as phenotypic measures because they represent relatively distinct cognitive domains and are included in both datasets. gfc was used given the in-scanner task differences across datasets. as described previously (‘phenotype classification’), low outlier scores were excluded, normed phenotypic scores were binarized and training data were subsampled to ensure balanced classes. for each phenotypic measure, six models were trained using (1) all ucla high and low scorers; (2) only ccp ucla high and low scorers; (3) only mcp ucla high and low scorers; (4) all yale high and low scorers; (5) only ccp yale high and low scorers; and (6) only mcp yale high and low scorers. correct and misclassified participant sets were derived from the gfc-based iteration with the median classification accuracy for the given phenotypic measure (ties settled by maximum correlation with relevant misclassification frequency). in each case, edges were selected on the basis of their correlation with binarized phenotype, and a sparsity threshold was used to facilitate comparison across models given differing sample sizes (that is, 500 most correlated edges; 500 most anticorrelated edges). as before, edge strengths were summed and normalized, and the resulting summary scores were submitted to a linear svm. trained models 1–3 were applied to all yale high and low scorers; models 4–6 were applied to all ucla high and low scorers. as in within-dataset analyses, given subsampling to balance classes, each analysis was repeated 100 times with different subsamples. results are presented as the mean (across 100 iterations) fraction of the whole sample that was correctly classified by each model, the mean fraction of each ccp group that was correctly classified (test: correct) by each model and the mean fraction of each mcp group that was correctly classified (test: misclassified) by each model (fig. 3a). within each category (for example, train: all/test: all, train: correct/test: correct, train: correct/test: misclassified, and so on), a nested anova was used to compare means to chance (mean accuracy for corresponding analyses using permuted phenotypic data), and p values were fdr adjusted across the nine result categories (an adjustment that we note is complicated by the non-independence of the test sets, but which we use as a more conservative estimate of significance than uncorrected p values). each model’s performance was also tested for significance using permutation tests (supplementary table 8), and group-specific (ccp- and mcp-based) model performance was compared to whole-sample-based model performance by paired, one-tailed wilcoxon signed-rank test, given the directional hypothesis that group-specific models outperform whole-sample models.edges selected on 75–100% of iterations were compared by jaccard distance (fig. 3b) and visualized using bioimage suite (edges incident to the highest-degree node for each model; illustrative example: fig. 3c; all cross-dataset models: extended data fig. 4). the above approach for identifying ccp and mcp groups was also used to explore between-group differences in fc in the yale sample at the edge (by two-sample t-test) and network (by the constrained network-based statistic133) levels (extended data fig. 5). cross-dataset classification results were replicated using the yale and hcp datasets (vocabulary/ciq and matrix reasoning/fiq measures; extended data fig. 3d and supplementary table 8).to characterize frequently misclassified participants and better understand why they are misclassified, we related 12 covariates (sex, age, self-reported race (binarized given limited sample size, with ‘white’ indicating that this category was selected as the participant’s only racial group; see ‘additional limitations and future directions’ in supplementary discussion), years of education, self-reported psychiatric symptoms (through the brief symptom inventory92 global severity index), self-reported stress (through the perceived stress scale93), self-reported sleep disturbance (through the pittsburgh sleep quality index95 global score), self-reported positive and negative affect (through the positive and negative affect schedule94), presence or absence of mental health diagnosis (ascertained by diagnostic interview), use of one or more psychiatric medications (antidepressant, mood stabilizer, antipsychotic, stimulant, or benzodiazepine or other sedative) and grand mean in-scanner frame-to-frame displacement), along with overall cognitive performance (by principal component analysis on phenotypic scores; top three principal components (pcs) retained given cumulative variance explained), to misclassification frequency, averaged separately for measures on which participants scored high and low. these covariates reflect key demographic features of participants, as well as a range of clinically relevant information. for the latter, we complemented an extensively validated structured diagnostic interview for the major dsm-5 disorders101 with self-report measures that reflect participants’ lived experiences with mental health concerns (for example, sleep disturbance and perceived stress—both common experiences that transcend diagnostic boundaries134,135,136,137). relationships between covariates are presented in supplementary fig. 2.for binary covariates (sex, race, diagnosis and medication), two-tailed mann–whitney u tests were used to compare misclassification frequency in the two groups. the remaining continuous covariates were rank correlated with misclassification frequency. this was repeated for high- and low-score misclassification frequency. p values were fdr adjusted, with number of tests = 2 × number of covariates (for example, 30 tests in the yale dataset). results for covariates significantly related to misclassification frequency are presented in the main text and figures; full results are presented in supplementary table 9.to further explore these relationships, these covariates (excluding score pcs) were related (again by two-tailed mann–whitney u test and spearman correlation) to mean phenotypic scores, z-scored within measure and averaged across measures on which each participant was frequently (50% or more of iterations and conditions) and infrequently (less than 50%) misclassified. all p values were fdr adjusted for multiple comparisons. results and multiple comparison correction for covariates significantly related to misclassification frequency are presented in the main text and figures; full results are presented in supplementary table 10.for additional insight into relationships among covariates, phenotype and misclassification frequency, covariates that were significantly related to misclassification frequency were entered into regressions of misclassification frequency on covariates (separately for high and low scores) and of mean phenotypic score on covariates (separately for frequently and infrequently misclassified groups).these analyses were repeated in the ucla and hcp datasets using comparable measures where available (hcp: symptom severity through the achenbach adult self-report, stress through the nih toolbox perceived stress survey and positive affect through the nih toolbox positive affect survey; ucla: symptom severity through the hopkins symptom checklist and diagnosis through the structured clinical interview for dsm-iv). relationships between covariates are presented in supplementary figs. 3 and 4, and results of these analyses are presented in extended data figs. 2e and 3e.finally, to investigate whether models reflect any information beyond these covariates, we regressed the difference between positive and negative gfc edge summary scores for a given modelled phenotype (see ‘phenotype classification’) on scores for that phenotypic measure, as well as all of the demographic and clinical covariates that were used for that dataset. we performed this analysis using the ccp-based models from each dataset (for yale data, which were involved in two cross-dataset analyses, yale–ucla models were used), for all classified phenotypes in cross-dataset analyses, using edges that were selected on at least 75% of iterations. the results are presented in extended data table 1.given the known effects of head motion on fc estimates138,139 and our finding that head motion was significantly correlated with misclassification frequency (supplementary table 9), we repeated the main classification analyses (see ‘phenotype classification’) after regressing mean frame-to-frame displacement out of each edge in the training set. the resulting residualized edges were used for model training, and corresponding regression coefficients were used to residualize the test participant’s edges before model testing. the misclassification frequency for a given condition–phenotype combination was calculated as described previously, and correlated with the misclassification frequency derived from raw functional-connectivity-based classification (supplementary fig. 1).all analyses of preprocessed data were performed in bioimage suite, matlab versions 2017a, 2018a and 2021b (mathworks), r v.3.6.0 for macos and graphpad prism v.9.0.1 for macos. all statistical tests are named and described with corresponding results, and sample sizes—which differ for each phenotypic measure given exclusions (for outlier, missing and intermediate scores)—are noted where relevant (supplementary tables and figure legends). for the main classification analyses, significance was assessed by nonparametric permutation tests. where relevant, p values were adjusted for multiple comparisons using the false discovery rate. significance testing is described and reported, along with the number of comparisons for correction, with the corresponding methods and results.further information on research design is available in the nature research reporting summary linked to this article.the primary dataset used in these analyses represents the first set of participants in an ongoing study. data will be released in waves through the nimh data archive, collection 3276 (https://nda.nih.gov/edit_collection.html?id=3276). the ucla cnp data can be obtained from the openfmri database (https://openfmri.org/dataset/ds000030/). the hcp data are publicly available on the connectomedb database (https://db.humanconnectome.org/app/template/login.vm). data used to generate the parcellation can be found at http://fcon_1000.projects.nitrc.org/indi/retro/yale_hires.html. source data are provided with this paper.matlab code to run classification, validation and covariate analyses is available at https://github.com/abigailsgreene/modelfailure/. bioimage suite tools used for analysis and visualization can be accessed at https://bioimagesuiteweb.github.io/webapp/.finn, e. s. et al. functional connectome fingerprinting: identifying individuals using patterns of brain connectivity. nat. neurosci. 18, 1664–1671 (2015).cas pubmed pubmed central article google scholar dubois, j., galdi, p., paul, l. k. & adolphs, r. a distributed brain network predicts general intelligence from resting-state human neuroimaging data. philos. trans. r. soc. b 373, 20170284 (2018).article google scholar rapuano, k. m. et al. behavioral and brain signatures of substance use vulnerability in childhood. dev. cogn. neurosci. 46, 100878 (2020).pubmed pubmed central article google scholar drysdale, a. t. et al. resting-state connectivity biomarkers define neurophysiological subtypes of depression. nat. med. 23, 28–38 (2016).pubmed pubmed central article cas google scholar hsu, w.-t., rosenberg, m. d., scheinost, d., constable, r. t. & chun, m. m. resting-state functional connectivity predicts neuroticism and extraversion in novel individuals. soc. cogn. affect. neurosci. 13, 224–232 (2018).pubmed pubmed central article google scholar rosenberg, m. d. et al. a neuromarker of sustained attention from whole-brain functional connectivity. nat. neurosci. 19, 165–171 (2015).pubmed pubmed central article cas google scholar avery, e. w. et al. distributed patterns of functional connectivity predict working memory performance in novel healthy and memory-impaired individuals. j. cogn. neurosci. 32, 241–255 (2020).pubmed article google scholar stark, g. f. et al. using functional connectivity models to characterize relationships between working and episodic memory. brain behav. 11, e02105 (2021).pubmed pubmed central article google scholar barron, d. s. et al. transdiagnostic, connectome-based prediction of memory constructs across psychiatric disorders. cereb. cortex 31, 2523–2533 (2020).pubmed central article google scholar wager, t. d. et al. an fmri-based neurologic signature of physical pain. n. engl. j. med. 368, 1388–1397 (2013).cas pubmed pubmed central article google scholar mihalik, a. et al. brain-behaviour modes of covariation in healthy and clinically depressed young people. sci. rep. 9, 11536 (2019).ads pubmed pubmed central article cas google scholar xia, c. h. et al. linked dimensions of psychopathology and connectivity in functional brain networks. nat. commun. 9, 3003 (2018).ads pubmed pubmed central article cas google scholar yarkoni, t. & westfall, j. choosing prediction over explanation in psychology: lessons from machine learning. perspect. psychol. sci. 12, 1100–1122 (2017).pubmed pubmed central article google scholar li, j. et al. cross-ethnicity/race generalization failure of behavioral prediction from resting-state functional connectivity. sci. adv. 8, 1812 (2022).article google scholar scheinost, d. et al. ten simple rules for predictive modeling of individual differences in neuroimaging. neuroimage 193, 35–45 (2019).pubmed article google scholar van essen, d. c. et al. the wu-minn human connectome project: an overview. neuroimage 80, 62–79 (2013).pubmed article google scholar poldrack, r. a. et al. a phenome-wide examination of neural and cognitive function. sci. data 3, 160110 (2016).cas pubmed pubmed central article google scholar fernández, a. l. & abe, j. bias in cross-cultural neuropsychological testing: problems and possible solutions. cult. brain 6, 1–35 (2018).article google scholar manly, j. j. critical issues in cultural neuropsychology: profit from diversity. neuropsychol. rev. 18, 179–183 (2008).pubmed pubmed central article google scholar casaletto, k. b. & heaton, r. k. neuropsychological assessment: past and future. j. int. neuropsychol. soc. 23, 778–790 (2017).pubmed pubmed central article google scholar gabrieli, j. d. e., ghosh, s. s. & whitfield-gabrieli, s. prediction as a humanitarian and pragmatic contribution from human cognitive neuroscience. neuron 85, 11–26 (2015).cas pubmed pubmed central article google scholar bzdok, d., varoquaux, g. & steyerberg, e. w. prediction, not association, paves the road to precision medicine. jama psychiatry 78, 127–128 (2021).pubmed article google scholar fisher, a. j., medaglia, j. d. & jeronimus, b. f. lack of group-to-individual generalizability is a threat to human subjects research. proc. natl acad. sci. usa 115, e6106–e6115 (2018).cas pubmed pubmed central google scholar greene, a. s., gao, s., scheinost, d. & constable, r. t. task-induced brain state manipulation improves prediction of individual traits. nat. commun. 9, 2807 (2018).ads pubmed pubmed central article cas google scholar shen, x. et al. using connectome-based predictive modeling to predict individual behavior from brain connectivity. nat. protoc. 12, 506–518 (2017).cas pubmed pubmed central article google scholar benkarim, o. et al. the cost of untracked diversity in brain-imaging prediction. preprint at biorxiv https://doi.org/10.1101/2021.06.16.448764 (2021).marek, s. et al. reproducible brain-wide association studies require thousands of individuals. nature 603, 654–660 (2022).ads cas pubmed pubmed central article google scholar lanka, p. et al. supervised machine learning for diagnostic classification from large-scale neuroimaging datasets. brain imaging behav. 14, 2378–2416 (2020).pubmed pubmed central article google scholar statucka, m. & cohn, m. origins matter: culture impacts cognitive testing in parkinson’s disease. front. hum. neurosci. 13, 269 (2019).pubmed pubmed central article google scholar whaley, a. l. stereotype threat and neuropsychological test performance in the u.s. african american population. arch. clin. neuropsychol. 36, 1361–1366 (2021).pubmed article google scholar thames, a. d. et al. effects of stereotype threat, perceived discrimination, and examiner race on neuropsychological performance: simple as black and white? j. int. neuropsychol. soc. 19, 583–593 (2013).pubmed pubmed central article google scholar chouldechova, a. & roth, a. a snapshot of the frontiers of fairness in machine learning. commun. acm 63, 82–89 (2020).article google scholar klare, b. f., burge, m. j., klontz, j. c., vorder bruegge, r. w. & jain, a. k. face recognition performance: role of demographic information. ieee trans. inf. forensics secur. 7, 1789–1801 (2012).article google scholar denton, e., hutchinson, b., mitchell, m., gebru, t. & zaldivar, a. image counterfactual sensitivity analysis for detecting unintended bias. preprint at arxiv https://doi.org/10.48550/arxiv.1906.06439 (2020).dressel, j. & farid, h. the accuracy, fairness, and limits of predicting recidivism. sci. adv. 4, eaao5580 (2018).ads pubmed pubmed central article google scholar angwin, j., larson, j., mattu, s., & kirchner, l. machine bias. propublica https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing (2016).roberts, m. et al. common pitfalls and recommendations for using machine learning to detect and prognosticate for covid-19 using chest radiographs and ct scans. nat. mach. intell. 3, 199–217 (2021).article google scholar obermeyer, z., powers, b., vogeli, c. & mullainathan, s. dissecting racial bias in an algorithm used to manage the health of populations. science 366, 447–453 (2019).ads cas pubmed article google scholar henrich, j., heine, s. j. & norenzayan, a. the weirdest people in the world? behav. brain sci. 33, 61–83 (2010).pubmed article google scholar winkler, a. m., ridgway, g. r., webster, m. a., smith, s. m. & nichols, t. e. permutation inference for the general linear model. neuroimage 92, 381–397 (2014).pubmed article google scholar winkler, a. m., webster, m. a., vidaurre, d., nichols, t. e. & smith, s. m. multi-level block permutation. neuroimage 123, 253–268 (2015).pubmed article google scholar elliott, m. l. et al. general functional connectivity: shared features of resting-state and task fmri drive reliable and heritable individual differences in functional brain networks. neuroimage 189, 516–532 (2019).pubmed article google scholar hedden, t., ketay, s., aron, a., markus, h. r. & gabrieli, j. d. e. cultural influences on neural substrates of attentional control. psychol. sci. 19, 12–17 (2008).pubmed article google scholar pérez-arce, p. the influence of culture on cognition. arch. clin. neuropsychol. 14, 581–592 (1999).pubmed article google scholar werry, a. e., daniel, m. & bergström, b. group differences in normal neuropsychological test performance for older non-hispanic white and black/african american adults. neuropsychology 33, 1089–1100 (2019).pubmed pubmed central article google scholar gasquoine, p. g. race-norming of neuropsychological tests. neuropsychol. rev. 19, 250–262 (2009).pubmed article google scholar manly, j. j. et al. the effect of african-american acculturation on neuropsychological test performance in normal and hiv-positive individuals. the hiv neurobehavioral research center (hnrc) group. j. int. neuropsychol. soc. 4, 291–302 (1998).cas pubmed article google scholar mattys, s. l., baddeley, a. & trenkic, d. is the superior verbal memory span of mandarin speakers due to faster rehearsal? mem. cogn. 46, 361–369 (2018).article google scholar rao, a., monteiro, j. m. & mourao-miranda, j. & alzheimer’s disease initiative. predictive modelling using neuroimaging data in the presence of confounds. neuroimage 150, 23–49 (2017).pubmed article google scholar manly, j. j., jacobs, d. m., touradji, p., small, s. a. & stern, y. reading level attenuates differences in neuropsychological test performance between african american and white elders. j. int. neuropsychol. soc. 8, 341–348 (2002).pubmed article google scholar vinopal, k. & morrissey, t. w. neighborhood disadvantage and children’s cognitive skill trajectories. child. youth serv. rev. 116, 105231 (2020).rivera mindt, m., byrd, d., saez, p. & manly, j. increasing culturally competent neuropsychological services for ethnic minority populations: a call to action. clin. neuropsychol. 24, 429–453 (2010).pubmed article google scholar macaulay, r. k., boeve, a. & halpin, a. comparing psychometric properties of the nih toolbox cognition battery to gold-standard measures in socioeconomically diverse older adults. arch. clin. neuropsychol. 36, 1523–1534 (2021). google scholar karr, j. e., mindt, m. r. & iverson, g. l. a multivariate interpretation of the spanish-language nih toolbox cognition battery: the normal frequency of low scores. arch. clin. neuropsychol. 37, 338–351 (2021).pubmed central article google scholar strauss, m. e. & smith, g. t. construct validity: advances in theory and methodology. annu. rev. clin. psychol. 5, 1–25 (2009).pubmed pubmed central article google scholar kucyi, a. et al. prediction of stimulus-independent and task-unrelated thought from functional brain networks. nat. commun. 12, 1793 (2021).ads cas pubmed pubmed central article google scholar hultman, r. et al. brain-wide electrical spatiotemporal dynamics encode depression vulnerability. cell 173, 166–180 (2018).cas pubmed pubmed central article google scholar hughes, j. l., camden, a. a., yangchen, t. & college, a. s. rethinking and updating demographic questions: guidance to improve descriptions of research samples. psi chi j. psychol. res. 21, 138–151 (2016).article google scholar williams, d. r. the concept of race in health services research: 1966 to 1990. health serv. res. 29, 267–274 (1994). google scholar kaplan, j. b. & bennett, t. use of race and ethnicity in biomedical publication. j. am. med. assoc. 289, 2709–2716 (2003).article google scholar fullilove, m. t. comment: abandoning ‘race’ as a variable in public health research–an idea whose time has come. am. j. public health 88, 1297–1298 (1998).cas pubmed pubmed central article google scholar wang, l.-i. race as proxy: situational racism and self-fulfilling stereotypes. depaul law rev. 53, 1013–1110 (2004). google scholar corbie-smith, g., henderson, g., blumenthal, c., dorrance, j. & estroff, s. conceptualizing race in research. j. natl med. assoc. 100, 1235–1243 (2008).pubmed google scholar ioannidis, j. p. a., powe, n. r. & yancy, c. recalibrating the use of race in medical research. j. am. med. assoc. 325, 623–624 (2021).article google scholar wolff, r. f. et al. probast: a tool to assess the risk of bias and applicability of prediction model studies. ann. intern. med. 170, 51–58 (2019).pubmed article google scholar casaletto, k. b. et al. demographically corrected normative standards for the english version of the nih toolbox cognition battery. j. int. neuropsychol. soc. 21, 378–391 (2015).pubmed pubmed central article google scholar nitsch, k. p. et al. uncorrected versus demographically-corrected scores on the nih toolbox cognition battery in persons with traumatic brain injury and stroke. rehabil. psychol. 62, 485–495 (2017).pubmed pubmed central article google scholar alfaro-almagro, f. et al. confound modelling in uk biobank brain imaging. neuroimage 224, 117002 (2021).pubmed article google scholar linn, k. a., gaonkar, b., doshi, j., davatzikos, c. & shinohara, r. t. addressing confounding in predictive models with an application to neuroimaging. int. j. biostat. 12, 31–44 (2016).mathscinet pubmed article google scholar dinga, r., schmaal, l., penninx, b. w. j. h., veltman, d. j. & marquand, a. f. controlling for effects of confounding variables on machine learning predictions. preprint at biorxiv https://doi.org/10.1101/2020.08.17.255034 (2020).yarkoni, t. the generalizability crisis. behav. brain sci. 45, e1 (2022).article google scholar breiman, l. bagging predictors. mach. learn. 24, 123–140 (1996).math google scholar rosenberg, m., noonan, s., degutis, j. & esterman, m. sustaining visual attention in the face of distraction: a novel gradual-onset continuous performance task. atten. percept. psychophys. 75, 426–439 (2013).pubmed article google scholar cui, z. & gong, g. the effect of machine learning regression algorithms and sample size on individualized behavioral prediction with functional connectivity features. neuroimage 178, 622–637 (2018).pubmed article google scholar varoquaux, g. cross-validation failure: small sample sizes lead to large error bars. neuroimage 180, 68–77 (2018).pubmed article google scholar insel, t. et al. research domain criteria (rdoc): toward a new classification framework for research on mental disorders. am. j. psychiatry 167, 748–751 (2010).pubmed article google scholar brainard, d. h. the psychophysics toolbox. spat. vis. 10, 433–436 (1997).cas pubmed article google scholar salehi, m. et al. there is no single functional atlas even for a single individual: functional parcel definitions change with task. neuroimage 208, 116366 (2020).pubmed article google scholar gevins, a. s. et al. effects of prolonged mental work on functional brain topography. electroencephalogr. clin. neurophysiol. 76, 339–350 (1990).cas pubmed article google scholar rosenberg, m. d., finn, e. s., constable, r. t. & chun, m. m. predicting moment-to-moment attentional state. neuroimage 114, 249–256 (2015).pubmed article google scholar conley, m. i. et al. the racially diverse affective expression (radiate) face stimulus set. psychiatry res. 270, 1059–1067 (2018).pubmed pubmed central article google scholar tottenham, n. et al. the nimstim set of facial expressions: judgments from untrained research participants. psychiatry res. 168, 242–249 (2009).pubmed pubmed central article google scholar verbruggen, f., logan, g. d. & stevens, m. a. stop-it: windows executable software for the stop-signal paradigm. behav. res. methods 40, 479–483 (2008).pubmed article google scholar delgado, m. r., nystrom, l. e., fissell, c., noll, d. c. & fiez, j. a. tracking the hemodynamic responses to reward and punishment in the striatum. j. neurophysiol. 84, 3072–3077 (2000).cas pubmed article google scholar speer, m. e., bhanji, j. p. & delgado, m. r. savoring the past: positive memories evoke value representations in the striatum. neuron 84, 847–856 (2014).cas pubmed pubmed central article google scholar baron-cohen, s., jolliffe, t., mortimore, c. & robertson, m. another advanced test of theory of mind: evidence from very high functioning adults with autism or asperger syndrome. j. child psychol. psychiatry 38, 813–822 (1997).cas pubmed article google scholar oldfield, r. c. the assessment and analysis of handedness: the edinburgh inventory. neuropsychologia 9, 97–113 (1971).cas pubmed article google scholar davis, m. h. a multidimensional approach to individual differences in empathy. j. pers. soc. psychol. 44, 113–126 (1983).ads article google scholar wilkinson, g. s. & robertson, g. j. wide range achievement test (wrat5) (pearson, 2017).gioia, g. a., isquith, p. k., guy, s. c. & kenworthy, l. behavior rating inventory of executive function. child neuropsychol. 6, 235–238 (2000).article google scholar kaplan, e., goodglass, h. & weintraub, s. boston naming test (pro-ed, 2001).derogatis, l. r. brief symptom inventory (pearson, 1993).cohen, s., kamarck, t. & mermelstein, r. a global measure of perceived stress. j. health soc. behav. 24, 385–396 (1983).cas pubmed article google scholar watson, d. & clark, l. the panas-x manual for the positive and negative affect schedule-expanded form. iowa res. online 277, 1–27 (1999). google scholar buysse, d. j., reynolds, c. f., monk, t. h., berman, s. r. & kupfer, d. j. the pittsburgh sleep quality index: a new instrument for psychiatric practice and research. psychiatry res. 28, 193–213 (1989).cas pubmed article google scholar evans, d. e. & rothbart, m. k. developing a model for adult temperament. j. res. pers. 41, 868–888 (2007).article google scholar delis, d., kaplan, e. & kramer, j. delis-kaplan executive function system (d-kefs) (pearson, 2001).wechsler, d. wechsler adult intelligence scale 4th edn (wais–iv) (pearson, 2008).wechsler, d. wechsler abbreviated scale of intelligence 2nd edn (pearson, 2011).sheslow, d. & adams, w. wide range assessment of memory and learning 2nd edn (pearson, 2003).sheehan, d. v. et al. the mini-international neuropsychiatric interview (m.i.n.i.): the development and validation of a structured diagnostic psychiatric interview for dsm-iv and icd-10. j. clin. psychiatry 59 (suppl. 20), 22–33 (1998).pubmed google scholar fastenau, p. s. & adams, k. m. heaton, grant, and matthews’ comprehensive norms: an overzealous attempt. j. clin. exp. neuropsychol. 18, 444–448 (1996).article google scholar freedman, d. & manly, j. use of normative data and measures of performance validity and symptom validity in assessment of cognitive function https://nap.nationalacademies.org/resource/21704/freedmanmanlycommissioned-paper.pdf (2015).first, m. b., williams, j. b. w., karg, r. s. & spitzer, r. l. structured clinical interview for dsm-5 disorders: clinician version (american psychiatric association publishing, 2016).smith, s. m. et al. advances in functional and structural mr image analysis and implementation as fsl. neuroimage 23, s208–s219 (2004).pubmed article google scholar lutkenhoff, e. s. et al. optimized brain extraction for pathological brains (optibet). plos one 9, e115551 (2014).ads pubmed pubmed central article cas google scholar frackowiak, r. s. j. et al. (eds) human brain function (academic press, 2004).joshi, a. et al. unified framework for development, deployment and robust testing of neuroimaging algorithms. neuroinformatics 9, 69–84 (2011).pubmed pubmed central article google scholar satterthwaite, t. d. et al. an improved framework for confound regression and filtering for control of motion artifact in the preprocessing of resting-state functional connectivity data. neuroimage 64, 240–256 (2013).pubmed article google scholar r core team. r: a language and environment for statistical computing http://www.r-project.org/ (r foundation for statistical computing, 2017).emerson, j. w. et al. the generalized pairs plot. j. comput. graph. stat. 22, 79–91 (2012).mathscinet article google scholar wei, t. & simko, v. corrplot: visualization of a correlation matrix. r v.0.84 https://cran.r-project.org/package=corrplot (2017).bengtsson, h. r.matlab: read and write mat files and call matlab from within r. r v.3.6.2 https://cran.r-project.org/package=r.matlab (2018).hochman, a. janus-faced race: is race biological, social, or mythical? am. j. biol. anthropol. 175, 453–464 (2021).derogatis, l. r., lipman, r. s., rickels, k., uhlenhuth, e. h. & covi, l. the hopkins symptom checklist (hscl): a self-report symptom inventory. behav. sci. 19, 1–15 (1974).cas pubmed article google scholar first, m., spitzer, r., gibbon, m. & williams, j. structured clinical interview for dsm-iv-tr axis i disorders, research version, patient edition (biometrics research, new york state psychiatric institute, 2002).weintraub, s. et al. cognition assessment using the nih toolbox. neurology 80, s54–s64 (2013).pubmed pubmed central article google scholar achenbach, t. the achenbach system of empirically based assessment (aseba): development, findings, theory and applications (university of vermont research center for children, youth, and families, 2009).uğurbil, k. et al. pushing spatial and temporal resolution for functional and diffusion mri in the human connectome project. neuroimage 80, 80–104 (2013).pubmed article cas google scholar smith, s. m. et al. resting-state fmri in the human connectome project. neuroimage 80, 144–168 (2013).pubmed article google scholar barch, d. m. et al. function in the human connectome: task-fmri and individual differences in behavior. neuroimage 80, 169–189 (2013).pubmed article google scholar glasser, m. f. et al. the minimal preprocessing pipelines for the human connectome project. neuroimage 80, 105–124 (2013).pubmed article google scholar shen, x., tokoglu, f., papademetris, x. & constable, r. t. groupwise whole-brain parcellation from resting-state fmri data for network node identification. neuroimage 82, 403–415 (2013).cas pubmed article google scholar yeo, b. t. t. et al. the organization of the human cerebral cortex estimated by intrinsic functional connectivity. j. neurophysiol. 106, 1125–1165 (2011).pubmed article google scholar smith, s. m., vidaurre, d., alfaro-almagro, f., nichols, t. e. & miller, k. l. estimation of brain age delta from brain imaging. neuroimage 200, 528–539 (2019).pubmed article google scholar guilmette, t. j. et al. american academy of clinical neuropsychology consensus conference statement on uniform labeling of performance test scores. clin. neuropsychol. 34, 437–453 (2020).pubmed article google scholar esterman, m., rosenberg, m. d. & noonan, s. k. intrinsic fluctuations in sustained attention and distractor processing. j. neurosci. 34, 1724–1730 (2014).cas pubmed pubmed central article google scholar karcher, n. r., o’brien, k. j., kandala, s. & barch, d. m. resting-state functional connectivity and psychotic-like experiences in childhood: results from the adolescent brain cognitive development study. biol. psychiatry 86, 7–15 (2019).pubmed pubmed central article google scholar mackenzie, g. b. & wonders, e. rethinking intelligence quotient exclusion criteria practices in the study of attention deficit hyperactivity disorder. front. psychol. 7, 794 (2016).pubmed pubmed central article google scholar bzdok, d. & ioannidis, j. p. a. exploration, inference, and prediction in neuroscience and biomedicine. trends neurosci. 42, 251–262 (2019).cas pubmed article google scholar greene, a. s., gao, s., noble, s., scheinost, d. & constable, r. t. how tasks change whole-brain functional organization to reveal brain–phenotype relationships. cell rep. 32, 870287 (2020).article cas google scholar le, t. t. et al. a nonlinear simulation framework supports adjusting for age when analyzing brainage. front. aging neurosci. 10, 317 (2018).pubmed pubmed central article google scholar noble, s. & scheinost, d. the constrained network-based statistic: a new level of inference for neuroimaging. med. image comput. comput. assist. interv. 12267, 458–468 (2020).pubmed pubmed central google scholar freeman, d., sheaves, b., waite, f., harvey, a. g. & harrison, p. j. sleep disturbance and psychiatric disorders. lancet psychiatry 7, 628–637 (2020).pubmed article google scholar riemann, d., krone, l. b., wulff, k. & nissen, c. sleep, insomnia, and depression. neuropsychopharmacology 45, 74–89 (2020).pubmed article google scholar catabay, c. j., stockman, j. k., campbell, j. c. & tsuyuki, k. perceived stress and mental health: the mediating roles of social support and resilience among black women exposed to sexual violence. j. affect. disord. 259, 143–149 (2019).pubmed pubmed central article google scholar hewitt, p. l., flett, g. l. & mosher, s. w. the perceived stress scale: factor structure and relation to depression symptoms in a psychiatric sample. j. psychopathol. behav. assess. 14, 247–257 (1992).article google scholar van dijk, k. r. a., sabuncu, m. r. & buckner, r. l. the influence of head motion on intrinsic functional connectivity mri. neuroimage 59, 431–438 (2012).pubmed article google scholar power, j. d., barnes, k. a., snyder, a. z., schlaggar, b. l. & petersen, s. e. spurious but systematic correlations in functional connectivity mri networks arise from subject motion. neuroimage 59, 2142–2154 (2012).pubmed article google scholar noble, s. et al. influences on the test–retest reliability of functional connectivity mri and its relationship with behavioral utility. cereb. cortex 27, 5415–5429 (2017).pubmed pubmed central article google scholar download referencesthis work was supported by funding from the nih (gm007205 and tr001864 to a.s.g., mh121095 to d.s. and r.t.c., gm007205 to c.h.). d.s.b was partially funded by the national institute of mental health (5 t32 mh 19961-22). we are grateful to have access to the publicly available ucla cnp and hcp datasets. data were provided in part by the human connectome project, wu-minn consortium (principal investigators: d. van essen and k. ugurbil; 1u54mh091657) funded by the 16 nih institutes and centers that support the nih blueprint for neuroscience research; and by the mcdonnell center for systems neuroscience at washington university. we thank n. turk-browne, d. lee, b. j. casey, m. nitabach, d. barson, s. gottlieb-cohen, c. kelley and f. arias for discussions and suggestions.interdepartmental neuroscience program, yale school of medicine, new haven, ct, usaabigail s. greene, corey horien, dustin scheinost & r. todd constablemd–phd program, yale school of medicine, new haven, ct, usaabigail s. greene & corey horiendepatment of radiology and biomedical imaging, yale school of medicine, new haven, ct, usaxilin shen, stephanie noble, c. alice hahn, jagriti arora, fuyuze tokoglu, dustin scheinost & r. todd constabledepartment of psychiatry, columbia university irving medical center, new york, ny, usamarisa n. spanndepartment of neurology, yale school of medicine, new haven, ct, usacarmen i. carrióndepartment of anesthesiology and pain medicine, university of washington, seattle, wa, usadaniel s. barrondepartment of psychiatry, yale school of medicine, new haven, ct, usadaniel s. barron, gerard sanacora, vinod h. srihari & scott w. woodsdepartment of psychiatry, brigham and women’s hospital, harvard medical school, boston, ma, usadaniel s. barrondepartment of anesthesiology, perioperative and pain medicine, brigham and women’s hospital, harvard medical school, boston, ma, usadaniel s. barrondepartment of biomedical engineering, yale school of engineering and applied science, new haven, ct, usadustin scheinost & r. todd constabledepartment of statistics and data science, yale university, new haven, ct, usadustin scheinostchild study center, yale school of medicine, new haven, ct, usadustin scheinostdepartment of neurosurgery, yale school of medicine, new haven, ct, usar. todd constableyou can also search for this author in pubmed google scholaryou can also search for this author in pubmed google scholaryou can also search for this author in pubmed google scholaryou can also search for this author in pubmed google scholaryou can also search for this author in pubmed google scholaryou can also search for this author in pubmed google scholaryou can also search for this author in pubmed google scholaryou can also search for this author in pubmed google scholaryou can also search for this author in pubmed google scholaryou can also search for this author in pubmed google scholaryou can also search for this author in pubmed google scholaryou can also search for this author in pubmed google scholaryou can also search for this author in pubmed google scholaryou can also search for this author in pubmed google scholaryou can also search for this author in pubmed google scholara.s.g. conceptualized the study, with guidance from r.t.c. and d.s. a.s.g. performed the analyses with support from x.s., s.n., d.s., c.h. and r.t.c. a.s.g. designed the yale study with support from m.n.s. and r.t.c. a.s.g., c.a.h., j.a. and f.t. collected the yale dataset, and c.a.h. managed the study. d.s.b. developed the bipolar disorder study, and j.a. collected data for this study. g.s., v.h.s. and s.w.w. supported all clinical aspects of the yale study. d.s.b. preprocessed the ucla cnp dataset and provided related support. c.i.c. provided guidance on result interpretation. a.s.g. wrote the manuscript, with contributions from r.t.c., s.n., c.i.c. and c.h., and comments from all authors.correspondence to abigail s. greene or r. todd constable.in the past two years, g.s. has served as a consultant or scientific advisory board member to axsome therapeutics, biogen, biohaven pharmaceuticals, boehringer ingelheim international, bristol-myers squibb, clexio, cowen, denovo biopharma, ecr1, ema wellness, engrail therapeutics, gilgamesh, janssen, levo, lundbeck, merck, navitor pharmaceuticals, neurocrine, novartis, noven pharmaceuticals, perception neuroscience, praxis therapeutics, sage pharmaceuticals, seelos pharmaceuticals, vistagen therapeutics and xw labs; and received research contracts from johnson & johnson (janssen), merck and usona. g.s. holds equity in biohaven pharmaceuticals and is a co-inventor on a us patent (8,778,979) held by yale university and a co-inventor on us provisional patent application no. 047162-7177p1 (00754), filed on 20 august 2018 by yale university office of cooperative research. yale university has a financial relationship with janssen pharmaceuticals and may receive financial benefits from this relationship. the university has put multiple measures in place to mitigate this institutional conflict of interest. questions about the details of these measures should be directed to yale university’s conflict of interest office. v.h.s. has served as a scientific advisory board member to takeda and janssen. the remaining authors declare no competing interests.nature thanks thomas nichols and the other, anonymous, reviewer(s) for their contribution to the peer review of this work. peer reviewer reports are available.publisher’s note springer nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.(a) classification accuracy for each phenotypic measure using fc calculated from all in-scanner conditions in the yale dataset, and five different analysis pipelines: an alternative, 368-node parcellation for fc matrix generation, two alternative classification algorithms (ensemble of weak learners and neural network), an alternative phenotypic binarization threshold (mean split), and an alternative (10-fold) cross-validation approach (see methods for additional description of each analysis). box plot line and hinges represent median and quartiles, respectively; whiskers extend to most extreme non-outliers; outliers plotted individually (+). number of classified individuals and size of training sample same as in main analyses (see supplementary table 4) for all analyses except mean split (4 measures [see below], number classified = 109-127, training sample size = 72-82) and 10-fold (number classified same as in main analyses, training sample size = 34-110). r1, rest 1; r2, rest 2; grad, gradual-onset continuous performance task; sst, stop signal task; gfc, general fc. (b) misclassification frequency (mf), averaged across in-scanner conditions and phenotypic measures to derive a single value per participant, compared between each alternative analysis and main-text analyses. rs, two-tailed rank correlation, n = 128-129, p values fdr adjusted. note that phenotype mean split is equivalent to mean ± 1/3 × s.d. for scaled scores; mean split-based model accuracy is not reported for these measures, nor are they included in the calculation of misclassification frequency. given the limited mean split-based results, we repeated this analysis in the hcp data, with comparable results (mean misclassification frequency rs = 0.86, p < 0.0001). 10-fold results reflect 1,000 analysis iterations per phenotypic measure and in-scanner condition (50 per cross-validation partition); all other analyses reflect 100 iterations. in this and all subsequent figures: bnt, boston naming test; wrat, wide range achievement test; vl, verbal learning; fw, finger windows; ln, letter–number sequencing; trails, trail making; vf, verbal fluency; cw, colour–word interference; 20q, 20 questions; vocab, vocabulary; mr, matrix reasoning.results as presented in fig. 1b, fig. 2, and fig. 4a. (a) significance via one-tailed permutation testing (as in fig. 1b); p values fdr adjusted (3 tests). for sample sizes, see supplementary table 5. (b) as in yale data, mean of permuted distribution did not significantly differ from 0.5 (all p > 0.05, fdr adjusted [3 tests]), mean and median of original data-based distribution significantly differed from 0.5 (all p < 0.0001, fdr adjusted [6 tests] via two-tailed t- and wilcoxon signed-rank tests), and the misclassification frequency distributions for original and permuted analyses significantly differed for each measure (all p < 0.0001, fdr adjusted [3 tests] via two-tailed, two-sample kolmogorov–smirnov test). (c) *p < 0.0001, fdr adjusted (3 tests) via paired, one-tailed wilcoxon signed-rank test (as in fig. 2b). (d) given the small number of included measures, we present these results only for consistency with main analyses. as in fig. 2c, different participants excluded for intermediate, missing, or outlier scores for each measure; number of correlated participants for each measure pair ranges from misclassification frequency: 103-138, measure: 162-163. (e) results as presented in fig. 4a. covariate relationships presented if they were significantly related to misclassification frequency in low or high scorers (p < 0.05, adjusted), or if they were significantly related to misclassification frequency in yale analyses (education, race) to demonstrate comparable trends. all p values fdr adjusted (22 tests). for full results and relationship of covariates to mean score, as well as sample sizes, see supplementary tables 9 and 10. pamret, paired associates memory task, retrieval; sst, stop signal task; mf, misclassification frequency.results as presented in fig. 1b, fig. 2a, b, fig. 3a, and fig. 4a. given the large hcp sample size, 10-fold cross-validation was used (20 partitions, 50 subsampling iterations each), with the requirement that family members be assigned to the same fold. given that only two measures were classified, we omit measure versus misclassification frequency similarity and hierarchical linkage analyses. (a) significance via one-tailed permutation testing (as in fig. 1b); p values fdr adjusted (2 tests). for sample sizes, see supplementary table 6. (b) permuted distribution means significantly differed from 0.5 via two-tailed, one-sample t-test (ciq mean = 0.491 [p < 0.0001], fiq mean = 0.498 [p = 0.04], both fdr adjusted [2 tests]). all else as in yale and ucla analyses: mean and median of original data-based distribution significantly differed from 0.5 (all p < 0.0001, fdr adjusted [4 tests] via two-tailed t- and wilcoxon signed-rank tests), and the misclassification frequency distributions for original and permuted analyses significantly differed for each measure (all p < 0.0001, fdr adjusted [2 tests] via two-tailed, two-sample kolmogorov–smirnov test). mf, misclassification frequency. (c) **p = 0.001, ****p < 0.0001, fdr adjusted (2 tests) via paired, one-tailed wilcoxon signed-rank test (as in fig. 2b). (d) results presented as in fig. 3a. bar height, grand mean; error bars, s.d. *p < 0.0001, fdr adjusted (9 tests) via two-tailed, nested anova. for each classified measure (ciq/vocabulary and fiq/mr for hcp/yale), six models were trained: 1 using all yale participants, 1 using yale ccp, 1 using yale mcp (see fig. 3 legend for training set sizes), 1 using all hcp participants (number of participants used for training after excluding intermediate and outlier scores and subsampling to balance classes: 230 and 350 for crystallized and fluid measures, respectively), 1 using hcp ccp (168, 216), and 1 using hcp mcp (62, 134). see supplementary tables 4 and 6 for test-set sizes. (e) results as presented in fig. 4a. covariate relationships presented if they were significantly related to misclassification frequency in low or high scorers (p < 0.05, adjusted). for full results and relationship of covariates to mean score, as well as sample sizes, see supplementary tables 9 and 10. ****p < 0.0001; all p values fdr adjusted (22 tests).results as presented in fig. 3c. for mr, ym and vocabulary, uc two nodes were tied for highest degree (mr, ym: 26 and 157; vocabulary, uc: 166 and 191). only one node for each model visualized for illustration.edges, gfc: t statistics for each gfc edge found to significantly differ (via two-sample t-test) between groups (p < 0.05, fdr adjusted), ordered by network. red, ccp>mcp; blue, mcp>ccp. networks, gfc: mean t statistics for each network pair (using gfc) found to significantly differ (via constrained nbs133) between groups (one-tailed p < 0.025, fdr adjusted). red, ccp>mcp; blue, mcp>ccp. significant edges across tasks and significant networks across tasks: number of times (i.e., tasks for which) edge (ordered by network) or network pair was significantly greater for ccp than mcp – number of times edge or network pair was significantly greater for mcp than ccp. mean gfc, ccp and mean gfc, mcp: gfc, averaged across participants within each group; main diagonal set to 0, and nodes ordered by network. note that ccp and mcp groups differ for each phenotypic measure and in-scanner task (range of number of participants using gfc across phenotypic measures: ccp = 46-81, mcp = 23-63). black dashed lines separate networks: 1 = medial frontal, 2 = frontoparietal, 3 = default mode, 4 = motor, 5 = visual a, 6 = visual b, 7 = visual association, 8 = salience, 9 = subcortical, 10 = cerebellum (for network visualization, see140).schematic representation of recommended framework for study design and analysis to yield more precise, useful, and unbiased models.reported racial and ethnic breakdowns of the yale, ucla and hcp samples.this file contains supplementary figs. 1–4, supplementary tables 1–11, supplementary discussion and supplementary references.open access this article is licensed under a creative commons attribution 4.0 international license, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the creative commons license, and indicate if changes were made. the images or other third party material in this article are included in the article’s creative commons license, unless indicated otherwise in a credit line to the material. if material is not included in the article’s creative commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. to view a copy of this license, visit http://creativecommons.org/licenses/by/4.0/.reprints and permissionsgreene, a.s., shen, x., noble, s. et al. brain–phenotype models fail for individuals who defy sample stereotypes. nature (2022). https://doi.org/10.1038/s41586-022-05118-wdownload citationreceived: 09 september 2021accepted: 15 july 2022published: 24 august 2022doi: https://doi.org/10.1038/s41586-022-05118-wanyone you share the following link with will be able to read this content:sorry, a shareable link is not currently available for this article. provided by the springer nature sharedit content-sharing initiative by submitting a comment you agree to abide by our terms and community guidelines. if you find something abusive or that does not comply with our terms or guidelines please flag it as inappropriate.