in belgrade’s republic square, dome-shaped cameras hang prominently on wall fixtures, silently scanning people walking across the central plaza. it’s one of 800 locations in the city that serbia’s government said last year it would monitor using cameras equipped with facial-recognition software, purchased from electronics firm huawei in shenzhen, china.the government didn’t ask belgrade’s residents whether they wanted the cameras, says danilo krivokapić, who directs a human-rights organization called the share foundation, based in the city’s old town. this year, it launched a campaign called hiljade kamera — ‘thousands of cameras’ — questioning the project’s legality and effectiveness, and arguing against automated remote surveillance.belgrade is experiencing a shift that has already taken place elsewhere. facial-recognition technology (frt) has long been in use at airport borders and on smartphones, and as a tool to help police identify criminals. but it is now creeping further into private and public spaces. from quito to nairobi, moscow to detroit, hundreds of municipalities have installed cameras equipped with frt, sometimes promising to feed data to central command centres as part of ‘safe city’ or ‘smart city’ solutions to crime. the covid-19 pandemic might accelerate their spread.the trend is most advanced in china, where more than 100 cities bought face-recognition surveillance systems last year, according to jessica batke, who has analysed thousands of government procurement notices for chinafile, a magazine published by the center on u.s.-china relations in new york city. a facial-recognition scanning system at the entrance to a subway station in zhengzhou, china (in december 2019).credit: gilles sabrié/nyt/redux/eyevinebut resistance is growing in many countries. researchers, as well as civil-liberties advocates and legal scholars, are among those disturbed by facial recognition’s rise. they are tracking its use, exposing its harms and campaigning for safeguards or outright bans. part of the work involves exposing the technology’s immaturity: it still has inaccuracies and racial biases. opponents are also concerned that police and law-enforcement agencies are using frt in discriminatory ways, and that governments could employ it to repress opposition, target protesters or otherwise limit freedoms — as with the surveillance in china’s xinjiang province. is facial recognition too biased to be let loose?legal challenges have emerged in europe and parts of the united states, where critics of the technology have filed lawsuits to prevent its use in policing. many us cities have banned public agencies from using facial recognition — at least temporarily — or passed legislation to demand more transparency on how police use surveillance tools. europe and the united states are now considering proposals to regulate the technology, so the next few years could define how frt’s use is constrained or entrenched. what unites the current wave of pushback is the insistence that these technologies are not inevitable, wrote amba kak, a legal scholar at new york university’s ai now institute, in a september report1 on regulating biometrics.by 2019, 64 countries used frt in surveillance, says steven feldstein, a policy researcher at the carnegie endowment for international peace in washington dc, who has analysed the technology’s global spread2. feldstein found that cities in 56 countries had adopted smart-city platforms. many of them purchased their cameras from chinese firms, often apparently encouraged by subsidized loans from chinese banks. (us, european, japanese and russian firms also sell cameras and software, feldstein noted.)belgrade’s project illustrates concerns that many have over the rise of smart-city systems: there is no evidence that they reduce crime more than ordinary video cameras do, and the public knows little about systems that are ostensibly for their benefit. krivokapić says he is worried that the technology seems more suited to offering an increasingly authoritarian government a tool to curb political dissent.having cameras around in a young democracy such as serbia can be problematic because of the potential for political misuse, says ljubiša bojić, coordinator of the digital sociometrics lab at the university of belgrade, which studies the effects of artificial intelligence (ai) on society. although the situation has changed since the turmoil of the nineties, the dogma of police state and fear of intelligence agencies makes serbia an inappropriate place for implementation of ai cameras.when the government announced the project, it gave few details. but share found a 2018 press release on huawei’s website (which the firm deleted) that announced tests of high-definition cameras in belgrade. the document said that the cameras had helped serbian police to solve several major criminal cases and improve security at major sporting events. this year, the government disclosed that the scheme involves purchasing 8,000 cameras for use in police cars, as body-worn cameras and on buildings.cameras from chinese electronics firm huawei above a cafe overlooking belgrade’s republic square.credit: vladimir zivojinovic for naturethere are many questions that remain unanswered, krivokapić says. for example, where will the data be stored? in serbia or in china? will huawei have access to the data? after share and others pressed for more details, the serbian government said that data wouldn’t be collected or kept by huawei. but lee tien, a senior staff attorney at the electronic frontier foundation in san francisco, california, says that one of the main reasons large technology firms — whether in china or elsewhere — get involved in supplying ai surveillance technology to governments is that they expect to collect a mass of data that could improve their algorithms.serbia models its data-protection laws on the european union’s general data protection regulation (gdpr), but it is unclear whether the interior ministry’s plans satisfy the country’s laws, serbia’s data-protection commissioner said in may. (the interior ministry declined to comment for this article, and huawei did not respond to questions.) why faces don’t always tell the truth about feelingsoverall, there haven’t been studies proving that ‘safe’ or ‘smart’ cities reduce crime, says pete fussey, a sociologist at the university of essex in colchester, uk, who researches human rights, surveillance and policing. he says anecdotal claims are being leveraged into a proof of principle for a surveillance technology that is still very new. the history of technology and law enforcement is littered with examples of hubris and outlandish claims, he says. it’s reasonably uncontroversial to say that surveillance cameras in general are more effective for tackling crimes against things, rather than people. once you start getting into automated surveillance, it becomes more difficult, partly because it is not used as much.in march, vladimir bykovsky, a moscow resident who’d recently returned from south korea, left his apartment for a few moments to throw out his rubbish. half an hour later, police were at his door. the officers said he had violated covid-19 quarantine rules and would receive a fine and court date. bykovsky asked how they’d known he’d left. the officers told him it was because of a camera outside his apartment block, which they said was connected to a facial-recognition surveillance system working across the whole of moscow.they said they’d received an alert that quarantine had been broken by a vladimir bykovsky, he says. i was just shocked.the russian capital rolled out a city-wide video surveillance system in january, using software supplied by moscow-based technology firm ntechlab. the firm’s former head, alexey minin, said at the time that it was the world’s largest system of live facial recognition. ntechlab co-founder artem kukharenko says it supplies its software to other cities, but wouldn’t name locations because of non-disclosure agreements. asked whether it cut down on crime, he pointed to moscow media reports of hooligans being detained during the 2018 world cup tournament, when the system was in test mode. other reports say the system spotted 200 quarantine breakers during the first few weeks of moscow’s covid-19 lockdown. a surveillance camera at a metro station in moscow (pictured in january 2020), which has an extensive network of facial-recognition cameras.credit: kirill kudryavtsev/afp/gettylike russia, governments in china, india and south korea have used facial recognition to help trace contacts and enforce quarantine; other countries probably have, too. in may, the chief executive of london’s heathrow airport said it would trial thermal scanners with facial-recognition cameras to identify potential virus carriers. many firms also say they have adapted their technologies to spot people wearing masks (although, as with many facial-recognition performance claims, there is no independent verification).researchers worry that the use of live-surveillance technologies is likely to linger after the pandemic. this could have a chilling effect on societal freedoms. last year, a group set up to provide ethical advice on policing asked more than 1,000 londoners about the police’s use of live facial recognition there; 38% of 16–24-year-olds and 28% of asian, black and mixed-ancestry people surveyed said they would stay away from events monitored with live facial recognition. some people who attend rallies have taken to wearing masks or camouflage-like ‘dazzle’ make-up to try to confuse facial-recognition systems. but their only ‘opt-out’ option is to not turn up.activist darya kozlova in moscow has her face painted with features said to confuse cameras.credit: yuri kadobnov/afp/gettyanother concern, especially in the united states, is that the watch lists that police use to check images against can be enormous — and can include people without their knowledge. researchers at the center on privacy and technology at georgetown university in washington dc estimated in 2016 that around half of all americans were in law-enforcement face-recognition networks, because many states allow police to search driver’s-licence databases.and earlier this year, the new york times revealed that software company clearview ai in new york city had scraped billions of images from social-media sites and compiled them into a facial-recognition database. the firm offered its service to police in and outside the united states.the clearview scandal threw into relief what researchers had long thought was possible, says ben sobel, who studies the ethics and governance of ai at the berkman klein center at harvard university in cambridge, massachusetts. technology capable of recognizing faces at scale is becoming more accessible and requiring less sophistication to run.social-media sites such as twitter, facebook and youtube have told clearview to stop scraping their sites, saying it breaches their terms of service. and several lawsuits have been filed against the firm, including under an illinois law that allows individuals in that state to sue firms who capture their biometric information — including from the face — without their consent. in june, the european data protection board issued an opinion that clearview’s service breaches the gdpr --— but no action has yet been taken. clearview, which stopped selling some of its services this year after media coverage, told nature that its image-search engine functions within the bounds of applicable laws. the ethical questions that haunt facial-recognition researchclearview isn’t the only firm to harvest online images of faces. a company called pimeyes in wrocław, poland, has a website that allows anyone to find matching photos online, and the firm claims to have scraped 900 million images — although, it says, not from social-media sites. and ntechlab launched the findface app in 2016 to permit face-matching on the russian social network vk. the company later withdrew the app.it now seems impossible to stop anyone from privately building up large facial-recognition databases from online photos. but in july, researchers at the university of chicago in illinois unveiled a piece of software called fawkes that adds imperceptible tweaks to images so that they look the same to the human eye, but like a different person to a machine-learning model. if people ‘cloak’ enough of their facial images through fawkes, they say, efforts such as clearview’s will learn the wrong features and fail to match new, unaltered images to its models. the researchers hope that photo-sharing or social-media platforms might offer the service to protect users, by applying the software before photos are displayed online.in september 2019, the london-based ada lovelace institute, a charity-funded research institute that scrutinizes ai and society, published a nationally representative survey3 of more than 4,000 british adults’ views on frt. it found that the majority of people supported facial recognition when they could see a public benefit, such as in criminal investigations, to unlock smartphones or to check passports in airports. but 29% were uncomfortable with the police using the technology, saying that it infringes on privacy and normalizes surveillance, and that they don’t trust the police to use it ethically. there was almost no support for its use in schools, at work or in supermarkets. the public expects facial-recognition technology in policing to be accompanied by safeguards and linked to a public benefit, the survey concluded.many researchers, and some companies, including google, amazon, ibm and microsoft, have called for bans on facial recognition — at least on police use of the technology — until stricter regulations are brought in. some point admiringly to the gdpr, which prohibits processing of biometric data without consent — although it also offers many exceptions, such as if data are manifestly public, or if the use is necessary for reasons of substantial public interest.when it comes to commercial use of facial recognition, some researchers worry that laws focused only on gaining consent to use it aren’t strict enough, says woodrow hartzog, a computer scientist and law professor at northeastern university in boston, massachusetts, who studies facial surveillance. it’s very hard for an individual to understand the risks of consenting to facial surveillance, he says. and they often don’t have a meaningful way to say ‘no’. halt the use of facial-recognition technology until it is regulatedhartzog, who views the technology as the most dangerous ever to be invented, says if us lawmakers allow firms to use facial recognition despite its inevitable abuses, they should write rules that prohibit the collection and storage of ‘faceprints’ from places such as gyms and restaurants, and prohibit the use of frt in combination with automated decision-making such as predictive policing, advert-targeting and employment.the algorithmic justice league, a researcher-led campaigning organization founded by computer scientist joy buolamwini at the massachusetts institute of technology in cambridge, has been prominent in calling for a us federal moratorium on facial recognition. in 2018, buolamwini co-authored a paper showing how facial-analysis systems are more likely to misidentify gender in darker-skinned and female faces4. and in may, she and other researchers argued in a report that the united states should create a federal office to manage frt applications — rather like the us food and drug administration approves drugs or medical devices5.what a federal office would do is provide multiple levels of clearance before a product can enter the market. if the risks far outweigh the benefits, maybe you don’t use that product, says erik learned-miller, a computer scientist at the university of massachusetts in amherst who co-authored the report.in china, too, people have expressed discomfort with widespread use of facial recognition — by private firms, at least. an online survey of more than 6,000 people in december 2019 by the nandu personal information protection research centre, a think tank affiliated with the southern metropolis daily newspaper in guangzhou, found that 80% of people worried about lax security in facial-recognition systems and 83% wanted more control over their face data, including the option to delete it. chinese newspapers have run articles questioning frt use, and the government is bringing in tighter data-protection laws. but the debate doesn’t usually question the use of cameras by the police and government, and the data-protection laws don’t put limits on government surveillance, says graham webster, who studies china’s digital policies at stanford university in california.a software engineer at hanwang technology in beijing tests a facial-recognition programme that identifies people wearing face masks.credit: thomas peter/reuterseurope’s data-protection rules say that police can process data for biometric purposes if it’s necessary and subject to appropriate safeguards. a key question here, says fussey, is whether it would be proportionate to, for example, put tens of thousands of people under video surveillance to catch a criminal.so far, british judges have suggested they think it might be, but only if the use of the technology by police has tighter controls. last year, a man named ed bridges sued police in south wales, alleging that his rights to privacy had been breached because he was scanned by live facial-recognition cameras on two occasions in cardiff, uk, when police were searching crowds to find people on a watch list. in august, a uk court ruled that the actions were unlawful: police didn’t have enough guidance and rules about when they could use the system and who would be in their database, and they hadn’t sufficiently checked the software’s racial or gender bias. but judges didn’t agree that the camera breached bridges’ privacy rights: it was a ‘proportionate’ interference, they said.the eu is considering an ai framework that could set rules for biometrics. this year, a white paper — a prelude to proposed legislation — suggested that special rules might be needed for ‘high-risk’ ai, which would include facial recognition. most people and firms who wrote into a consultation that followed the document felt that further regulations were needed to use frt in public spaces.ultimately, the people affected by frt need to discuss what they find acceptable, says aidan peppin, a social scientist at the ada lovelace institute. this year, he has been helping to run a citizens’ biometrics council, featuring in-depth workshops with around 60 people across the country. participants provide their views on biometrics, which will inform a uk review of legislation in the area. the public voice needs to be front and centre in this debate, he says.